{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Shape of training features  (784, 60000)\n",
      "Shape of test features  (784, 10000)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "tr_x = tr_x.T\n",
    "te_x = te_x.T\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_x = tf.cast(tr_x, tf.float32) \n",
    "te_x = tf.cast(te_x, tf.float32)\n",
    "tr_y = tf.cast(tr_y, tf.float32)\n",
    "te_y = tf.cast(te_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = tf.Variable(tf.random.normal(shape=[784,300], mean=0.0, stddev=0.05))\n",
    "w2 = tf.Variable(tf.random.normal(shape=[300,100], mean=0.0, stddev=0.05))\n",
    "w3 = tf.Variable(tf.random.normal(shape=[100,10], mean=0.0, stddev=0.05))\n",
    "b = tf.Variable([0.])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forwardPass(x, w1,w2, w3, b):\n",
    "    w1 = tf.transpose(w1)\n",
    "    w2 = tf.transpose(w2)\n",
    "    w3 = tf.transpose(w3)\n",
    "     \n",
    "    y_pred = tf.matmul(w1, x) + b \n",
    "    a1 = tf.maximum(y_pred, 1) \n",
    "    \n",
    "    y_pred = tf.matmul(w2, a1) + b \n",
    "    a2 = tf.maximum(y_pred, 1) \n",
    "    \n",
    "    y_pred = tf.matmul(w3, a2) + b\n",
    "    return softmax(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(y_pred):\n",
    "    y_pred_exp = tf.math.exp(y_pred)\n",
    "    summation = tf.reduce_sum(y_pred_exp, 0, keepdims=True) \n",
    "    return y_pred_exp / summation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    \n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=0)\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(x, y, w1,w2,w3, b):\n",
    "  \n",
    "  y_pred_softmax = forwardPass(x,w1,w2,w3, b)\n",
    "  predictions_correct = tf.cast(tf.equal(tf.round(y_pred_softmax), y), tf.float32)\n",
    "  return tf.reduce_mean(predictions_correct),y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(num_Iterations = 50):\n",
    "   \n",
    "    trainingLosses= []\n",
    "    testLosses= []\n",
    "    trainingAccuracies = []\n",
    "    testAccuracies = []\n",
    "    \n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        y_pred = forwardPass(tr_x,w1,w2,w3, b)\n",
    "        currentLoss = cross_entropy(tr_y, y_pred)\n",
    "      \n",
    "      gradients = tape.gradient(currentLoss, [w1,w2,w3, b])\n",
    "        \n",
    "     \n",
    "      tr_accuracy, y_pred_softmax = calculate_accuracy(tr_x, tr_y, w1,w2,w3, b)\n",
    "      te_accuracy, y_pred_softmax = calculate_accuracy(te_x, te_y, w1,w2,w3, b)\n",
    "      te_currentLoss = cross_entropy(te_y, y_pred_softmax)\n",
    "    \n",
    "      # Appending and print the information for training instances\n",
    "      trainingAccuracies.append(tr_accuracy.numpy())\n",
    "      trainingLosses.append(currentLoss.numpy())\n",
    "      print (\"Iteration \", i, \": train_loss = \",currentLoss.numpy(), \"  train_acc: \", tr_accuracy.numpy())\n",
    "      \n",
    "      # Appending and print the information for validation instances\n",
    "      testLosses.append(te_currentLoss.numpy())\n",
    "      testAccuracies.append(te_accuracy.numpy())\n",
    "      print (\"Iteration \", i, \": test_loss = \",te_currentLoss.numpy(), \"  test_acc: \", te_accuracy.numpy())\n",
    "      print(\"*\"*60)\n",
    "    \n",
    "      #Calling Adam optimizer the for updating the weights and trainfing the data\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.001).apply_gradients(zip(gradients, [w1,w2,w3,b])) \n",
    "        \n",
    "    \n",
    "    plt.style.use(\"ggplot\") \n",
    "    plt.figure()\n",
    "    plt.plot(testLosses, label=\"Val Loss\")\n",
    "    plt.plot(trainingLosses, label=\"Train Loss\")\n",
    "    plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
    "    plt.plot(testAccuracies, label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : train_loss =  2.4385839   train_acc:  0.9\n",
      "Iteration  0 : test_loss =  2.4387796   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  1 : train_loss =  2.3695083   train_acc:  0.9\n",
      "Iteration  1 : test_loss =  2.3695836   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  2 : train_loss =  2.332434   train_acc:  0.9\n",
      "Iteration  2 : test_loss =  2.332573   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  3 : train_loss =  2.296598   train_acc:  0.9\n",
      "Iteration  3 : test_loss =  2.2968414   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  4 : train_loss =  2.2625449   train_acc:  0.9\n",
      "Iteration  4 : test_loss =  2.2628605   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  5 : train_loss =  2.2365532   train_acc:  0.9\n",
      "Iteration  5 : test_loss =  2.2369833   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  6 : train_loss =  2.202121   train_acc:  0.9\n",
      "Iteration  6 : test_loss =  2.2027948   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  7 : train_loss =  2.161383   train_acc:  0.9\n",
      "Iteration  7 : test_loss =  2.162332   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  8 : train_loss =  2.1155846   train_acc:  0.9\n",
      "Iteration  8 : test_loss =  2.1169667   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  9 : train_loss =  2.0707266   train_acc:  0.9\n",
      "Iteration  9 : test_loss =  2.072511   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  10 : train_loss =  2.0229032   train_acc:  0.9\n",
      "Iteration  10 : test_loss =  2.024766   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  11 : train_loss =  1.9811435   train_acc:  0.9\n",
      "Iteration  11 : test_loss =  1.9832605   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  12 : train_loss =  1.9242977   train_acc:  0.9\n",
      "Iteration  12 : test_loss =  1.9261125   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  13 : train_loss =  1.8797187   train_acc:  0.90008664\n",
      "Iteration  13 : test_loss =  1.8814262   test_acc:  0.90009\n",
      "************************************************************\n",
      "Iteration  14 : train_loss =  1.8302675   train_acc:  0.9\n",
      "Iteration  14 : test_loss =  1.8324027   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  15 : train_loss =  1.7944474   train_acc:  0.9005067\n",
      "Iteration  15 : test_loss =  1.7959623   test_acc:  0.90045\n",
      "************************************************************\n",
      "Iteration  16 : train_loss =  1.7456752   train_acc:  0.900005\n",
      "Iteration  16 : test_loss =  1.7478511   test_acc:  0.89998\n",
      "************************************************************\n",
      "Iteration  17 : train_loss =  1.7122594   train_acc:  0.90171665\n",
      "Iteration  17 : test_loss =  1.7139549   test_acc:  0.90154\n",
      "************************************************************\n",
      "Iteration  18 : train_loss =  1.6632496   train_acc:  0.90011\n",
      "Iteration  18 : test_loss =  1.6655164   test_acc:  0.90009\n",
      "************************************************************\n",
      "Iteration  19 : train_loss =  1.6299866   train_acc:  0.9028817\n",
      "Iteration  19 : test_loss =  1.6317601   test_acc:  0.90266\n",
      "************************************************************\n",
      "Iteration  20 : train_loss =  1.5872568   train_acc:  0.90080166\n",
      "Iteration  20 : test_loss =  1.5894176   test_acc:  0.90082\n",
      "************************************************************\n",
      "Iteration  21 : train_loss =  1.5521679   train_acc:  0.90370667\n",
      "Iteration  21 : test_loss =  1.5544393   test_acc:  0.90339\n",
      "************************************************************\n",
      "Iteration  22 : train_loss =  1.5184604   train_acc:  0.90301335\n",
      "Iteration  22 : test_loss =  1.5211236   test_acc:  0.90296\n",
      "************************************************************\n",
      "Iteration  23 : train_loss =  1.4849925   train_acc:  0.90532833\n",
      "Iteration  23 : test_loss =  1.4875458   test_acc:  0.90496\n",
      "************************************************************\n",
      "Iteration  24 : train_loss =  1.454343   train_acc:  0.905325\n",
      "Iteration  24 : test_loss =  1.4577699   test_acc:  0.90504\n",
      "************************************************************\n",
      "Iteration  25 : train_loss =  1.4185963   train_acc:  0.9075367\n",
      "Iteration  25 : test_loss =  1.4214426   test_acc:  0.90715\n",
      "************************************************************\n",
      "Iteration  26 : train_loss =  1.3982216   train_acc:  0.9072483\n",
      "Iteration  26 : test_loss =  1.4024656   test_acc:  0.9069\n",
      "************************************************************\n",
      "Iteration  27 : train_loss =  1.3692125   train_acc:  0.9097767\n",
      "Iteration  27 : test_loss =  1.3719356   test_acc:  0.9095\n",
      "************************************************************\n",
      "Iteration  28 : train_loss =  1.3484701   train_acc:  0.90952665\n",
      "Iteration  28 : test_loss =  1.3536125   test_acc:  0.90885\n",
      "************************************************************\n",
      "Iteration  29 : train_loss =  1.3221399   train_acc:  0.91178\n",
      "Iteration  29 : test_loss =  1.3249307   test_acc:  0.91131\n",
      "************************************************************\n",
      "Iteration  30 : train_loss =  1.3042974   train_acc:  0.9119\n",
      "Iteration  30 : test_loss =  1.3098475   test_acc:  0.91097\n",
      "************************************************************\n",
      "Iteration  31 : train_loss =  1.2782174   train_acc:  0.9139633\n",
      "Iteration  31 : test_loss =  1.2810903   test_acc:  0.91366\n",
      "************************************************************\n",
      "Iteration  32 : train_loss =  1.2616537   train_acc:  0.91433334\n",
      "Iteration  32 : test_loss =  1.2673366   test_acc:  0.91321\n",
      "************************************************************\n",
      "Iteration  33 : train_loss =  1.2392126   train_acc:  0.9165133\n",
      "Iteration  33 : test_loss =  1.242375   test_acc:  0.91594\n",
      "************************************************************\n",
      "Iteration  34 : train_loss =  1.2235359   train_acc:  0.916195\n",
      "Iteration  34 : test_loss =  1.229486   test_acc:  0.91524\n",
      "************************************************************\n",
      "Iteration  35 : train_loss =  1.2028501   train_acc:  0.91969836\n",
      "Iteration  35 : test_loss =  1.2062132   test_acc:  0.91931\n",
      "************************************************************\n",
      "Iteration  36 : train_loss =  1.1910094   train_acc:  0.9185733\n",
      "Iteration  36 : test_loss =  1.197247   test_acc:  0.91757\n",
      "************************************************************\n",
      "Iteration  37 : train_loss =  1.1726464   train_acc:  0.92233\n",
      "Iteration  37 : test_loss =  1.1761899   test_acc:  0.92194\n",
      "************************************************************\n",
      "Iteration  38 : train_loss =  1.159656   train_acc:  0.92128\n",
      "Iteration  38 : test_loss =  1.1661276   test_acc:  0.92018\n",
      "************************************************************\n",
      "Iteration  39 : train_loss =  1.1447227   train_acc:  0.92481333\n",
      "Iteration  39 : test_loss =  1.1484045   test_acc:  0.92446\n",
      "************************************************************\n",
      "Iteration  40 : train_loss =  1.1297867   train_acc:  0.924125\n",
      "Iteration  40 : test_loss =  1.1363827   test_acc:  0.92317\n",
      "************************************************************\n",
      "Iteration  41 : train_loss =  1.122729   train_acc:  0.92718\n",
      "Iteration  41 : test_loss =  1.1265099   test_acc:  0.92702\n",
      "************************************************************\n",
      "Iteration  42 : train_loss =  1.1032081   train_acc:  0.927175\n",
      "Iteration  42 : test_loss =  1.1097912   test_acc:  0.92636\n",
      "************************************************************\n",
      "Iteration  43 : train_loss =  1.0985364   train_acc:  0.93084\n",
      "Iteration  43 : test_loss =  1.1025006   test_acc:  0.93061\n",
      "************************************************************\n",
      "Iteration  44 : train_loss =  1.0788854   train_acc:  0.9298817\n",
      "Iteration  44 : test_loss =  1.0857803   test_acc:  0.92933\n",
      "************************************************************\n",
      "Iteration  45 : train_loss =  1.0747019   train_acc:  0.93376833\n",
      "Iteration  45 : test_loss =  1.0792153   test_acc:  0.93361\n",
      "************************************************************\n",
      "Iteration  46 : train_loss =  1.0544294   train_acc:  0.9324667\n",
      "Iteration  46 : test_loss =  1.061875   test_acc:  0.93185\n",
      "************************************************************\n",
      "Iteration  47 : train_loss =  1.05201   train_acc:  0.9360217\n",
      "Iteration  47 : test_loss =  1.056831   test_acc:  0.93591\n",
      "************************************************************\n",
      "Iteration  48 : train_loss =  1.0328622   train_acc:  0.933805\n",
      "Iteration  48 : test_loss =  1.0405197   test_acc:  0.93304\n",
      "************************************************************\n",
      "Iteration  49 : train_loss =  1.0319788   train_acc:  0.93738335\n",
      "Iteration  49 : test_loss =  1.0370996   test_acc:  0.93722\n",
      "************************************************************\n",
      "Iteration  50 : train_loss =  1.0129495   train_acc:  0.93616164\n",
      "Iteration  50 : test_loss =  1.020785   test_acc:  0.93529\n",
      "************************************************************\n",
      "Iteration  51 : train_loss =  1.0153749   train_acc:  0.93880665\n",
      "Iteration  51 : test_loss =  1.0213766   test_acc:  0.9387\n",
      "************************************************************\n",
      "Iteration  52 : train_loss =  0.9971594   train_acc:  0.93716335\n",
      "Iteration  52 : test_loss =  1.0050229   test_acc:  0.93697\n",
      "************************************************************\n",
      "Iteration  53 : train_loss =  0.9974005   train_acc:  0.93970835\n",
      "Iteration  53 : test_loss =  1.004021   test_acc:  0.9399\n",
      "************************************************************\n",
      "Iteration  54 : train_loss =  0.9820843   train_acc:  0.93840164\n",
      "Iteration  54 : test_loss =  0.99032587   test_acc:  0.93829\n",
      "************************************************************\n",
      "Iteration  55 : train_loss =  0.98135036   train_acc:  0.9406317\n",
      "Iteration  55 : test_loss =  0.988529   test_acc:  0.94051\n",
      "************************************************************\n",
      "Iteration  56 : train_loss =  0.968433   train_acc:  0.9394\n",
      "Iteration  56 : test_loss =  0.9768118   test_acc:  0.93935\n",
      "************************************************************\n",
      "Iteration  57 : train_loss =  0.9697422   train_acc:  0.94134665\n",
      "Iteration  57 : test_loss =  0.9773939   test_acc:  0.94133\n",
      "************************************************************\n",
      "Iteration  58 : train_loss =  0.95640033   train_acc:  0.94022834\n",
      "Iteration  58 : test_loss =  0.965116   test_acc:  0.94019\n",
      "************************************************************\n",
      "Iteration  59 : train_loss =  0.95492786   train_acc:  0.94198\n",
      "Iteration  59 : test_loss =  0.9628996   test_acc:  0.94203\n",
      "************************************************************\n",
      "Iteration  60 : train_loss =  0.9448363   train_acc:  0.94096\n",
      "Iteration  60 : test_loss =  0.9539024   test_acc:  0.94082\n",
      "************************************************************\n",
      "Iteration  61 : train_loss =  0.9416234   train_acc:  0.94257164\n",
      "Iteration  61 : test_loss =  0.94991565   test_acc:  0.94272\n",
      "************************************************************\n",
      "Iteration  62 : train_loss =  0.93345165   train_acc:  0.94162\n",
      "Iteration  62 : test_loss =  0.94292736   test_acc:  0.94139\n",
      "************************************************************\n",
      "Iteration  63 : train_loss =  0.92876524   train_acc:  0.94320834\n",
      "Iteration  63 : test_loss =  0.9373906   test_acc:  0.94327\n",
      "************************************************************\n",
      "Iteration  64 : train_loss =  0.92214274   train_acc:  0.94222164\n",
      "Iteration  64 : test_loss =  0.9320564   test_acc:  0.94194\n",
      "************************************************************\n",
      "Iteration  65 : train_loss =  0.9175862   train_acc:  0.94370335\n",
      "Iteration  65 : test_loss =  0.9264279   test_acc:  0.94384\n",
      "************************************************************\n",
      "Iteration  66 : train_loss =  0.91156936   train_acc:  0.94299334\n",
      "Iteration  66 : test_loss =  0.92195666   test_acc:  0.94251\n",
      "************************************************************\n",
      "Iteration  67 : train_loss =  0.9065727   train_acc:  0.9441417\n",
      "Iteration  67 : test_loss =  0.91567737   test_acc:  0.94442\n",
      "************************************************************\n",
      "Iteration  68 : train_loss =  0.90151143   train_acc:  0.94359165\n",
      "Iteration  68 : test_loss =  0.91226524   test_acc:  0.94311\n",
      "************************************************************\n",
      "Iteration  69 : train_loss =  0.8969078   train_acc:  0.94415\n",
      "Iteration  69 : test_loss =  0.90667355   test_acc:  0.9442\n",
      "************************************************************\n",
      "Iteration  70 : train_loss =  0.89225316   train_acc:  0.94366\n",
      "Iteration  70 : test_loss =  0.90316004   test_acc:  0.94326\n",
      "************************************************************\n",
      "Iteration  71 : train_loss =  0.88867337   train_acc:  0.94382334\n",
      "Iteration  71 : test_loss =  0.8984922   test_acc:  0.94396\n",
      "************************************************************\n",
      "Iteration  72 : train_loss =  0.8814921   train_acc:  0.9443067\n",
      "Iteration  72 : test_loss =  0.89275634   test_acc:  0.94374\n",
      "************************************************************\n",
      "Iteration  73 : train_loss =  0.8799357   train_acc:  0.9434933\n",
      "Iteration  73 : test_loss =  0.8890666   test_acc:  0.94371\n",
      "************************************************************\n",
      "Iteration  74 : train_loss =  0.8762715   train_acc:  0.94363\n",
      "Iteration  74 : test_loss =  0.88829726   test_acc:  0.94299\n",
      "************************************************************\n",
      "Iteration  75 : train_loss =  0.87514746   train_acc:  0.94407\n",
      "Iteration  75 : test_loss =  0.8836777   test_acc:  0.94391\n",
      "************************************************************\n",
      "Iteration  76 : train_loss =  0.8694596   train_acc:  0.94483\n",
      "Iteration  76 : test_loss =  0.88129705   test_acc:  0.94409\n",
      "************************************************************\n",
      "Iteration  77 : train_loss =  0.8689069   train_acc:  0.94378835\n",
      "Iteration  77 : test_loss =  0.8778757   test_acc:  0.94355\n",
      "************************************************************\n",
      "Iteration  78 : train_loss =  0.86396056   train_acc:  0.94529665\n",
      "Iteration  78 : test_loss =  0.87585497   test_acc:  0.94459\n",
      "************************************************************\n",
      "Iteration  79 : train_loss =  0.8608591   train_acc:  0.94457334\n",
      "Iteration  79 : test_loss =  0.8697496   test_acc:  0.94442\n",
      "************************************************************\n",
      "Iteration  80 : train_loss =  0.85822475   train_acc:  0.94606\n",
      "Iteration  80 : test_loss =  0.8700252   test_acc:  0.94535\n",
      "************************************************************\n",
      "Iteration  81 : train_loss =  0.8551783   train_acc:  0.94422835\n",
      "Iteration  81 : test_loss =  0.8642035   test_acc:  0.94392\n",
      "************************************************************\n",
      "Iteration  82 : train_loss =  0.8517674   train_acc:  0.94649\n",
      "Iteration  82 : test_loss =  0.8636169   test_acc:  0.94587\n",
      "************************************************************\n",
      "Iteration  83 : train_loss =  0.8498189   train_acc:  0.9445083\n",
      "Iteration  83 : test_loss =  0.85894424   test_acc:  0.9443\n",
      "************************************************************\n",
      "Iteration  84 : train_loss =  0.84582126   train_acc:  0.9468283\n",
      "Iteration  84 : test_loss =  0.85779804   test_acc:  0.94628\n",
      "************************************************************\n",
      "Iteration  85 : train_loss =  0.8443481   train_acc:  0.94472164\n",
      "Iteration  85 : test_loss =  0.8536481   test_acc:  0.94461\n",
      "************************************************************\n",
      "Iteration  86 : train_loss =  0.8401526   train_acc:  0.94720834\n",
      "Iteration  86 : test_loss =  0.85228455   test_acc:  0.94666\n",
      "************************************************************\n",
      "Iteration  87 : train_loss =  0.8394012   train_acc:  0.94481\n",
      "Iteration  87 : test_loss =  0.8488658   test_acc:  0.94471\n",
      "************************************************************\n",
      "Iteration  88 : train_loss =  0.83470666   train_acc:  0.94753665\n",
      "Iteration  88 : test_loss =  0.8469742   test_acc:  0.94708\n",
      "************************************************************\n",
      "Iteration  89 : train_loss =  0.8345275   train_acc:  0.9451283\n",
      "Iteration  89 : test_loss =  0.8441002   test_acc:  0.94492\n",
      "************************************************************\n",
      "Iteration  90 : train_loss =  0.8294389   train_acc:  0.9478617\n",
      "Iteration  90 : test_loss =  0.8418458   test_acc:  0.94736\n",
      "************************************************************\n",
      "Iteration  91 : train_loss =  0.8298197   train_acc:  0.94526833\n",
      "Iteration  91 : test_loss =  0.83960664   test_acc:  0.94512\n",
      "************************************************************\n",
      "Iteration  92 : train_loss =  0.82448256   train_acc:  0.9481817\n",
      "Iteration  92 : test_loss =  0.8370172   test_acc:  0.94768\n",
      "************************************************************\n",
      "Iteration  93 : train_loss =  0.8252311   train_acc:  0.9454733\n",
      "Iteration  93 : test_loss =  0.83516914   test_acc:  0.94525\n",
      "************************************************************\n",
      "Iteration  94 : train_loss =  0.8197031   train_acc:  0.9485033\n",
      "Iteration  94 : test_loss =  0.8323779   test_acc:  0.94791\n",
      "************************************************************\n",
      "Iteration  95 : train_loss =  0.82118475   train_acc:  0.94596833\n",
      "Iteration  95 : test_loss =  0.83125633   test_acc:  0.94571\n",
      "************************************************************\n",
      "Iteration  96 : train_loss =  0.81497395   train_acc:  0.948545\n",
      "Iteration  96 : test_loss =  0.827833   test_acc:  0.9477\n",
      "************************************************************\n",
      "Iteration  97 : train_loss =  0.8166948   train_acc:  0.94623\n",
      "Iteration  97 : test_loss =  0.82695264   test_acc:  0.9461\n",
      "************************************************************\n",
      "Iteration  98 : train_loss =  0.8099111   train_acc:  0.94889164\n",
      "Iteration  98 : test_loss =  0.82291204   test_acc:  0.94805\n",
      "************************************************************\n",
      "Iteration  99 : train_loss =  0.8123498   train_acc:  0.9464033\n",
      "Iteration  99 : test_loss =  0.8227761   test_acc:  0.94624\n",
      "************************************************************\n",
      "Iteration  100 : train_loss =  0.80505794   train_acc:  0.9492767\n",
      "Iteration  100 : test_loss =  0.81817764   test_acc:  0.94835\n",
      "************************************************************\n",
      "Iteration  101 : train_loss =  0.8081817   train_acc:  0.94648665\n",
      "Iteration  101 : test_loss =  0.8188229   test_acc:  0.94638\n",
      "************************************************************\n",
      "Iteration  102 : train_loss =  0.8003242   train_acc:  0.9495583\n",
      "Iteration  102 : test_loss =  0.8136217   test_acc:  0.9486\n",
      "************************************************************\n",
      "Iteration  103 : train_loss =  0.80407643   train_acc:  0.94671\n",
      "Iteration  103 : test_loss =  0.81482023   test_acc:  0.94661\n",
      "************************************************************\n",
      "Iteration  104 : train_loss =  0.79621875   train_acc:  0.94986165\n",
      "Iteration  104 : test_loss =  0.80961806   test_acc:  0.94893\n",
      "************************************************************\n",
      "Iteration  105 : train_loss =  0.8000284   train_acc:  0.94684666\n",
      "Iteration  105 : test_loss =  0.81099796   test_acc:  0.94664\n",
      "************************************************************\n",
      "Iteration  106 : train_loss =  0.7916213   train_acc:  0.95012164\n",
      "Iteration  106 : test_loss =  0.8051958   test_acc:  0.94926\n",
      "************************************************************\n",
      "Iteration  107 : train_loss =  0.7962699   train_acc:  0.94699836\n",
      "Iteration  107 : test_loss =  0.8073659   test_acc:  0.94682\n",
      "************************************************************\n",
      "Iteration  108 : train_loss =  0.78736025   train_acc:  0.9504067\n",
      "Iteration  108 : test_loss =  0.80103517   test_acc:  0.94949\n",
      "************************************************************\n",
      "Iteration  109 : train_loss =  0.7922159   train_acc:  0.94716\n",
      "Iteration  109 : test_loss =  0.8035515   test_acc:  0.94704\n",
      "************************************************************\n",
      "Iteration  110 : train_loss =  0.78496933   train_acc:  0.950345\n",
      "Iteration  110 : test_loss =  0.79890794   test_acc:  0.94936\n",
      "************************************************************\n",
      "Iteration  111 : train_loss =  0.78733045   train_acc:  0.94759\n",
      "Iteration  111 : test_loss =  0.798645   test_acc:  0.94734\n",
      "************************************************************\n",
      "Iteration  112 : train_loss =  0.78002554   train_acc:  0.9507217\n",
      "Iteration  112 : test_loss =  0.79397565   test_acc:  0.94978\n",
      "************************************************************\n",
      "Iteration  113 : train_loss =  0.78314215   train_acc:  0.94743\n",
      "Iteration  113 : test_loss =  0.79486865   test_acc:  0.94718\n",
      "************************************************************\n",
      "Iteration  114 : train_loss =  0.77752626   train_acc:  0.95075\n",
      "Iteration  114 : test_loss =  0.7917328   test_acc:  0.94991\n",
      "************************************************************\n",
      "Iteration  115 : train_loss =  0.7789824   train_acc:  0.9478833\n",
      "Iteration  115 : test_loss =  0.7906115   test_acc:  0.94757\n",
      "************************************************************\n",
      "Iteration  116 : train_loss =  0.7731252   train_acc:  0.95129335\n",
      "Iteration  116 : test_loss =  0.787339   test_acc:  0.95027\n",
      "************************************************************\n",
      "Iteration  117 : train_loss =  0.7742263   train_acc:  0.9479833\n",
      "Iteration  117 : test_loss =  0.7862474   test_acc:  0.94766\n",
      "************************************************************\n",
      "Iteration  118 : train_loss =  0.77354556   train_acc:  0.95067334\n",
      "Iteration  118 : test_loss =  0.78800255   test_acc:  0.94968\n",
      "************************************************************\n",
      "Iteration  119 : train_loss =  0.7702798   train_acc:  0.9486383\n",
      "Iteration  119 : test_loss =  0.7822676   test_acc:  0.9481\n",
      "************************************************************\n",
      "Iteration  120 : train_loss =  0.7723849   train_acc:  0.9479167\n",
      "Iteration  120 : test_loss =  0.7867257   test_acc:  0.94709\n",
      "************************************************************\n",
      "Iteration  121 : train_loss =  0.7707401   train_acc:  0.94902664\n",
      "Iteration  121 : test_loss =  0.78425753   test_acc:  0.9489\n",
      "************************************************************\n",
      "Iteration  122 : train_loss =  0.770607   train_acc:  0.946065\n",
      "Iteration  122 : test_loss =  0.7849151   test_acc:  0.94569\n",
      "************************************************************\n",
      "Iteration  123 : train_loss =  0.766819   train_acc:  0.9493117\n",
      "Iteration  123 : test_loss =  0.7796665   test_acc:  0.94883\n",
      "************************************************************\n",
      "Iteration  124 : train_loss =  0.7630329   train_acc:  0.9463883\n",
      "Iteration  124 : test_loss =  0.7768973   test_acc:  0.94617\n",
      "************************************************************\n",
      "Iteration  125 : train_loss =  0.7614678   train_acc:  0.94954\n",
      "Iteration  125 : test_loss =  0.7742631   test_acc:  0.94921\n",
      "************************************************************\n",
      "Iteration  126 : train_loss =  0.7606225   train_acc:  0.9463417\n",
      "Iteration  126 : test_loss =  0.7738981   test_acc:  0.94597\n",
      "************************************************************\n",
      "Iteration  127 : train_loss =  0.7577461   train_acc:  0.9496167\n",
      "Iteration  127 : test_loss =  0.77091897   test_acc:  0.94948\n",
      "************************************************************\n",
      "Iteration  128 : train_loss =  0.7541291   train_acc:  0.94665664\n",
      "Iteration  128 : test_loss =  0.76756316   test_acc:  0.94613\n",
      "************************************************************\n",
      "Iteration  129 : train_loss =  0.7536891   train_acc:  0.94978833\n",
      "Iteration  129 : test_loss =  0.7667994   test_acc:  0.94959\n",
      "************************************************************\n",
      "Iteration  130 : train_loss =  0.7476794   train_acc:  0.94702333\n",
      "Iteration  130 : test_loss =  0.7611074   test_acc:  0.94637\n",
      "************************************************************\n",
      "Iteration  131 : train_loss =  0.7497963   train_acc:  0.9499033\n",
      "Iteration  131 : test_loss =  0.7628959   test_acc:  0.94964\n",
      "************************************************************\n",
      "Iteration  132 : train_loss =  0.74287385   train_acc:  0.947325\n",
      "Iteration  132 : test_loss =  0.75630605   test_acc:  0.94662\n",
      "************************************************************\n",
      "Iteration  133 : train_loss =  0.74502957   train_acc:  0.95014834\n",
      "Iteration  133 : test_loss =  0.7584486   test_acc:  0.94986\n",
      "************************************************************\n",
      "Iteration  134 : train_loss =  0.74457186   train_acc:  0.94705164\n",
      "Iteration  134 : test_loss =  0.75837225   test_acc:  0.94629\n",
      "************************************************************\n",
      "Iteration  135 : train_loss =  0.7414624   train_acc:  0.9503317\n",
      "Iteration  135 : test_loss =  0.75509775   test_acc:  0.94999\n",
      "************************************************************\n",
      "Iteration  136 : train_loss =  0.73962474   train_acc:  0.94731164\n",
      "Iteration  136 : test_loss =  0.753509   test_acc:  0.94658\n",
      "************************************************************\n",
      "Iteration  137 : train_loss =  0.7380339   train_acc:  0.950475\n",
      "Iteration  137 : test_loss =  0.7517835   test_acc:  0.95026\n",
      "************************************************************\n",
      "Iteration  138 : train_loss =  0.73494923   train_acc:  0.94759834\n",
      "Iteration  138 : test_loss =  0.74892116   test_acc:  0.94677\n",
      "************************************************************\n",
      "Iteration  139 : train_loss =  0.7350813   train_acc:  0.9505017\n",
      "Iteration  139 : test_loss =  0.74897313   test_acc:  0.95035\n",
      "************************************************************\n",
      "Iteration  140 : train_loss =  0.7305836   train_acc:  0.94787335\n",
      "Iteration  140 : test_loss =  0.74462825   test_acc:  0.94703\n",
      "************************************************************\n",
      "Iteration  141 : train_loss =  0.7318224   train_acc:  0.95068\n",
      "Iteration  141 : test_loss =  0.7457124   test_acc:  0.95053\n",
      "************************************************************\n",
      "Iteration  142 : train_loss =  0.72620153   train_acc:  0.9481533\n",
      "Iteration  142 : test_loss =  0.7403419   test_acc:  0.94729\n",
      "************************************************************\n",
      "Iteration  143 : train_loss =  0.7287153   train_acc:  0.9508333\n",
      "Iteration  143 : test_loss =  0.74267566   test_acc:  0.95072\n",
      "************************************************************\n",
      "Iteration  144 : train_loss =  0.7218267   train_acc:  0.94839\n",
      "Iteration  144 : test_loss =  0.73608214   test_acc:  0.94751\n",
      "************************************************************\n",
      "Iteration  145 : train_loss =  0.7256772   train_acc:  0.950975\n",
      "Iteration  145 : test_loss =  0.73971677   test_acc:  0.95091\n",
      "************************************************************\n",
      "Iteration  146 : train_loss =  0.7175303   train_acc:  0.948705\n",
      "Iteration  146 : test_loss =  0.73185617   test_acc:  0.94758\n",
      "************************************************************\n",
      "Iteration  147 : train_loss =  0.7225857   train_acc:  0.95113164\n",
      "Iteration  147 : test_loss =  0.7366882   test_acc:  0.95103\n",
      "************************************************************\n",
      "Iteration  148 : train_loss =  0.7123219   train_acc:  0.949015\n",
      "Iteration  148 : test_loss =  0.72691154   test_acc:  0.94771\n",
      "************************************************************\n",
      "Iteration  149 : train_loss =  0.7208024   train_acc:  0.95121\n",
      "Iteration  149 : test_loss =  0.73515546   test_acc:  0.95097\n",
      "************************************************************\n",
      "Iteration  150 : train_loss =  0.7083069   train_acc:  0.94905835\n",
      "Iteration  150 : test_loss =  0.72304606   test_acc:  0.94786\n",
      "************************************************************\n",
      "Iteration  151 : train_loss =  0.7177521   train_acc:  0.95120835\n",
      "Iteration  151 : test_loss =  0.7324738   test_acc:  0.95092\n",
      "************************************************************\n",
      "Iteration  152 : train_loss =  0.7073742   train_acc:  0.94904166\n",
      "Iteration  152 : test_loss =  0.722385   test_acc:  0.94807\n",
      "************************************************************\n",
      "Iteration  153 : train_loss =  0.71479326   train_acc:  0.95148164\n",
      "Iteration  153 : test_loss =  0.72968113   test_acc:  0.95112\n",
      "************************************************************\n",
      "Iteration  154 : train_loss =  0.7035491   train_acc:  0.94929\n",
      "Iteration  154 : test_loss =  0.718674   test_acc:  0.94826\n",
      "************************************************************\n",
      "Iteration  155 : train_loss =  0.7125817   train_acc:  0.951615\n",
      "Iteration  155 : test_loss =  0.7276235   test_acc:  0.9512\n",
      "************************************************************\n",
      "Iteration  156 : train_loss =  0.7000967   train_acc:  0.94956666\n",
      "Iteration  156 : test_loss =  0.71529484   test_acc:  0.94839\n",
      "************************************************************\n",
      "Iteration  157 : train_loss =  0.7100832   train_acc:  0.95175\n",
      "Iteration  157 : test_loss =  0.72507775   test_acc:  0.95134\n",
      "************************************************************\n",
      "Iteration  158 : train_loss =  0.6965708   train_acc:  0.94970167\n",
      "Iteration  158 : test_loss =  0.7120531   test_acc:  0.9485\n",
      "************************************************************\n",
      "Iteration  159 : train_loss =  0.7071056   train_acc:  0.951845\n",
      "Iteration  159 : test_loss =  0.72201395   test_acc:  0.95143\n",
      "************************************************************\n",
      "Iteration  160 : train_loss =  0.6932307   train_acc:  0.949835\n",
      "Iteration  160 : test_loss =  0.7089069   test_acc:  0.94858\n",
      "************************************************************\n",
      "Iteration  161 : train_loss =  0.7044975   train_acc:  0.95190334\n",
      "Iteration  161 : test_loss =  0.71941733   test_acc:  0.95153\n",
      "************************************************************\n",
      "Iteration  162 : train_loss =  0.6906894   train_acc:  0.9499933\n",
      "Iteration  162 : test_loss =  0.7066973   test_acc:  0.94882\n",
      "************************************************************\n",
      "Iteration  163 : train_loss =  0.7018824   train_acc:  0.95218\n",
      "Iteration  163 : test_loss =  0.7169066   test_acc:  0.95167\n",
      "************************************************************\n",
      "Iteration  164 : train_loss =  0.68730754   train_acc:  0.95013833\n",
      "Iteration  164 : test_loss =  0.7034352   test_acc:  0.94892\n",
      "************************************************************\n",
      "Iteration  165 : train_loss =  0.6993884   train_acc:  0.95226836\n",
      "Iteration  165 : test_loss =  0.7144631   test_acc:  0.95176\n",
      "************************************************************\n",
      "Iteration  166 : train_loss =  0.68449265   train_acc:  0.950305\n",
      "Iteration  166 : test_loss =  0.7007742   test_acc:  0.94905\n",
      "************************************************************\n",
      "Iteration  167 : train_loss =  0.6970336   train_acc:  0.952305\n",
      "Iteration  167 : test_loss =  0.712367   test_acc:  0.95178\n",
      "************************************************************\n",
      "Iteration  168 : train_loss =  0.68176335   train_acc:  0.95045835\n",
      "Iteration  168 : test_loss =  0.6982003   test_acc:  0.94924\n",
      "************************************************************\n",
      "Iteration  169 : train_loss =  0.6947106   train_acc:  0.95244\n",
      "Iteration  169 : test_loss =  0.7102379   test_acc:  0.95193\n",
      "************************************************************\n",
      "Iteration  170 : train_loss =  0.6790893   train_acc:  0.95066667\n",
      "Iteration  170 : test_loss =  0.6956784   test_acc:  0.94937\n",
      "************************************************************\n",
      "Iteration  171 : train_loss =  0.6920526   train_acc:  0.952725\n",
      "Iteration  171 : test_loss =  0.7076994   test_acc:  0.95208\n",
      "************************************************************\n",
      "Iteration  172 : train_loss =  0.67543477   train_acc:  0.95088\n",
      "Iteration  172 : test_loss =  0.6919993   test_acc:  0.94968\n",
      "************************************************************\n",
      "Iteration  173 : train_loss =  0.6921321   train_acc:  0.95270336\n",
      "Iteration  173 : test_loss =  0.708194   test_acc:  0.95212\n",
      "************************************************************\n",
      "Iteration  174 : train_loss =  0.6715099   train_acc:  0.950945\n",
      "Iteration  174 : test_loss =  0.6885296   test_acc:  0.94987\n",
      "************************************************************\n",
      "Iteration  175 : train_loss =  0.6897664   train_acc:  0.9528133\n",
      "Iteration  175 : test_loss =  0.7060382   test_acc:  0.95211\n",
      "************************************************************\n",
      "Iteration  176 : train_loss =  0.6685759   train_acc:  0.9512283\n",
      "Iteration  176 : test_loss =  0.6856939   test_acc:  0.95019\n",
      "************************************************************\n",
      "Iteration  177 : train_loss =  0.687309   train_acc:  0.9529\n",
      "Iteration  177 : test_loss =  0.7038784   test_acc:  0.95215\n",
      "************************************************************\n",
      "Iteration  178 : train_loss =  0.6657318   train_acc:  0.95139\n",
      "Iteration  178 : test_loss =  0.6828505   test_acc:  0.95033\n",
      "************************************************************\n",
      "Iteration  179 : train_loss =  0.68509924   train_acc:  0.95299\n",
      "Iteration  179 : test_loss =  0.7019423   test_acc:  0.95222\n",
      "************************************************************\n",
      "Iteration  180 : train_loss =  0.6635996   train_acc:  0.951545\n",
      "Iteration  180 : test_loss =  0.68061906   test_acc:  0.95045\n",
      "************************************************************\n",
      "Iteration  181 : train_loss =  0.6832792   train_acc:  0.9531083\n",
      "Iteration  181 : test_loss =  0.70021164   test_acc:  0.9524\n",
      "************************************************************\n",
      "Iteration  182 : train_loss =  0.661713   train_acc:  0.951735\n",
      "Iteration  182 : test_loss =  0.679171   test_acc:  0.95055\n",
      "************************************************************\n",
      "Iteration  183 : train_loss =  0.68164754   train_acc:  0.95325667\n",
      "Iteration  183 : test_loss =  0.698762   test_acc:  0.95255\n",
      "************************************************************\n",
      "Iteration  184 : train_loss =  0.6596496   train_acc:  0.95184165\n",
      "Iteration  184 : test_loss =  0.6772625   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  185 : train_loss =  0.6799921   train_acc:  0.95339334\n",
      "Iteration  185 : test_loss =  0.6971944   test_acc:  0.95277\n",
      "************************************************************\n",
      "Iteration  186 : train_loss =  0.6577401   train_acc:  0.9519567\n",
      "Iteration  186 : test_loss =  0.6753992   test_acc:  0.9508\n",
      "************************************************************\n",
      "Iteration  187 : train_loss =  0.6783518   train_acc:  0.95343\n",
      "Iteration  187 : test_loss =  0.6957021   test_acc:  0.95267\n",
      "************************************************************\n",
      "Iteration  188 : train_loss =  0.6562536   train_acc:  0.95202833\n",
      "Iteration  188 : test_loss =  0.6740521   test_acc:  0.9509\n",
      "************************************************************\n",
      "Iteration  189 : train_loss =  0.676712   train_acc:  0.95362\n",
      "Iteration  189 : test_loss =  0.69414884   test_acc:  0.95302\n",
      "************************************************************\n",
      "Iteration  190 : train_loss =  0.65404403   train_acc:  0.9521217\n",
      "Iteration  190 : test_loss =  0.67193806   test_acc:  0.95104\n",
      "************************************************************\n",
      "Iteration  191 : train_loss =  0.6751582   train_acc:  0.953725\n",
      "Iteration  191 : test_loss =  0.69268394   test_acc:  0.95314\n",
      "************************************************************\n",
      "Iteration  192 : train_loss =  0.6522764   train_acc:  0.95224\n",
      "Iteration  192 : test_loss =  0.6702904   test_acc:  0.9511\n",
      "************************************************************\n",
      "Iteration  193 : train_loss =  0.67362225   train_acc:  0.95374\n",
      "Iteration  193 : test_loss =  0.69130105   test_acc:  0.95316\n",
      "************************************************************\n",
      "Iteration  194 : train_loss =  0.650827   train_acc:  0.9522783\n",
      "Iteration  194 : test_loss =  0.6689731   test_acc:  0.95122\n",
      "************************************************************\n",
      "Iteration  195 : train_loss =  0.6722969   train_acc:  0.9539033\n",
      "Iteration  195 : test_loss =  0.6900292   test_acc:  0.95328\n",
      "************************************************************\n",
      "Iteration  196 : train_loss =  0.6487288   train_acc:  0.9523583\n",
      "Iteration  196 : test_loss =  0.6669992   test_acc:  0.95111\n",
      "************************************************************\n",
      "Iteration  197 : train_loss =  0.67080235   train_acc:  0.95398\n",
      "Iteration  197 : test_loss =  0.68863916   test_acc:  0.95339\n",
      "************************************************************\n",
      "Iteration  198 : train_loss =  0.64697164   train_acc:  0.952495\n",
      "Iteration  198 : test_loss =  0.66540635   test_acc:  0.95134\n",
      "************************************************************\n",
      "Iteration  199 : train_loss =  0.6694206   train_acc:  0.95405\n",
      "Iteration  199 : test_loss =  0.6874172   test_acc:  0.95342\n",
      "************************************************************\n",
      "Iteration  200 : train_loss =  0.6453194   train_acc:  0.952615\n",
      "Iteration  200 : test_loss =  0.66388375   test_acc:  0.95155\n",
      "************************************************************\n",
      "Iteration  201 : train_loss =  0.66806895   train_acc:  0.954215\n",
      "Iteration  201 : test_loss =  0.68616265   test_acc:  0.95356\n",
      "************************************************************\n",
      "Iteration  202 : train_loss =  0.6437039   train_acc:  0.9526917\n",
      "Iteration  202 : test_loss =  0.6624263   test_acc:  0.95162\n",
      "************************************************************\n",
      "Iteration  203 : train_loss =  0.66671044   train_acc:  0.9542367\n",
      "Iteration  203 : test_loss =  0.68495584   test_acc:  0.95359\n",
      "************************************************************\n",
      "Iteration  204 : train_loss =  0.64208215   train_acc:  0.95276165\n",
      "Iteration  204 : test_loss =  0.6609055   test_acc:  0.95182\n",
      "************************************************************\n",
      "Iteration  205 : train_loss =  0.665365   train_acc:  0.95438164\n",
      "Iteration  205 : test_loss =  0.6837112   test_acc:  0.95375\n",
      "************************************************************\n",
      "Iteration  206 : train_loss =  0.6404681   train_acc:  0.9528817\n",
      "Iteration  206 : test_loss =  0.6594498   test_acc:  0.95184\n",
      "************************************************************\n",
      "Iteration  207 : train_loss =  0.664013   train_acc:  0.95439\n",
      "Iteration  207 : test_loss =  0.68249875   test_acc:  0.95375\n",
      "************************************************************\n",
      "Iteration  208 : train_loss =  0.6389857   train_acc:  0.95300835\n",
      "Iteration  208 : test_loss =  0.65806305   test_acc:  0.95194\n",
      "************************************************************\n",
      "Iteration  209 : train_loss =  0.6626875   train_acc:  0.95452666\n",
      "Iteration  209 : test_loss =  0.68127286   test_acc:  0.95385\n",
      "************************************************************\n",
      "Iteration  210 : train_loss =  0.63743323   train_acc:  0.95309\n",
      "Iteration  210 : test_loss =  0.6566621   test_acc:  0.95195\n",
      "************************************************************\n",
      "Iteration  211 : train_loss =  0.6613762   train_acc:  0.95451164\n",
      "Iteration  211 : test_loss =  0.68010545   test_acc:  0.95391\n",
      "************************************************************\n",
      "Iteration  212 : train_loss =  0.63593864   train_acc:  0.95322835\n",
      "Iteration  212 : test_loss =  0.6552871   test_acc:  0.95215\n",
      "************************************************************\n",
      "Iteration  213 : train_loss =  0.660063   train_acc:  0.95464\n",
      "Iteration  213 : test_loss =  0.67888486   test_acc:  0.95402\n",
      "************************************************************\n",
      "Iteration  214 : train_loss =  0.6342648   train_acc:  0.953295\n",
      "Iteration  214 : test_loss =  0.65377635   test_acc:  0.95212\n",
      "************************************************************\n",
      "Iteration  215 : train_loss =  0.6592067   train_acc:  0.954715\n",
      "Iteration  215 : test_loss =  0.67811906   test_acc:  0.95424\n",
      "************************************************************\n",
      "Iteration  216 : train_loss =  0.63282156   train_acc:  0.95340836\n",
      "Iteration  216 : test_loss =  0.65245587   test_acc:  0.95232\n",
      "************************************************************\n",
      "Iteration  217 : train_loss =  0.6579479   train_acc:  0.95477664\n",
      "Iteration  217 : test_loss =  0.6770717   test_acc:  0.95436\n",
      "************************************************************\n",
      "Iteration  218 : train_loss =  0.6313532   train_acc:  0.95355165\n",
      "Iteration  218 : test_loss =  0.6510934   test_acc:  0.95241\n",
      "************************************************************\n",
      "Iteration  219 : train_loss =  0.65665597   train_acc:  0.9548517\n",
      "Iteration  219 : test_loss =  0.67588836   test_acc:  0.95439\n",
      "************************************************************\n",
      "Iteration  220 : train_loss =  0.6299633   train_acc:  0.9536433\n",
      "Iteration  220 : test_loss =  0.6498003   test_acc:  0.95245\n",
      "************************************************************\n",
      "Iteration  221 : train_loss =  0.65538865   train_acc:  0.9549367\n",
      "Iteration  221 : test_loss =  0.6747163   test_acc:  0.95447\n",
      "************************************************************\n",
      "Iteration  222 : train_loss =  0.6286121   train_acc:  0.953745\n",
      "Iteration  222 : test_loss =  0.6486127   test_acc:  0.95251\n",
      "************************************************************\n",
      "Iteration  223 : train_loss =  0.65409493   train_acc:  0.95506334\n",
      "Iteration  223 : test_loss =  0.6734724   test_acc:  0.95447\n",
      "************************************************************\n",
      "Iteration  224 : train_loss =  0.6270525   train_acc:  0.953795\n",
      "Iteration  224 : test_loss =  0.64723825   test_acc:  0.95254\n",
      "************************************************************\n",
      "Iteration  225 : train_loss =  0.65281415   train_acc:  0.95508164\n",
      "Iteration  225 : test_loss =  0.6723139   test_acc:  0.9546\n",
      "************************************************************\n",
      "Iteration  226 : train_loss =  0.62574154   train_acc:  0.9538867\n",
      "Iteration  226 : test_loss =  0.6460404   test_acc:  0.95259\n",
      "************************************************************\n",
      "Iteration  227 : train_loss =  0.65157485   train_acc:  0.95523167\n",
      "Iteration  227 : test_loss =  0.6711424   test_acc:  0.95471\n",
      "************************************************************\n",
      "Iteration  228 : train_loss =  0.62417567   train_acc:  0.95397335\n",
      "Iteration  228 : test_loss =  0.6446121   test_acc:  0.95265\n",
      "************************************************************\n",
      "Iteration  229 : train_loss =  0.6503587   train_acc:  0.9552983\n",
      "Iteration  229 : test_loss =  0.67000705   test_acc:  0.9548\n",
      "************************************************************\n",
      "Iteration  230 : train_loss =  0.62279975   train_acc:  0.9540383\n",
      "Iteration  230 : test_loss =  0.6433585   test_acc:  0.95274\n",
      "************************************************************\n",
      "Iteration  231 : train_loss =  0.64911824   train_acc:  0.9553433\n",
      "Iteration  231 : test_loss =  0.6688797   test_acc:  0.95481\n",
      "************************************************************\n",
      "Iteration  232 : train_loss =  0.62147915   train_acc:  0.954135\n",
      "Iteration  232 : test_loss =  0.6421452   test_acc:  0.95279\n",
      "************************************************************\n",
      "Iteration  233 : train_loss =  0.64795274   train_acc:  0.9554567\n",
      "Iteration  233 : test_loss =  0.66781145   test_acc:  0.95503\n",
      "************************************************************\n",
      "Iteration  234 : train_loss =  0.6200213   train_acc:  0.95420164\n",
      "Iteration  234 : test_loss =  0.640808   test_acc:  0.95287\n",
      "************************************************************\n",
      "Iteration  235 : train_loss =  0.646775   train_acc:  0.95551336\n",
      "Iteration  235 : test_loss =  0.66672903   test_acc:  0.95505\n",
      "************************************************************\n",
      "Iteration  236 : train_loss =  0.6186749   train_acc:  0.9542583\n",
      "Iteration  236 : test_loss =  0.63957536   test_acc:  0.95289\n",
      "************************************************************\n",
      "Iteration  237 : train_loss =  0.6456188   train_acc:  0.9555783\n",
      "Iteration  237 : test_loss =  0.66570467   test_acc:  0.95504\n",
      "************************************************************\n",
      "Iteration  238 : train_loss =  0.6175896   train_acc:  0.95438164\n",
      "Iteration  238 : test_loss =  0.6385624   test_acc:  0.95299\n",
      "************************************************************\n",
      "Iteration  239 : train_loss =  0.6444425   train_acc:  0.95564\n",
      "Iteration  239 : test_loss =  0.6645791   test_acc:  0.95517\n",
      "************************************************************\n",
      "Iteration  240 : train_loss =  0.6161521   train_acc:  0.9544967\n",
      "Iteration  240 : test_loss =  0.6372531   test_acc:  0.95317\n",
      "************************************************************\n",
      "Iteration  241 : train_loss =  0.6432313   train_acc:  0.955725\n",
      "Iteration  241 : test_loss =  0.6634429   test_acc:  0.95515\n",
      "************************************************************\n",
      "Iteration  242 : train_loss =  0.61473596   train_acc:  0.954555\n",
      "Iteration  242 : test_loss =  0.6359515   test_acc:  0.95322\n",
      "************************************************************\n",
      "Iteration  243 : train_loss =  0.6420905   train_acc:  0.95579165\n",
      "Iteration  243 : test_loss =  0.6623877   test_acc:  0.95529\n",
      "************************************************************\n",
      "Iteration  244 : train_loss =  0.61344755   train_acc:  0.9546267\n",
      "Iteration  244 : test_loss =  0.6347655   test_acc:  0.95323\n",
      "************************************************************\n",
      "Iteration  245 : train_loss =  0.64093786   train_acc:  0.95585\n",
      "Iteration  245 : test_loss =  0.661319   test_acc:  0.95533\n",
      "************************************************************\n",
      "Iteration  246 : train_loss =  0.61218756   train_acc:  0.95471\n",
      "Iteration  246 : test_loss =  0.6336028   test_acc:  0.95326\n",
      "************************************************************\n",
      "Iteration  247 : train_loss =  0.6397941   train_acc:  0.95593\n",
      "Iteration  247 : test_loss =  0.6602562   test_acc:  0.95534\n",
      "************************************************************\n",
      "Iteration  248 : train_loss =  0.61094284   train_acc:  0.9547517\n",
      "Iteration  248 : test_loss =  0.63245213   test_acc:  0.95334\n",
      "************************************************************\n",
      "Iteration  249 : train_loss =  0.63865083   train_acc:  0.95598334\n",
      "Iteration  249 : test_loss =  0.65920466   test_acc:  0.95545\n",
      "************************************************************\n",
      "Iteration  250 : train_loss =  0.6097025   train_acc:  0.95480335\n",
      "Iteration  250 : test_loss =  0.6313162   test_acc:  0.95345\n",
      "************************************************************\n",
      "Iteration  251 : train_loss =  0.63754374   train_acc:  0.95603836\n",
      "Iteration  251 : test_loss =  0.6581761   test_acc:  0.95553\n",
      "************************************************************\n",
      "Iteration  252 : train_loss =  0.6084646   train_acc:  0.95487165\n",
      "Iteration  252 : test_loss =  0.6301782   test_acc:  0.95352\n",
      "************************************************************\n",
      "Iteration  253 : train_loss =  0.6364563   train_acc:  0.95611835\n",
      "Iteration  253 : test_loss =  0.6571922   test_acc:  0.95567\n",
      "************************************************************\n",
      "Iteration  254 : train_loss =  0.6076023   train_acc:  0.95489335\n",
      "Iteration  254 : test_loss =  0.62947446   test_acc:  0.95351\n",
      "************************************************************\n",
      "Iteration  255 : train_loss =  0.63533276   train_acc:  0.95616\n",
      "Iteration  255 : test_loss =  0.656107   test_acc:  0.95571\n",
      "************************************************************\n",
      "Iteration  256 : train_loss =  0.6063797   train_acc:  0.95498\n",
      "Iteration  256 : test_loss =  0.6283209   test_acc:  0.95354\n",
      "************************************************************\n",
      "Iteration  257 : train_loss =  0.63423306   train_acc:  0.95623165\n",
      "Iteration  257 : test_loss =  0.6551001   test_acc:  0.95576\n",
      "************************************************************\n",
      "Iteration  258 : train_loss =  0.605164   train_acc:  0.9550433\n",
      "Iteration  258 : test_loss =  0.62719876   test_acc:  0.95367\n",
      "************************************************************\n",
      "Iteration  259 : train_loss =  0.63315207   train_acc:  0.9563133\n",
      "Iteration  259 : test_loss =  0.6541108   test_acc:  0.95581\n",
      "************************************************************\n",
      "Iteration  260 : train_loss =  0.6039444   train_acc:  0.95512664\n",
      "Iteration  260 : test_loss =  0.626079   test_acc:  0.9537\n",
      "************************************************************\n",
      "Iteration  261 : train_loss =  0.6320749   train_acc:  0.95636666\n",
      "Iteration  261 : test_loss =  0.6531291   test_acc:  0.95593\n",
      "************************************************************\n",
      "Iteration  262 : train_loss =  0.6027232   train_acc:  0.95521665\n",
      "Iteration  262 : test_loss =  0.624949   test_acc:  0.95369\n",
      "************************************************************\n",
      "Iteration  263 : train_loss =  0.63101184   train_acc:  0.9564367\n",
      "Iteration  263 : test_loss =  0.652153   test_acc:  0.95596\n",
      "************************************************************\n",
      "Iteration  264 : train_loss =  0.6015412   train_acc:  0.95531166\n",
      "Iteration  264 : test_loss =  0.62386894   test_acc:  0.95379\n",
      "************************************************************\n",
      "Iteration  265 : train_loss =  0.6299853   train_acc:  0.9565117\n",
      "Iteration  265 : test_loss =  0.651229   test_acc:  0.956\n",
      "************************************************************\n",
      "Iteration  266 : train_loss =  0.6003803   train_acc:  0.9553933\n",
      "Iteration  266 : test_loss =  0.6228115   test_acc:  0.95386\n",
      "************************************************************\n",
      "Iteration  267 : train_loss =  0.6289367   train_acc:  0.95658\n",
      "Iteration  267 : test_loss =  0.6502836   test_acc:  0.95606\n",
      "************************************************************\n",
      "Iteration  268 : train_loss =  0.5991092   train_acc:  0.95545\n",
      "Iteration  268 : test_loss =  0.6216189   test_acc:  0.95387\n",
      "************************************************************\n",
      "Iteration  269 : train_loss =  0.6285702   train_acc:  0.9565617\n",
      "Iteration  269 : test_loss =  0.64992785   test_acc:  0.95598\n",
      "************************************************************\n",
      "Iteration  270 : train_loss =  0.5978294   train_acc:  0.9555333\n",
      "Iteration  270 : test_loss =  0.6204205   test_acc:  0.95395\n",
      "************************************************************\n",
      "Iteration  271 : train_loss =  0.627464   train_acc:  0.95666665\n",
      "Iteration  271 : test_loss =  0.64891   test_acc:  0.95614\n",
      "************************************************************\n",
      "Iteration  272 : train_loss =  0.5966151   train_acc:  0.95561165\n",
      "Iteration  272 : test_loss =  0.6193298   test_acc:  0.95401\n",
      "************************************************************\n",
      "Iteration  273 : train_loss =  0.62639076   train_acc:  0.95675665\n",
      "Iteration  273 : test_loss =  0.64795554   test_acc:  0.95613\n",
      "************************************************************\n",
      "Iteration  274 : train_loss =  0.59547776   train_acc:  0.95571\n",
      "Iteration  274 : test_loss =  0.61831367   test_acc:  0.95409\n",
      "************************************************************\n",
      "Iteration  275 : train_loss =  0.6253655   train_acc:  0.956855\n",
      "Iteration  275 : test_loss =  0.6470546   test_acc:  0.9562\n",
      "************************************************************\n",
      "Iteration  276 : train_loss =  0.59437615   train_acc:  0.9557533\n",
      "Iteration  276 : test_loss =  0.61733663   test_acc:  0.95418\n",
      "************************************************************\n",
      "Iteration  277 : train_loss =  0.624326   train_acc:  0.956915\n",
      "Iteration  277 : test_loss =  0.64611745   test_acc:  0.95624\n",
      "************************************************************\n",
      "Iteration  278 : train_loss =  0.5932868   train_acc:  0.955815\n",
      "Iteration  278 : test_loss =  0.61635196   test_acc:  0.95427\n",
      "************************************************************\n",
      "Iteration  279 : train_loss =  0.623316   train_acc:  0.95697665\n",
      "Iteration  279 : test_loss =  0.6452167   test_acc:  0.95626\n",
      "************************************************************\n",
      "Iteration  280 : train_loss =  0.5922173   train_acc:  0.95587665\n",
      "Iteration  280 : test_loss =  0.61538947   test_acc:  0.95432\n",
      "************************************************************\n",
      "Iteration  281 : train_loss =  0.6222912   train_acc:  0.9570367\n",
      "Iteration  281 : test_loss =  0.6442963   test_acc:  0.95634\n",
      "************************************************************\n",
      "Iteration  282 : train_loss =  0.59116304   train_acc:  0.9559417\n",
      "Iteration  282 : test_loss =  0.61444736   test_acc:  0.95436\n",
      "************************************************************\n",
      "Iteration  283 : train_loss =  0.6212807   train_acc:  0.9571133\n",
      "Iteration  283 : test_loss =  0.64339024   test_acc:  0.95631\n",
      "************************************************************\n",
      "Iteration  284 : train_loss =  0.59010583   train_acc:  0.956\n",
      "Iteration  284 : test_loss =  0.61350644   test_acc:  0.95442\n",
      "************************************************************\n",
      "Iteration  285 : train_loss =  0.62027943   train_acc:  0.9571883\n",
      "Iteration  285 : test_loss =  0.64249176   test_acc:  0.95638\n",
      "************************************************************\n",
      "Iteration  286 : train_loss =  0.5890414   train_acc:  0.9560583\n",
      "Iteration  286 : test_loss =  0.6125513   test_acc:  0.95447\n",
      "************************************************************\n",
      "Iteration  287 : train_loss =  0.6193098   train_acc:  0.9572567\n",
      "Iteration  287 : test_loss =  0.6416434   test_acc:  0.95643\n",
      "************************************************************\n",
      "Iteration  288 : train_loss =  0.5880266   train_acc:  0.95611835\n",
      "Iteration  288 : test_loss =  0.61164993   test_acc:  0.9545\n",
      "************************************************************\n",
      "Iteration  289 : train_loss =  0.61832434   train_acc:  0.957325\n",
      "Iteration  289 : test_loss =  0.64075637   test_acc:  0.9565\n",
      "************************************************************\n",
      "Iteration  290 : train_loss =  0.58700746   train_acc:  0.95615333\n",
      "Iteration  290 : test_loss =  0.61073154   test_acc:  0.95455\n",
      "************************************************************\n",
      "Iteration  291 : train_loss =  0.6173469   train_acc:  0.9573867\n",
      "Iteration  291 : test_loss =  0.63988674   test_acc:  0.95654\n",
      "************************************************************\n",
      "Iteration  292 : train_loss =  0.5860177   train_acc:  0.95619833\n",
      "Iteration  292 : test_loss =  0.6098353   test_acc:  0.95457\n",
      "************************************************************\n",
      "Iteration  293 : train_loss =  0.61638385   train_acc:  0.95745164\n",
      "Iteration  293 : test_loss =  0.63902414   test_acc:  0.95664\n",
      "************************************************************\n",
      "Iteration  294 : train_loss =  0.5850171   train_acc:  0.95625\n",
      "Iteration  294 : test_loss =  0.60892856   test_acc:  0.95462\n",
      "************************************************************\n",
      "Iteration  295 : train_loss =  0.61542207   train_acc:  0.9575267\n",
      "Iteration  295 : test_loss =  0.6381736   test_acc:  0.9567\n",
      "************************************************************\n",
      "Iteration  296 : train_loss =  0.58405405   train_acc:  0.95630336\n",
      "Iteration  296 : test_loss =  0.60807157   test_acc:  0.9547\n",
      "************************************************************\n",
      "Iteration  297 : train_loss =  0.61446786   train_acc:  0.9576167\n",
      "Iteration  297 : test_loss =  0.63731897   test_acc:  0.95679\n",
      "************************************************************\n",
      "Iteration  298 : train_loss =  0.583066   train_acc:  0.9563533\n",
      "Iteration  298 : test_loss =  0.6071842   test_acc:  0.95473\n",
      "************************************************************\n",
      "Iteration  299 : train_loss =  0.613525   train_acc:  0.95767164\n",
      "Iteration  299 : test_loss =  0.63648164   test_acc:  0.95683\n",
      "************************************************************\n",
      "Iteration  300 : train_loss =  0.58209103   train_acc:  0.9564083\n",
      "Iteration  300 : test_loss =  0.6063125   test_acc:  0.95485\n",
      "************************************************************\n",
      "Iteration  301 : train_loss =  0.6125999   train_acc:  0.957725\n",
      "Iteration  301 : test_loss =  0.63566965   test_acc:  0.95688\n",
      "************************************************************\n",
      "Iteration  302 : train_loss =  0.5810693   train_acc:  0.956505\n",
      "Iteration  302 : test_loss =  0.605398   test_acc:  0.95489\n",
      "************************************************************\n",
      "Iteration  303 : train_loss =  0.61166173   train_acc:  0.9578133\n",
      "Iteration  303 : test_loss =  0.6348352   test_acc:  0.95692\n",
      "************************************************************\n",
      "Iteration  304 : train_loss =  0.58009154   train_acc:  0.95655835\n",
      "Iteration  304 : test_loss =  0.60452616   test_acc:  0.95498\n",
      "************************************************************\n",
      "Iteration  305 : train_loss =  0.61072886   train_acc:  0.957885\n",
      "Iteration  305 : test_loss =  0.6339997   test_acc:  0.95698\n",
      "************************************************************\n",
      "Iteration  306 : train_loss =  0.5791408   train_acc:  0.95661336\n",
      "Iteration  306 : test_loss =  0.60366786   test_acc:  0.95508\n",
      "************************************************************\n",
      "Iteration  307 : train_loss =  0.6098073   train_acc:  0.9579433\n",
      "Iteration  307 : test_loss =  0.63317174   test_acc:  0.95704\n",
      "************************************************************\n",
      "Iteration  308 : train_loss =  0.5781913   train_acc:  0.956665\n",
      "Iteration  308 : test_loss =  0.6028087   test_acc:  0.9551\n",
      "************************************************************\n",
      "Iteration  309 : train_loss =  0.60887945   train_acc:  0.95797336\n",
      "Iteration  309 : test_loss =  0.6323417   test_acc:  0.95704\n",
      "************************************************************\n",
      "Iteration  310 : train_loss =  0.57727385   train_acc:  0.95673335\n",
      "Iteration  310 : test_loss =  0.6019838   test_acc:  0.95513\n",
      "************************************************************\n",
      "Iteration  311 : train_loss =  0.6079549   train_acc:  0.95802665\n",
      "Iteration  311 : test_loss =  0.6315136   test_acc:  0.95715\n",
      "************************************************************\n",
      "Iteration  312 : train_loss =  0.57634646   train_acc:  0.95679665\n",
      "Iteration  312 : test_loss =  0.60115117   test_acc:  0.95514\n",
      "************************************************************\n",
      "Iteration  313 : train_loss =  0.6070436   train_acc:  0.9580783\n",
      "Iteration  313 : test_loss =  0.63070285   test_acc:  0.95717\n",
      "************************************************************\n",
      "Iteration  314 : train_loss =  0.57539773   train_acc:  0.956845\n",
      "Iteration  314 : test_loss =  0.6002886   test_acc:  0.95525\n",
      "************************************************************\n",
      "Iteration  315 : train_loss =  0.606151   train_acc:  0.958145\n",
      "Iteration  315 : test_loss =  0.62992024   test_acc:  0.95721\n",
      "************************************************************\n",
      "Iteration  316 : train_loss =  0.5744551   train_acc:  0.9569233\n",
      "Iteration  316 : test_loss =  0.59944195   test_acc:  0.95528\n",
      "************************************************************\n",
      "Iteration  317 : train_loss =  0.60523176   train_acc:  0.9581983\n",
      "Iteration  317 : test_loss =  0.62909704   test_acc:  0.95724\n",
      "************************************************************\n",
      "Iteration  318 : train_loss =  0.5735055   train_acc:  0.9569567\n",
      "Iteration  318 : test_loss =  0.5985794   test_acc:  0.95532\n",
      "************************************************************\n",
      "Iteration  319 : train_loss =  0.6043275   train_acc:  0.9582767\n",
      "Iteration  319 : test_loss =  0.6282816   test_acc:  0.95731\n",
      "************************************************************\n",
      "Iteration  320 : train_loss =  0.5726   train_acc:  0.95702\n",
      "Iteration  320 : test_loss =  0.59776455   test_acc:  0.95542\n",
      "************************************************************\n",
      "Iteration  321 : train_loss =  0.60342526   train_acc:  0.9583283\n",
      "Iteration  321 : test_loss =  0.62746924   test_acc:  0.95737\n",
      "************************************************************\n",
      "Iteration  322 : train_loss =  0.5716903   train_acc:  0.95709336\n",
      "Iteration  322 : test_loss =  0.5969368   test_acc:  0.95548\n",
      "************************************************************\n",
      "Iteration  323 : train_loss =  0.6025268   train_acc:  0.958385\n",
      "Iteration  323 : test_loss =  0.62665725   test_acc:  0.95742\n",
      "************************************************************\n",
      "Iteration  324 : train_loss =  0.5707973   train_acc:  0.95713335\n",
      "Iteration  324 : test_loss =  0.59612435   test_acc:  0.9555\n",
      "************************************************************\n",
      "Iteration  325 : train_loss =  0.6016405   train_acc:  0.9584517\n",
      "Iteration  325 : test_loss =  0.6258581   test_acc:  0.95741\n",
      "************************************************************\n",
      "Iteration  326 : train_loss =  0.5699113   train_acc:  0.95718664\n",
      "Iteration  326 : test_loss =  0.5953289   test_acc:  0.95551\n",
      "************************************************************\n",
      "Iteration  327 : train_loss =  0.60078096   train_acc:  0.95851\n",
      "Iteration  327 : test_loss =  0.6251001   test_acc:  0.95744\n",
      "************************************************************\n",
      "Iteration  328 : train_loss =  0.56902814   train_acc:  0.957245\n",
      "Iteration  328 : test_loss =  0.5945493   test_acc:  0.9556\n",
      "************************************************************\n",
      "Iteration  329 : train_loss =  0.599902   train_acc:  0.95858\n",
      "Iteration  329 : test_loss =  0.6243095   test_acc:  0.95752\n",
      "************************************************************\n",
      "Iteration  330 : train_loss =  0.5681273   train_acc:  0.95729834\n",
      "Iteration  330 : test_loss =  0.593735   test_acc:  0.95568\n",
      "************************************************************\n",
      "Iteration  331 : train_loss =  0.5990393   train_acc:  0.958645\n",
      "Iteration  331 : test_loss =  0.6235157   test_acc:  0.95755\n",
      "************************************************************\n",
      "Iteration  332 : train_loss =  0.56724936   train_acc:  0.95735836\n",
      "Iteration  332 : test_loss =  0.59295094   test_acc:  0.95571\n",
      "************************************************************\n",
      "Iteration  333 : train_loss =  0.5981874   train_acc:  0.9586783\n",
      "Iteration  333 : test_loss =  0.62274104   test_acc:  0.95758\n",
      "************************************************************\n",
      "Iteration  334 : train_loss =  0.5663813   train_acc:  0.957405\n",
      "Iteration  334 : test_loss =  0.5921631   test_acc:  0.95575\n",
      "************************************************************\n",
      "Iteration  335 : train_loss =  0.5973336   train_acc:  0.95875\n",
      "Iteration  335 : test_loss =  0.6219667   test_acc:  0.95763\n",
      "************************************************************\n",
      "Iteration  336 : train_loss =  0.5655233   train_acc:  0.9574583\n",
      "Iteration  336 : test_loss =  0.5913853   test_acc:  0.95577\n",
      "************************************************************\n",
      "Iteration  337 : train_loss =  0.59648526   train_acc:  0.958815\n",
      "Iteration  337 : test_loss =  0.6211939   test_acc:  0.95772\n",
      "************************************************************\n",
      "Iteration  338 : train_loss =  0.5646757   train_acc:  0.95749\n",
      "Iteration  338 : test_loss =  0.5906136   test_acc:  0.95578\n",
      "************************************************************\n",
      "Iteration  339 : train_loss =  0.59564584   train_acc:  0.9588633\n",
      "Iteration  339 : test_loss =  0.6204352   test_acc:  0.95776\n",
      "************************************************************\n",
      "Iteration  340 : train_loss =  0.5638124   train_acc:  0.9575267\n",
      "Iteration  340 : test_loss =  0.5898364   test_acc:  0.95579\n",
      "************************************************************\n",
      "Iteration  341 : train_loss =  0.5948222   train_acc:  0.958895\n",
      "Iteration  341 : test_loss =  0.6197158   test_acc:  0.95783\n",
      "************************************************************\n",
      "Iteration  342 : train_loss =  0.5629593   train_acc:  0.95758665\n",
      "Iteration  342 : test_loss =  0.58908933   test_acc:  0.95579\n",
      "************************************************************\n",
      "Iteration  343 : train_loss =  0.5939832   train_acc:  0.9589767\n",
      "Iteration  343 : test_loss =  0.6189337   test_acc:  0.95784\n",
      "************************************************************\n",
      "Iteration  344 : train_loss =  0.5621274   train_acc:  0.9576383\n",
      "Iteration  344 : test_loss =  0.5883391   test_acc:  0.95585\n",
      "************************************************************\n",
      "Iteration  345 : train_loss =  0.5931545   train_acc:  0.95903665\n",
      "Iteration  345 : test_loss =  0.61818063   test_acc:  0.95793\n",
      "************************************************************\n",
      "Iteration  346 : train_loss =  0.5612921   train_acc:  0.9576833\n",
      "Iteration  346 : test_loss =  0.58757746   test_acc:  0.95594\n",
      "************************************************************\n",
      "Iteration  347 : train_loss =  0.59235144   train_acc:  0.95910835\n",
      "Iteration  347 : test_loss =  0.61745125   test_acc:  0.95799\n",
      "************************************************************\n",
      "Iteration  348 : train_loss =  0.56046367   train_acc:  0.9577133\n",
      "Iteration  348 : test_loss =  0.586811   test_acc:  0.956\n",
      "************************************************************\n",
      "Iteration  349 : train_loss =  0.591542   train_acc:  0.95915836\n",
      "Iteration  349 : test_loss =  0.6167223   test_acc:  0.95804\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzdeXhU9d3//+dZZs1km8lGwpJAZIkgWxBBsCBhERFRue3dX+/6tWpbq1ap33pXvdvbWsXSKtXaH1RtU2/t3autVStaW5dULBaq7CoGgUiAbGRfZz/L94+BKZFAFhImk3we18VFctbXmcy8z5nPOedzJNM0TQRBEIQhS451AEEQBGFgiUIvCIIwxIlCLwiCMMSJQi8IgjDEiUIvCIIwxIlCLwiCMMSp3U3Q0NDAhg0baGlpQZIkioqKWL58eadpPvnkE37yk5+QkZEBwOzZs1m9ejUAe/fu5dlnn8UwDBYtWsSqVasGYDMEQRCEM+m20CuKwle+8hXGjh2L3+/n3nvv5aKLLmLkyJGdpps0aRL33ntvp2GGYVBcXMz3vvc9PB4P9913H4WFhafN25Xq6upebkpEWloaDQ0NfZr3fIunrCDyDqR4ygrxlTeeskLf82ZnZ59xXLdNN6mpqYwdOxYAh8NBTk4OTU1NPVpxWVkZWVlZZGZmoqoqc+fOZceOHT2MLQiCIPSHXrXR19XVUV5eTn5+/mnjDh48yD333MMjjzxCRUUFAE1NTXg8nug0Ho+nxzsJQRAEoX9023RzUiAQYP369dx44404nc5O4/Ly8ti4cSN2u53du3fz6KOP8uSTT9JV7wqSJHW5/JKSEkpKSgBYt24daWlpvdmOKFVV+zzv+RZPWUHkHUjxlBXiK288ZYWBydujQq9pGuvXr2f+/PnMnj37tPGnFv4ZM2ZQXFxMW1sbHo+HxsbG6LjGxkZSU1O7XEdRURFFRUXR3/vaphZP7XHxlBVE3oEUT1mhd3lN0yQQCGAYxhkP9AaSzWYjGAye9/X21dnymqaJLMvY7fbTXsuztdF3W+hN0+Spp54iJyeHFStWdDlNS0sLycnJSJJEWVkZhmGQmJhIQkICNTU11NXV4Xa72bZtG3feeWd3qxQEYQgJBAJYLBZUtccNCP1KVVUURYnJuvuiu7yaphEIBHA4HD1fZncTHDhwgC1btjB69GjuueceAL70pS9F9+ZLlizh/fff56233kJRFKxWK2vWrEGSJBRF4aabbmLt2rUYhsHChQsZNWpUj8MJghD/DMOIWZEfilRV7fU3FGmwdlMsLq8cfETegRNPWaF3eX0+32nn9c4nVVXRNC1m6++tnuTt6jU9p8sr44Wu67z00ma2vbMt1lEEQRAGlSFT6BVF4cWOVLaU9u2bgCAIQ9M111zDu+++22nYL3/5S+67774zzrN69Wo+/PDDHg8f7IZMoQdI1zuoDQ7KlihBEGLkmmuuYdOmTZ2Gbdq0aVh1xzK0Cr0Uok63xjqGIAiDyIoVKygpKYmewKyoqKC2tpaLL76Ye++9lyuuuIKFCxfy2GOP9Wn5zc3N3HTTTRQVFbFixQpKS0sB+Oc//8nixYtZvHgxS5YsoaOjg9raWq699loWL17M5ZdfzgcffNBv23k2Q+pUeIbVpFRLiHUMQRDOwPj9LzEryvt1mdKoPOR//9oZx7vdbqZNm8a7777L0qVL2bRpEytXrkSSJL773e+SmpqKrut88YtfpLS0lIKCgl6tf/369UyePJlf//rX/OMf/+Cuu+7i7bff5qmnnuKRRx5h1qxZeL1ebDYb//u//8sXvvAF7rrrLnRdx+/3n+vm98iQOqJPc6r4VDsdre2xjiIIwiCyatWqaPPNqc02r732GkuXLmXp0qUcOHCAQ4cO9XrZ27dv57rrrgNg3rx5NDc309bWxqxZs3jwwQcpLi6mtbUVVVWZNm0aL7zwAuvXr2f//v24XK7+28izGFpH9MkO8EF9TR2u5MRYxxEE4XPOduQ9kJYtW8aDDz7Ixx9/TCAQYMqUKRw7doynn36a119/nZSUFNasWUMgEOj1ss/U1csdd9zBokWLeOedd7jqqqv4wx/+wCWXXMJLL73E3/72N+666y5uvfVW/u3f/q0/NvGshtQRfYYnCYD6+tYYJxEEYTBJSEhgzpw53H333dGj+fb2dhwOB0lJSdTX17N58+Y+LfuSSy7h5ZdfBmDbtm243W4SExM5cuQIkyZN4vbbb2fq1KmUlZVRWVlJWloaX/7yl/n3f/93Pv74437bxrMZWkf02Wmwr4Gapo5YRxEEYZBZtWoVt9xyC7/4xS8AuPDCC5k8eTILFy5k9OjRzJo1q0fLueGGG6J3+s6cOZMf//jH3H333RQVFWG323niiScA+NWvfsW2bduQZZnx48ezcOFCNm3axFNPPYWqqiQkJPCzn/1sYDb2c4bcnbE3P7udCVI7/3njon5O1P+G8t2Qg0E85Y2nrCDujB1I4s7YHrhQ9bHfTMIwjFhHEQRBGBSGXKGfkuWkyZpIXWVNrKMIgiAMCkOu0E+dlAvA/gPHYhtEEARhkBhyhX78heNx6EH213pjHUUQBGFQGFJX3QCoFpUJehOfavZYRxEEQRgUhtwRPcCkRDhmddPe3BbrKIIgCDE3JAt9wZh0TEnmQGlZrKMIghBjTU1N0c7Fpk2bxsyZM6O/h0Khs8774Ycf8v3vf79X65s9ezZNTU3nErnfDbmmG4ALCsYhHzhMaWULhbEOIwhCTLndbt5++20g0gFZQkICt956a3S8pmlnfNTh1KlTmTp16nnJOZC6LfQNDQ1s2LCBlpYWJEmiqKiI5cuXd5rmvffei3YYZLfbueWWW8jNzQXg9ttvx263I8syiqKwbt26/t+Kz3EkOBgbauTT0JD8wiIIwjlas2YNKSkp7Nu3jylTprBy5UoeeOABAoEAdrudn/70p+Tn57Nt2zaeeuopnn/+edavX09VVRXHjh2jqqqKW265hZtvvrlH66usrOTuu++mqakJt9vN448/Tk5ODq+99hqPP/44siyTlJTEyy+/zKeffspdd91FKBTCNE2eeeYZxo4de07b222hVxSFr3zlK4wdOxa/38+9997LRRddxMiRI6PTZGRk8IMf/ACXy8WePXt45plneOSRR6LjH3jgAZKSks4paG9Ncmi8GU4n6Pdj68XT0gVBGDi/2llLeXPvOw47m7xUO7cUZvZ6vsOHD/OHP/wBRVFob2/n5ZdfRlVVtmzZwo9//GN++ctfnjZPWVkZf/zjH/F6vcyfP58bbrgBi8XS7br+67/+i9WrV3P99dfz+9//nu9///v8+te/5oknnuC3v/0tI0aMoLU10kfXc889x80338y1115LKBRC1/Veb9vndXvIm5qaGt2bOBwOcnJyTmt/mjBhQrS7zQsuuIDGxsZzDnaupud5CCkWPtpZGusogiAMQitWrEBRFADa2tr4xje+weWXX86DDz7IgQMHupxn0aJF2Gw23G43aWlp1NfX92hdu3bt4pprrgHguuuuY/v27QAUFhby7W9/m9/+9rfRgl5YWMjPf/5zNmzYQGVlJY5+OFDtVRt9XV0d5eXl5Ofnn3Gad955h+nTp3catnbtWgAWL15MUVFRl/OVlJRQUlICwLp160hLS+tNtChVVUlLS2Ph4vk8dmAL24+1cUUflzXQTmaNFyLvwImnrNC7vLW1tdE28FsvyRnIWGd0cv2yLEf/JSYmRoevX7+eefPm8dxzz3Hs2DGuvfZaVFVFURQkSUJVVWRZxuFwROc5uZP4fPu+JEkoitJp+MllqKqKaZrIsoyqqqxfv55du3ZRUlLC0qVL+dvf/sZ1113HzJkzefvtt/nyl7/MT3/6U+bPn99pHTabrVfvlx4X+kAgwPr167nxxhvP2EHRvn372Lx5Mz/84Q+jwx566CHcbjetra08/PDDZGdnd/kEl6Kiok47gb528HRqZ0szpSY+CCdRV1uLfOKPMpgM5Y6sBoN4yhtPWaF3eYPBYLQoxsKpnYQZhhH9p+t6dHhraysZGRlomsbvfvc7TNNE0zR0XY/+fHK+UzscO3UZJ5mmedrwmTNn8tJLL7F69WpeeOEFZs2ahaZpHDlyJHrC98033+TYsWO0tbWRk5PDV7/6VcrLy9m3bx9z5szptI5gMHja63+2Ts16VOg1TWP9+vXMnz+f2bNndznN0aNHefrpp7nvvvtITPzXQz/cbjcAycnJzJo1i7Kysl4/qquvLspw8F6ji6rPKhg1Pve8rFMQhPjzzW9+kzVr1vDMM89w6aWXnvPyioqKkCQJgKuuuoqHHnqIu+++m6eeeip6Mhbg4Ycfpry8HNM0mTdvHhdeeCEbN27kxRdfRFVVMjIy+Pa3v33Oebrtptg0TTZs2IDL5eLGG2/scpqGhgYefPBB7rjjDiZMmBAdHggEME0Th8NBIBDg4YcfZvXq1UybNq3bYH3tpvjUI42qI1XctrWdbybXs2zF/G7mPP+G8lHcYBBPeeMpK4huigfSQHRT3O0R/YEDB9iyZQujR4/mnnvuAeBLX/pS9I+8ZMkSXnzxRTo6OvjVr34FEL2MsrW1NfpkdV3XmTdvXo+KfH8ZMXoEKe/W8ElDkGXnba2CIAiDS7eFfuLEibzwwgtnnebWW2/tdAPCSZmZmTz66KN9T3eOZFlmotTGIT0hZhkEQRBibcjfUTQ2UaHGloq3RfR7IwjC8DT0C31WCgDlh47ENoggCEKMDP1Cf8EoAA5XDa5OhgRBEM6XIV/o3WmpJIe9lLeGYx1FEAQhJoZ8oZckiTzaOSweRCIIw9L57qYYIjeP5uTk8O677/Yxdf8akt0Uf95YJ2wKugn5/FidooMzQRhOYtFN8SuvvMLFF1/MK6+8woIFC/qUuz8Nj0KfmYheqXD00FEumDox1nEEQYixgeym2DRNXn/9dX73u99x7bXXRpcJsHHjRl566SUkSeLyyy/n/vvvp7y8nHvvvZfGxkYURaG4uLhT78D9YXgU+rwcqGzhcEWdKPSCEEP7dvtoazn3bndPlZSiMHlG7++8Hahuinfs2MGoUaPIzc1lzpw5vPPOOyxfvpx33nmHN954gz//+c84HA6am5sB+Na3vsXtt9/OFVdcQSAQQJb7v0V9WBT6rJEZ2PVaypuDsY4iCMIg8fluitesWUN5eTmSJBEOd33xxsluik/2HllfX39a1wOvvPIKV199NQBXX301L774IsuXL+e9997ji1/8YrTb4dTUVDo6OqipqeGKK64AIg9uGoguG4ZFoVdkmTF6K0eM7h8QIAjCwOnLkfdAObWvmEcffZS5c+dSXFxMRUUFq1ev7nIem80W/VlRlNMeCqLrOn/5y1946623ePLJJzFNk+bmZjo6OjBNM9rR2UnddDXWb4b8VTcn5dl0jqgp6HHUuZEgCOdHe3s7WVlZAN12+XI27733HgUFBezcuZMPPviA7du3s3z5ct544w2+8IUv8Pvf/x6/3w9Ac3MziYmJjBgxgjfeeAOIdD/s8/nOfYM+Z/gUercDv2qn7lhVrKMIgjDIfPOb3+RHP/oRV1999Tk9uu+VV15h2bLOXSheeeWVvPLKKyxcuJAlS5ZwxRVXsHjxYp566ikAnnzySYqLiykqKuLqq6+mrq7unLalK912Uxwr/dFN8akOlH7Gf+4J892sFuYuuuRc4/WLodw17WAQT3njKSuIbooH0kB0UzxsjujHjB2JbBocqe+IdRRBEITzalicjAWw222MCLdSfvYb4QRBEIacYXNED5CnBDiCK9YxBEEQzqthVehzk1XqbCm0NzTGOoogCMJ5023TTUNDAxs2bKClpQVJkigqKmL58uWdpjFNk2effZY9e/Zgs9m47bbbGDt2LAB79+7l2WefxTAMFi1axKpVqwZmS3ogNysVyuBoWQWT0zwxyyEIgnA+dXtErygKX/nKV3j88cdZu3Ytb775JpWVlZ2m2bNnD8ePH+fJJ5/k61//evTZsYZhUFxczP3338/jjz/O1q1bT5v3fBo7LtI3fXlNc8wyCIIgnG/dFvrU1NTo0bnD4SAnJ4emps4P8di5cyeXXXYZkiQxfvx4vF4vzc3NlJWVkZWVRWZmJqqqMnfuXHbs2DEwW9IDbk8SSZqPI22ib3pBGC6uueaa07oL/uUvf8l99913xnlWr17Nhx9+2OW4xsZGxowZw29+85v+jDmgenXVTV1dHeXl5eTn53ca3tTURFpaWvR3j8dDU1MTTU1NeDyeTsMPHTrU5bJLSkooKSkBYN26dZ2W1xuqqp513jzJS3nY3ufl96fusg42Iu/Aiaes0Lu8tbW1Z+wG+Hy45ppreO211ygqKooOe/XVV3nggQfOmEuSJBRF6XL8X/7yF2bOnMmrr77KV7/61QHJ3N3rdbKvnR4vr6cTBgIB1q9fz4033njahfpd3XMlSdIZh3elqKio0x+irzePdHcjR65d5y+Sh+OVlaj22D6MZCjfJDMYxFPeeMoKvcsbDAajnYfFwooVK/jRj36E1+vFZrNRUVHB8ePHmTlzJt/5znf48MMPCQQCXHnllXznO98BIjVN1/Uub1x6+eWX+e///m/uuOMOKioqGDFiBAB//OMfefrppwGYNGkSP//5z6mvr+fee+/l6NGjAPzoRz9i1qxZZ83bkxumgsHgaa//2W6Y6lGh1zSN9evXM3/+fGbPnn3aeI/H02mljY2NpKamomkajY2Npw2PpVxPAuHjFmqOVDJqYn73MwiC0G+2bNlCfX19vy4zPT2dyy677Izj3W4306ZN491332Xp0qVs2rSJlStXIkkS3/3ud0lNTUXXdb74xS9SWlpKQUHBGZdVVVVFXV0d06dPZ8WKFbz66qt84xvf4MCBAzz55JNs2rQJt9sd7YL4+9//PpdccgnFxcXouo7X6+3Xbe+pbtvoTdPkqaeeIicnhxUrVnQ5TWFhIVu2bME0TQ4ePIjT6SQ1NZVx48ZRU1NDXV0dmqaxbds2CgsL+30jeiMnyw1AdY14WLggDBerVq1i06ZNAGzatCl69d9rr73G0qVLWbp0KQcOHDhj0/JJr776KldddRUQ6YL45DK3bt3KlVdeidsdqS8nD2i3bt3KDTfcAEQubElKSur/jeuBbo/oDxw4wJYtWxg9ejT33HMPAF/60peiR/BLlixh+vTp7N69mzvvvBOr1cptt90GRDbspptuYu3atRiGwcKFCxk1atQAbk73RozOgr0VVDeJrhAE4Xw725H3QFq2bBkPPvggH3/8MYFAgClTpnDs2DGefvppXn/9dVJSUlizZg2BQOCsy3nllVdoaGjgT3/6ExA5/3D48OEuuyAeTLot9BMnTuy2205Jkrjlllu6HDdjxgxmzJjRt3QDICkxAZfmp9obP50cCYJwbhISEpgzZw5333139Gi+vb0dh8NBUlIS9fX1bN68mTlz5pxxGWVlZfh8Pnbt2hUd9thjj7Fp0yaWL1/OzTffzNe+9rVo001qairz5s3j+eef52tf+xq6ruPz+UhMTBzw7f28YXVn7EnZRgfV4WHTzY8gCESab0pLS6NPf7rwwguZPHkyCxcu5O677+72JOmmTZuiT4I6afny5WzatIkJEyZw5513snr1aoqKinjwwQcB+OEPf8i2bdtYtGgRy5Yt48CBAwOzcd0YNt0Un+rx32zmYy2BX3/14j6to78M5SstBoN4yhtPWUF0UzyQRDfF/STbpdBoTcLXLO6QFQRh6BuWhT43KwWAo4eOxTiJIAjCwBuWhT5v3EgADos+bwRhwA3S1uG41tvXdFgW+vT0VFyanyMt4ikkgjDQZFmOqzbywU7TNGS5d6V7WF56IkkSuWYb5bot1lEEYciz2+0EAgGCwWBMrjW32WwEg8Hzvt6+Olte0zSRZRl7L7tvGZaFHiDPYfJm0I0WDKLaRMEXhIEiSRIOhyNm6x/KVzT11LBsuoFInzchxUrN4YpYRxEEQRhQw7bQ543JBKC8ojbGSQRBEAbWsC30o3JHoBg65Q2+WEcRBEEYUMO2jd5qsTBSa+FIaPB2RCQIgtAfhu0RPUCeJUS5nCyu8xUEYUgb1oU+N9lKszWRlhrRTi8IwtA1rAt9Xk7kebZHDlfGOIkgCMLAGd6FPj/yEJTy460xTiIIgjBwhu3JWIDkpATc4XbKQ3qsowiCIAyYbgv9xo0b2b17N8nJyaxfv/608a+++irvvfceAIZhUFlZSXFxMS6Xi9tvvx273Y4syyiKwrp16/p/C85RnuTliB67u/YEQRAGWreFfsGCBSxbtowNGzZ0OX7lypWsXLkSgJ07d/L666/jcrmi4x944IGYPRC3J3ITJPYE3ATbO7AlurqfQRAEIc5020ZfUFDQqXCfzdatW7n00kvPOdT5NDYzCUNSqCg7GusogiAIA6LfTsYGg0H27t3LJZdc0mn42rVr+e53v0tJSUl/rapf5eZGHr91uDJ+Oj0SBEHojX47Gbtr1y4mTJjQ6ej/oYcewu1209raysMPP0x2djYFBQVdzl9SUhLdGaxbt460tLQ+5VBVtVfzprrdODdv5rOWUJ/X2Ve9zRprIu/AiaesEF954ykrDEzefiv0W7duZd68eZ2Gud1uAJKTk5k1axZlZWVnLPRFRUUUFRVFf+9rN5196eJzktnCx2En9fX157W/bNF96sCKp7zxlBXiK288ZYW+5x3wh4P7fD5KS0spLCyMDgsEAvj9/ujPH330EaNHj+6P1fW7C90WquwemiqrYh1FEASh33V7RP/EE09QWlpKe3s7t956K9dff330sWBLliwBYPv27UydOrXTU09aW1t57LHHANB1nXnz5jFt2rSB2IZzNjl/BOwOUrr/GPNHjYx1HEEQhH7VbaFfs2ZNtwtZsGABCxYs6DQsMzOTRx99tM/Bzqe8/JHIuw5ypNHL/FiHEQRB6GfD+s7Yk6wWCyPCbRwTzwoXBGEIGtZ93ZxqtBKkwkyIdQxBEIR+Jwr9CaNdMsdtKQTa22MdRRAEoV+JQn/C6PRETEmmUtwhKwjCECMK/Qn5E8YA8Gm5eAiJIAhDiyj0J2RluEkPt/Fxs+iyWBCEoUUU+lNcaPFRqngwwuFYRxEEQeg3otCf4sIMJ22WBKpFO70gCEOIKPSnyM8bAcBnR4/HOIkgCEL/EYX+FKNys1ENjcONvlhHEQRB6DfizthTWFSFMVoLhw0l1lEEQRD6jTii/5xxNo3DSipGWPSHIAjC0CAK/edMyHTRYXFSeeBwrKMIgiD0C1HoP6egIBeAfYeqYxtEEAShn4hC/zkjsjy4wx2UNotr6QVBGBpEof8cSZIoUL18IqVi6Fqs4wiCIJwzUei7cGGGgyZrErVlR2IdRRAE4Zx1e3nlxo0b2b17N8nJyaxfv/608Z988gk/+clPyMjIAGD27NmsXr0agL179/Lss89iGAaLFi1i1apV/Rx/YFw4YTTUt7DvQAUjJuTHOo4gCMI56bbQL1iwgGXLlrFhw4YzTjNp0iTuvffeTsMMw6C4uJjvfe97eDwe7rvvPgoLCxk5cvA/k3X06EyStGo+aQixONZhBEEQzlG3TTcFBQW4XK5eL7isrIysrCwyMzNRVZW5c+eyY8eOPoU83yRJYpLcTqmZiGkYsY4jCIJwTvqljf7gwYPcc889PPLII1RUVADQ1NSEx+OJTuPxeGhqauqP1Z0XF3ps1NrdNJQfi3UUQRCEc3LOXSDk5eWxceNG7HY7u3fv5tFHH+XJJ5/ENM3TppUk6YzLKSkpoaSkBIB169aRlpbWpzyqqvZ53lPNKSzg129X89mRWibNLjzn5XWlv7KeLyLvwImnrBBfeeMpKwxM3nMu9E6nM/rzjBkzKC4upq2tDY/HQ2NjY3RcY2MjqampZ1xOUVERRUVF0d8bGhr6lCctLa3P857K7XHh1IPsPNbMJf2wvK70V9bzReQdOPGUFeIrbzxlhb7nzc7OPuO4c266aWlpiR69l5WVYRgGiYmJjBs3jpqaGurq6tA0jW3btlFYODBHxgNBVWQulFr5yEjG1MVTpwRBiF/dHtE/8cQTlJaW0t7ezq233sr111+PpkVuJFqyZAnvv/8+b731FoqiYLVaWbNmDZIkoSgKN910E2vXrsUwDBYuXMioUaMGfIP607QsJzvqXNTsP0j25EmxjiMIgtAn3Rb6NWvWnHX8smXLWLZsWZfjZsyYwYwZM/qWbBCYdtE4KKll74FKUegFQYhb4s7Ys8jJSCFV81LaIppuBEGIX6LQn4UkSUxUvOyXUjANUewFQYhPotB3Y1KajQZbCg3l4oHhgiDEJ1Hou1EwfjQAH+49GOMkgiAIfSMKfTfy87LI1tt4q0HBDIs+6gVBiD+i0HdDkiSWjrRxwDWKit0fxTqOIAhCr4lC3wPzZo4HYMdndTFOIgiC0Hui0PdAWrKDMVozuzsssY4iCILQa6LQ99CMJIP9jhG0HxVX3wiCEF9Eoe+h+bMmoMsKf9+yJ9ZRBEEQekUU+h4aNzKNfLONt/wpmMFArOMIgiD0mCj0vXD5KAdHE7Ko2PtJrKMIgiD0mCj0vXDxtMiDwj8oOx7jJIIgCD0nCn0vpCc7yNea2OpLwPB7Yx1HEAShR0Sh76XF4z2UO7P4+K9/i3UUQRCEHhGFvpcuv/gCUgw/r9XKXT4XVxAEYbARhb6XrIrM/MQQe11j8FdVxjqOIAhCt0Sh74NLCnIIyxZ27T4Q6yiCIAjd6vZRghs3bmT37t0kJyezfv3608a/9957bNq0CQC73c4tt9xCbm4uALfffjt2ux1ZllEUhXXr1vVv+hiZlJ9Dyvt7eLsqzDyfF8mZEOtIgiAIZ9RtoV+wYAHLli1jw4YNXY7PyMjgBz/4AS6Xiz179vDMM8/wyCOPRMc/8MADJCUl9V/iQUCRJVaNS+B/juSx/+3NFFy9ItaRBEEQzqjbppuCggJcLtcZx0+YMCE6/oILLqCxsbH/0g1iV8y+gAQjyBvV4hGDgiAMbt0e0ffGO++8w/Tp0zsNW7t2LQCLFy+mqKjojPOWlJRQUlICwLp160hLS+tTBlVV+zxvby1I1igx87B2tJ/nhk0AACAASURBVJOUm9fr+c9n1v4g8g6ceMoK8ZU3nrLCwOTtt0K/b98+Nm/ezA9/+MPosIceegi3201raysPP/ww2dnZFBQUdDl/UVFRpx1BQ0NDn3KkpaX1ed7emj8lh9e3tfDi717nqlu+iCRJvZr/fGbtDyLvwImnrBBfeeMpK/Q9b3Z29hnH9ctVN0ePHuXpp5/mnnvuITExMTrc7XYDkJyczKxZsygrK+uP1Q0aE3Mzmax28Ec1H/+B/bGOIwiC0KVzLvQNDQ089thj3HHHHZ32KIFAAL/fH/35o48+YvTo0ee6ukFFkiT+v0vH0WZ1sWWHeHi4IAiDU7dNN0888QSlpaW0t7dz6623cv3116NpGgBLlizhxRdfpKOjg1/96lcA0csoW1tbeeyxxwDQdZ158+Yxbdq0AdyU2CjISWGMcYg3/UksaWtFTkqOdSRBEIROJHOQ3sdfXV3dp/li0R73xvsH+MVnJnc2beHyO77W47b64dJ2GCvxlDeeskJ85Y2nrDCI2+iHu8UXj6fA6qc4sRD/QdFWLwjC4CIKfT9QZIkbL83Da3Hy9jZR6AVBGFxEoe8nE7JTKJDaeMUcifeD92IdRxAEIUoU+n50w+WTaLIl88etn2Fq4VjHEQRBAESh71eTshK5PDXMa+mFVG4RR/WCIAwOotD3sxsWTMSKwbP7OzCbh0e/P4IgDG6i0PezVKeF1eNd7EoZzz+e+x1mKBjrSIIgDHOi0A+AqwpzucCu8dO0y/nkjXdiHUcQhGFOFPoBYFVkHlpZgMsMselYELOtJdaRBEEYxkShHyAOi8zi0Q52pozntWd+hxkUTTiCIMSGKPQD6JpL8pmSZFI8YiFlf9sc6ziCIAxTotAPoESbwn8um4jdCLPpUBvGJ3tjHUkQhGFIFPoB5rIqLM5z8V7aRfz0b2WiCUcQhPNOFPrz4Ktzc1mRafKeZzLVr7/KIO0wVBCEIUoU+vNAkSWuu/QCZNPgZ8cTqXjlpVhHEgRhGBGF/jxxO1RumplJRVIOv6x2YDbWxTqSIAjDhCj059FVkzxcl5/ARynj+NMv/hdf6UexjiQIwjDQ7aMEN27cyO7du0lOTmb9+vWnjTdNk2effZY9e/Zgs9m47bbbGDt2LAB79+7l2WefxTAMFi1axKpVq/p/C+LMkulj2Hz8M57LW476/CauWrUAKX9SrGMJgjCEdXtEv2DBAu6///4zjt+zZw/Hjx/nySef5Otf/3r02bGGYVBcXMz999/P448/ztatW6msrOy/5HEqyaawYdV4Jlr8FI9YyNMvbUOvPBrrWIIgDGHdFvqCggJcLtcZx+/cuZPLLrsMSZIYP348Xq+X5uZmysrKyMrKIjMzE1VVmTt3Ljt27OjX8PHsujnjAPjryEvZ8cdNmAf2xTiRIAhD1Tm30Tc1NZGWlhb93ePx0NTURFNTEx6P57ThQsTFo5L4+x1zSVc0fpSxhP99Zavo1lgQhAHRbRt9d7q6JlySpDMOP5OSkhJKSkoAWLduXaedR2+oqtrnec83VVX5zvIp/OTtg7yUM58pjz3G3OuvwbloeayjdSmeXluIr7zxlBXiK288ZYWByXvOhd7j8dDQ0BD9vbGxkdTUVDRNo7Gx8bThZ1JUVERRUVH091OX2RtpaWl9nvd8S0tLY2KSyZMrxnLrnw7yQMH/YeWb/+Cru7YiXXsDUoqn+4WcR/H02kJ85Y2nrBBfeeMpK/Q9b3Z29hnHnXPTTWFhIVu2bME0TQ4ePIjT6SQ1NZVx48ZRU1NDXV0dmqaxbds2CgsLz3V1Q5LLpvDTFfl8IcfOqznzuIU5fPb8/4hr7QVB6BeS2c39+E888QSlpaW0t7eTnJzM9ddfj6ZpACxZsgTTNCkuLubDDz/EarVy2223MW5c5ETj7t27ee655zAMg4ULF3Lttdf2OFh1dXWfNiie9t6fz+oL62z44Dj/ONpOXnsVN5e/zuSbvoo0YUoMU/5LPL220H3e6FvfBCQwDJDlE/+f+N0wTSQJNM3AalUIBnRUi4ymGZimiarIBAIaNpuKzx/CalEJhTVkQFFVOtr9OBPsdLT7cSRY0UI6um5gtVnoaPfhcjno6PCTnp5GY0MThmGgWhW8HQFcLgfeDj8WmwVDN9E1DYvNgtfrJ8HhoMPrw2pR0U0TPaxjtVnw+fw4HA78/gAWi4ppmITCYew2G16vD6fTid8fQFFkTCAcCmN32PD5/NjtdoKBIIqiABKhUAi73Y7P58PhsBMMhpBlGUmSos2wPr8Ph91OMBRGliQkWSYUDEbm8/uw2Rxo4RAgocgywVAQu81BIBjAYrGiaeHIOEUhGAxgs9kJBPxYrTZ0XcMwTFRVJRgMYrPZCQYDWCwWDN3AMI0T4wLRcapiwTANdF3HYrESDPpxuRLp6GhHVS0Yhomua1gsluh8oVAQRVExTU6Ms0bGWe2EwkEUWQFA08PRcVarnXA4dPq4UACrxYamhZGkyGulaWEsFhuhkB+LxY6mhZAkGVmWCYdDp4yzoekh7lzzlX4/ou+20MfKcCz0J2071sb6f1SjmXD3J79l7sQR+Fb8Bxa7nZamdhRVoaPdj9UW+SBLkoKm6eiagW4Y6JqJbhgYmoGmG4TCQRwOKx0dflRFIqzphEJhrFYL7e0dOOx2vD4fqqJiGDqhUAir1UbAH8BitRAIBpBlGUVW8Pt9WCwW/H4fVpudYNCPLEe+GGqahkW1EAh6sVodhEMBQAJJwjQNLBYbdpsDTQ9jsznQdY1wOITNasfnb8dhT8Dn92KxWDEMA00LYbM58fnaTxQHH6pqxTQNNC2IqtoJBjuwWh2EQn5kOdISqRthLKqdUNiHRbUT1gLIUqR4aXpkvrDmxaI40PTgiaIloxtBVNmOpvtRFBu6EUJCAiQMM4Qs2zCMALJkwTB1IqVOxkRDQsFEj2wvJz9Sp/4sxIeTf7NT/z85RsLEQJJkTPPk/5G/b+RnHVlSMUwNSVIBA0yQZAXDCCPL1hP/K2CCiYEsqehGGFWxoeshkhIzWXP312lubu51clHoBwEtrBMK6xw9fBxJkamqqMVisVBXV4uua4TCQUKhAADBkDdyJKAHwDQxMTi/BaPzm12SVDANkKToG9OiOtD0AKpixzQNIkdsJ8ZZHITDfiwWx4n8JrKsouthwmEfkiRHiqikIEsKuhFCVRxouh+L6kDXIx8GWVYJh31Yra7I/5YEdENDkmQUxYKmBbHbEwkFvVhtTnRdQ5IkVMVCKBzA6UjEH/BiszowDB0getTlsCcQCPqw2xyYpolhGFitNoJBPw6Hi2DQj81mxzRNTNPEZrMRDARwOBMIhYJYrFYwwTD0yLhQEKfDSSgUwmKxRJap69gdDgKBAE6nk2AwiEW1IMkQCoVxOiNH3k6nE1mW0HUdSZLRNA2H047f58fhdBAOhVEUGVlR0MIaDocdn99PgtNJOBxGVhRURSYUCuF0OvD5AzjsNjRNR5LAYrEQCAZxJThPHPHb0XQdSZKwWFT8/gAulxOfL4DTaUfTdMDEZrPi9wVIcDlPHPHb0A0TTIOMjHRqjteS6HLi9Qaw263R19Fut+H1+klMTMDrixzxG2bkSNrhsOPt8OFyOfEHQthskZ1zOKyTkGCPfFtJcBAIhLBYVGRJIqyFsdvt+P0BHA575PVQFSRJIhwOn/gGE/kmEwqFUFUVWY68Hg6HA6fTidfrRdM0ZDlyJK1pWuTvFgxitVrR9cj7Q1EUwuEwNpst+rfUT7xWsiyf+KZgIRwOoyhKtNDLsoxhGCiKgq7ryLJ82riT/5/8NmSaZnS6U6cZiDZ6UegHQFurH78vzKGDR2luaaWhvgZ/sBVN9502rSQpSMhYVCeqakWSZBz2BAzTxGZz0BY0aAgYqKbBaO9xMtJSMLJHk+BynfggS0DkDaaokTexqkjIioyiRL4SWywWOtoCJKU4CYcMrDYVq9WCPxDCneqivd1HcoqLYDCMqipYLAq6buB02giHNSwWFcMwSE9Pj14ie7YrqLqj6zqapqOe+LCe/ICYptmpWeBcDYb3Qk/FU1aIr7zxlBUG5mTsOV91M1zpukEoaHD0SC0tLe1UV9Xg9bXj97cRCLUARnRah91NQkIq6Z4JpKQkI8kwavQI0tLdWK0nGgDkM58X31nVwVMf1LDVmcuqo+9w3eH3cNxwG1LexH7ZlsQkJwAJCfbTxtls1mg+RVH6pQgrinKiHZjosuHcdh6CIJyZKPRn4fMGaWvzU1XZgN/np76+gUAggM/fjj/YjGGEOLU9VpZUEl1ppKWNJyUllTG52Xg8SaSkJna5/J7uuQtzXDx+5Vie2VHLi1IRW4MzuH3DzykY5Ub+0jeQMkb030YLgjDkDKlCv+nld0+c5dZ6NZ+JidfbgaIo+HxtSJJEMNSBbgTp3DYuo8gqTkcK6Z6RuFyJZGal4XTauWD8qOhR70BItCn833nZLM5P5v//p8L3pt/GZfUf8h8//C7p06chXfMVJHf6gKxbEIT4NqQKfVXNoehJt96yWBwYuobdnohh6GSkjcHhcJCUlERGZioJCQ6yR3oGrJD31EVZCfxsxThe+qSRV/ZP5YP0yVxRuZVrHrybpMuKkCZeBJOmIp2lKUgQhOFlSBX6227/WtydeOkLh0XmP6alszg/md/sredVLuP97Jks3beZ+e88hjvDg1Q4D2n6JTBilGj7FoRhbkgV+uEm02XlO/NyWDHBz8/+WcNz+St4Ln8FOeEWVv/zr1z419dImzAeacZcpBlzkBzOWEcWBCEGRKEfAiamO/jFyrFUt4V4v6KdN8os/GzSl7BisKj6Axa89BL5f/oNUv5E5IVXQu4FSLbTr7ARBGFoEoV+CMlOsnLthR6umpjK0ZYQrx1ookSew1+z55CjtzG/ZjeFT21grNEaOcqfUghTCpEsllhHFwRhAIlCPwRZFJl8j51vz83ma4WZbDvWzubDDn6vLOD3IxeQq7dy8fEPmbX3eXLDT6BMnII0eQbS5JlIaZmxji8IQj8ThX6Ic1kVluSnsCQ/hdaAxubyVnZWOXlBSeaFnMsYiZcLmstZ/NpfyX7heZI87kjBnzwDxl+IZLHGehMEQThHotAPI8l2lVWTPKya5KG6LcT+eh9vf9bKB5ZENqdOxorB9EAVkw/sZfwHvyI31IhlfAFS/iSk7NGYl18R600QBKEPRKEfprKTrGQnWVk0LnKk/3Gtj39WtHOwwcoH9lEA5OAlq72WpX9/l6zAGxgbH4H8SUhjLkCaMQfc6UgecZOWIAx2otALJNtV5o1JYt6YJEzT5ONaH9XtId481MJBayK7EsciY5In+5lw/BPG7K9g9t9/iCvsRx5fgOTJRJp6MYwYiZQ9OtabIwjC54hCL3QiSRIXZSVwUVYCyy5Ipcmv8Wm9j38e6+BYewJ/MWZBxiyen7AKwzC4snEPrqo65u7+BS7Njz0pCanwUkhNi7TzZ2QjxfhuYkEY7kShF87K7VCZOzqJuaOTSEtL4697j+AN6/zlYDNtQZ0XmQUeeG7scrLlIOP9NSzc8SYpoXZGvlAMrkSkSdMw/T6kCwqgpQkyspBS02D6HNFVgyCcBz0q9Hv37uXZZ5/FMAwWLVrEqlWrOo1/9dVXee+99wAwDIPKykqKi4txuVzcfvvt2O32aIdf69at6/+tEM6bWSNdACzIS6YjqFPRGmRHVQctAZ3dNV62SmPZPO1WAMZaQ4z01nFp9U4shsz4V18gaLGTHGhDxoSRuZGuGkbmQf5EpISue/kUBOHcdFvoDcOguLiY733ve3g8Hu677z4KCwsZOXJkdJqVK1eycuVKAHbu3Mnrr7+Oy+WKjn/ggQdISkoagPhCLLlsCpMynEzKiHStYJomrQGdN8paCOsmn9T5+KdmY8uYyHtFyluNiYQqQ7qssbxuBwv+/CIJWgBUC0yZiZQ3HunSInC6kFTxhVMQ+kO3n6SysjKysrLIzIzcSDN37lx27NjRqdCfauvWrVx66aX9m1KIC5IkkeJQ+fcpadFh9d4wDb4wQS1ykjfJptAS0Cit81OszeHX8+aQYzcZH6hlbvlWcj95BffLz0OKB+nC6RAKRnrjnDAZ0keIDtoEoQ+6LfRNTU14PJ7o7x6Ph0OHDnU5bTAYZO/evdx8882dhq9duxaAxYsXU1RUdC55hTiTnmAhPSHSxcK0EQmdxh1o8PNhjZeDjQG21yu8k3cdcp7JTLWNUccPMufgXnSrnbydv8Bi6pCYDOMmIY3KRcqbAHkXILnEN0VB6E63hb6rR8qe6ahq165dTJgwoVOzzUMPPYTb7aa1tZWHH36Y7OxsCgoKTpu3pKSEkpISANatW0daWtpp0/SEqqp9nvd8i6es0P9509Lg0hNPQwxqOnur2nhjfx17qmzsSEvm5bRZACQWSEy0BclrqyCv9hAT33iN5PDvkRxOTNWCfe7loCrY5y1GdiUiOV3Iyalx9frGU1aIr7zxlBUGJm+3hd7j8dDY2Bj9vbGxkdTU1C6n3bp1K/Pmzes0zO12A5CcnMysWbMoKyvrstAXFRV1Otrva5/y8dQffTxlhYHPOy4Bbi/0QKGHj457aQnoqDLsrPJS1hRglzIOY8Q4bDnLGGE1mNp+hFRfI3PffSuS7/UXweGMNPfkF6D6vehzLodgAGnyTLBaIdUDNsegawIS74WBE09ZIUYPBx83bhw1NTXU1dXhdrvZtm0bd95552nT+Xw+SktL+da3vhUdFggEME0Th8NBIBDgo48+YvXq1b3eAGH4uSjrX808c0dHmmdCukF5c5A3D7VwvCPEpsBYSB7L/8yZhSqZFMjtZNQexqYHuaR6N2FZZcoLvwZA2fRbSHBBKIh00cUYwQDyF67A9LYhFUyLnAxOSBx0OwBB6A/dFnpFUbjppptYu3YthmGwcOFCRo0axVtvRY6ilixZAsD27duZOnUqdvu/+jlvbW3lscceA0DXdebNm8e0adMGYjuEYcCqyExIczAhzQFAbUeIem+ko7ZjLUH2N8t8lBZ5f/0182IMJLKkACEDZrQdxt1SQ2LYy8x9pQBkffwQAKbVBiluCIWQCi/FbKhDvmwJZltL5NGMNjtUlMPYCaBpSAmurgMKwiAlmV01wg8C1dXVfZovnr6mxVNWGNx5dcMkpJv8+UAThgkvlzYyOtXJoXovFkUipP/rbe5Aw4/K9LbPaLO4mFP3IeneBux6kILWcnRJJinsi0xstYEzIXKj18hcqKlAmrsIs7UZKb8AKT0zskOYNhtqKiF/EgQD4EoCSerxN4TB/Np2JZ7yxlNWiFHTjSDEA0WWcMgS/zY5chLr2gIPWRlpHKw4TlVbiOr2EH8vb0OVJT6q9aHKsCdpHACfjVmCjImBhEfz0qw4mdvwMa22JGbX7sUTbGVf/qUsOL6bWncBhVvfISRbcH20A1O1gBbGfO130NEOI0ZBwB+5IzgnF6O2Cmn+EjhUijT7C5jtLUhJqZCWAd4OGD0OOtowPR5M0xRNR8KAEIVeGJIsSuRo2uO04HFaon33mKZJWVMAVZZ4t7yNsGFS1RqktN6PLEEjkXMD/0i/CICPk3KRTBNTkigZMZugYmWktxafamdceyWjvLU02pJZeHwXh0YXcmndh7RbEkirrcdeXYVNDyOXHwTA3L4FJDCRICkZmhpgTD5UHaFl6sUYrS1gdyDlT8IsP4g8fwnmodJIh3EBP9hskDEC2logJxfaWyAxBUD0JySclWi6iaF4ygpDO29NewinRebtz1pJtSvRnUBdR5hGv9bt/Iqho8sKrrAPTVZwagGy/I2Uu7JZWv0+n6SMpahmO60WF6mhNkb66mi0JTOt6SB19lRyfPXokozdCP9robIMhhFpPlIUCIXAkw51NZA3HiqPRJqKDCNyZdGUmZgH9iF9YRkcPhA5yWwYgAmj86GuGsZNhPZWSDqxg1D79hjJeHovxFNWGJimG1HoYyiessLwyxvWTQzTZMuRNlxWheJdtYxKtrH3uJcEq4I3pGP08tMjmwYADi2A1+Ik099InT2V8W3HCCpWFEOnoLWcssRRLK3+J/uT85jdsI+AYsOhB8jx1dNsTSSvo5oWayKpwTYAFE4JIslgGmCxRq4m8nvBnQ5N9TAyD+prYNTYyOWmDXWRE9D7dkd2EEfLIL8AyWrD7GhDmnQR5tHPkAqmQ0tj5KS1rJCWkUlDW1tcNDUNl/etKPSDVDxlBZEXIpd41naEafBphDSDbRXtjE62senTJsZ77Hx43EeiTSGgGXhDBooEeh8/YZJpYEoyVj2MgkFIUkkLtlDr8JDfdoxjCSOY1FqOCYRlCzOaPuWTlLEsq/onB5NGM635ILokI5smo73HabIlMbrjOAHFRoIe6GKFEpgmqGpkBxHwR+5Gbm+F9CxobQZPRuT8Q9UxmDkX86MdSJctw6w6gpSTG9kRHK9Emnkp5sF9kZPUzQ2QlBo5qe33Q2Y2NNWBJxN0DclqO5c/SbeGy/tWFPpBKp6ygsjbnbBu0hrU6AjqAHxS5ycnycqm/U3ke+y8X9GO06IQNgwONwVJsiu0BnSsJ64KkoD++DCebEaya0EspoZXdTDC30CVM4PJzZ9xxDWCia1HsBkhWqyJzK37iA/dF1BUvZ0jrmzGtx3DYmoEZQvj2iups7sZ460hoNhwaf4zr1iSQZFB0yKXpAYD4EqEQCByI5srCY5XRi5TPXwQJk2N7AQSEpFyRmOW7UeavxTz4x1Isy6LfINwJiCNGIVZUY40/ZLIN4688ZGdkNUGCYmRG+SSUiI7jS6aoobL+1YU+kEqnrKCyNsfgpqBJMFnjQEyXBb+fqSNMck2DrWZtHV4kSWJd4+0MTHNzo4qLyOTrBzvCGNVJLQTl5DKEr1uMjqbk98cVENDk1WcWgCrEabFmkiOt5aqhEwKWg5T7UxntPc4yaEOqp3pfOH4LnZ7JlJUs51KZwYjfXUkh7w02xKZ3FxGRUIW+e0V+BQ7Ls2PbBqRPovOGubEt4qTJ5dNE2yOSPPTyW8XngzQNdDCkDUKPvsUaeZczNI9SFNnYwb9EA4jTZiCWbqX5OXX0bbjH0gF0zBDIdB1pNx8zCOHkKYURnY+GdmR8xmyHNk5hcNIDmf/vci9IAp9DwzGD/eZxFNWEHkH0qlZdSNSzGvawzgsMrUdYdqCGg6LzNaj7UxMd/DnA83ku+2U1vvRDRObKvNZU4B0p0q9T8NpkQlokfMB/blTONXndxA2PYTVCNNuSSA90ES93U1eexXNtiSSQ+1k+ps4mDyGBcd38Y+MqSyu+YA6uxtPsJVsXz1HXNnMq9vL/uRcpjcdpN3iJEELkBTuIKDYSA80E5QtnU9YnzXgiXMVqhr5lmG1gqx0bpJyp0e+VWTmRIp8cwPkF8Anu5EuWYh54GOkSVNBVTEb6pCmz8b8aCfypUWRHcWoPLDYMNuaIzuWw59GToI3NkBySqQJTNMgJRWCwR7tPESh74F4/XDHA5F34JxL1o6gjqpIlDcHyEmysaOynfQECxWtIZr8Gil2hTcOtTB9RAJvlbVwQZqD2vYQvrCBVZVp9kd2DL6wgSpL6IbZL01IPSWbBqqhEVKsODU/PtVBSrANTY4c1Wf6myhPzGFm434+TsnnkoaPabMkoJo6E1uPsi9lLItrtvNRSj6FjfvxqXYUU2eM9zgVzkymtHxGrT2VjEAzhiRjNcKohh55+E2vw564EurE/RPY7JFvHz5vpPmorSXyjaO1GdxpYHdC9TEomBbZeRTOwzz6GeSMRkrxYB4ti+xQ9r6PlDses/IIGd/7CY3NLb2OJgr9IBVPWUHkHUjnK2tYNwgbJiHNpNGvkWRT2F7ZwaR0B1uPtZPpslDbEaa8OUB2kpU3D7UwIzuBnSeakdqCOm1BDadVpS2gYVclApqJKoNu9M85ht6QTQPjlG8Vdi2IjEFAsZIabKfRnsJIby3VzjTGtVdhINFsS2Rq0yF2eSZxRdU2Pk7NZ0bTpxhItFsSmNp8kH0p47isdg9HXNmM6ahBwkSTVXJ89TRZk8gKNBKQrThPnNQ+p2uPTn7zUFQYmUvmE8+LI/ruiA/3wBF5B85gzaobJpIETX4NCQhoJrUdIcaPzODVvUeZmpnA+ye+QbQHdUrrfIx12/nrwRZm5iSwp9pLlsuKX9Op92ok2hTagnrMdxCfJ5sGsmlEdhZ6MHLiOeyjw+IkNdiGxdBotCWT21HNZ0mjmNZ0gMOuHMZ2VOHUAlQ5M7i0bi870gpYXL2dzxJHkuutISHsp9bhZmbjfkpTxlLYuJ86eyqpwXZsRoigbCUr0Ei76sSQJOyLVpD39W+JQt+dwfqB6Uo8ZQWRdyDFU1boPq9+4sRAS0DDMMEwTSpbQ2QnWfn7kTYmpTvYUdVBklUhbJjsqu6gIMPJGwcj3yD21fpwO1V0A6rbQ7gdKk1+jQTr/2vv/mKjKtM4jn/Pmc70z7RMZ6b/2NK6lnaFrhv+ZJoqwRCWilniroYQErKR1BsuKjGRYJDEKImaEKHBaCDeGKNc4YWNNxtkARcTakJ3K8GtW3AKSFXa0k47badTZuacdy+mDEIZWrXDOTN5Pjdtp+dMf/MkfWbOe855X51IzMTlSA4xGYoFu1rpt7p1zkJXBqbmwGkmcChj5k0jwqTTje9mmAmnm0XxCMXxCP3uSh4du8x/S+t4dOwywbJ6jretZTQ0MvcfvIvMdSOEeKAcenIww190+3LHymIXQGqpyRU/m4r67yvKAWhdVYFSEIkZxGdOSl8Lx6jxuOi8NkGDv4D/3Yji0DRcDo1zP0yycrGbf1waZUWVmwuDEdxOBy6HRu9wlFpPPlfHblLhziMUNXDO5IomTJy6lvobC3HCWmk6AKaWPLcQ1/OIz7TYSWfytYbyPQCM5HsYmfn+grch9TVQUZSq3UKSRi+EsI28mSZXBkp6HAAAB+xJREFUWni7Nd16s/jbsuQiRsvLb1+58pc/JBdBema5D6WSJ5GnEyYOTWMoEqfC7eSHm06KzCg/jceYTpiUzJyXWLnYzT/7xmgsL+JyaJpI3MBf5ORfV8ZpqnbzVf8E9b5CbkTixAyTQqeDoUgcz8z9D26nTsxQxM3kEFTC/O1HFyt+l5kpsKXRCyFygqZpaECRM/mJusaTvOO2ucrL8LDB4hJXattbC9sEqmc31h2BSnQNwtMGebrGVNxkMmbgKXBwYWCKZeWFnPthkupFLkamEgxF4lQvcnH6cpjmJcWcuhxmqa+AkakEP47HeKg0n3//OMmy8kIuDkdZXOIiGjcZm06es7j1phGJm6yscs/KsxCk0QshxM847jqqKM53UEHyqGJ9XXK45Znlvln7/Xnmd39ddvt3NxPJS1ZD0QSeAgc/jcfIz0seCdyIxFlc4uKr/gn+VFnEf36apMbjmvW8C0EavRBCZEh+XnLcvtydfKP4vff2CnwPlSaPOLb80Q+QWjktE/SMPbMQQghbmNcn+vPnz/Phhx9imiYbNmzg2WefveP3PT09vP3221RUVADQ3NycWgR8rn2FEEJk1pyN3jRNPvjgA1599VX8fj979+4lEAiwZMmSO7Zbvnw5r7zyyq/aVwghRObMOXQTDAapqqqisrKSvLw81qxZQ1dX17ye/LfsK4QQYmHM+Yk+FArh9/tTP/v9fr777rtZ2126dImXX34Zr9fLc889R01Nzbz3BTh58iQnT54EYP/+/ZSVlf3iFwOQl5f3q/d90LIpK0jeTMqmrJBdebMpK2Qm75yN/l4zJNy9fNjDDz/MkSNHKCgooLu7mwMHDvDuu+/Oa99bWlpaaGlpSf38a28Hz6ZbybMpK0jeTMqmrJBdebMpK2RmmuI5h278fj8jI7fnXRgZGcHr9d6xTVFREQUFycuGVq9ejWEYjI+Pz2tfIYQQmTVno1+6dCnXr19naGiIRCJBZ2cngUDgjm3GxsZSn96DwSCmaVJSUjKvfYUQQmTWvGav7O7u5qOPPsI0TdavX8/mzZs5ceIEABs3buT48eOcOHECh8OBy+Vi+/btPPLII2n3FUII8QCpHLNnzx6rI8xbNmVVSvJmUjZlVSq78mZTVqUyk1fujBVCiBwnjV4IIXKcY9++ffusDrHQ6urqrI4wb9mUFSRvJmVTVsiuvNmUFRY+r22XEhRCCLEwZOhGCCFyXM7MR58Ns2S+8MILFBQUoOs6DoeD/fv3Mzk5yaFDh7hx4wbl5eW89NJLFBdnZjmxuRw5coTu7m48Hg/t7e0A983X0dHB6dOn0XWd559/npUrV1qa9ZNPPuHUqVMsWrQIgG3btrF69WrLsw4PD3P48GHGxsbQNI2WlhY2bdpk29qmy2vX+sZiMV5//XUSiQSGYfDYY4+xdetWW9Y3XdaM13bBr+OxgGEYaufOnWpgYEDF43G1e/du1d/fb3WsWdra2lQ4HL7jsaNHj6qOjg6llFIdHR3q6NGjVkRTSinV09Oj+vr61K5du1KPpcvX39+vdu/erWKxmBocHFQ7d+5UhmFYmvXYsWPqs88+m7Wt1VlDoZDq6+tTSik1NTWlXnzxRdXf32/b2qbLa9f6mqapotGoUkqpeDyu9u7dqy5evGjL+qbLmuna5sTQTTbPktnV1cW6desAWLdunaW5GxsbZx1NpMvX1dXFmjVrcDqdVFRUUFVVRTAYtDRrOlZn9Xq9qZNrhYWFVFdXEwqFbFvbdHnTsTqvpmmpKVgMw8AwDDRNs2V902VNZ6Gy5sTQzS+ZJdNqb731FgBPPvkkLS0thMPh1Pw/Xq+X8fFxK+PNki5fKBSioaEhtZ3P57tvM3hQPv/8c7788kvq6urYvn07xcXFtso6NDTElStXqK+vz4ra/jxvb2+vbetrmiZ79uxhYGCAp556ioaGBtvW915Zv/7664zWNicavfoFs2Ra6Y033sDn8xEOh3nzzTfvO9uc3d2r5lbbuHFjamWzY8eO8fHHH9PW1mabrNPT07S3t9Pa2kpRUVHa7eya18711XWdAwcOEIlEOHjwINeuXUu7rdV575U107XNiaGbbJkl0+dLrg7v8XhoamoiGAzi8XgYHR0FYHR0NHUyxi7S5bu75qFQKPX6rFJaWoqu6+i6zoYNG+jr6wPskTWRSNDe3s4TTzxBc3MzYO/a3iuvnet7i9vtprGxkfPnz9u6vndnzXRtc6LRZ8MsmdPT00Sj0dT3Fy5coLa2lkAgwJkzZwA4c+YMTU1NVsacJV2+QCBAZ2cn8XicoaEhrl+/Tn19vZVRU//UAOfOnaOmpgawPqtSivfff5/q6mqefvrp1ON2rW26vHat7/j4OJFIBEhe1fLNN99QXV1ty/qmy5rp2ubMDVN2nyVzcHCQgwcPAsmTMGvXrmXz5s1MTExw6NAhhoeHKSsrY9euXZZdXvnOO+/w7bffMjExgcfjYevWrTQ1NaXN9+mnn/LFF1+g6zqtra2sWrXK0qw9PT1cvXoVTdMoLy9nx44dqSM7K7P29vby2muvUVtbmxpS3LZtGw0NDbasbbq8Z8+etWV9v//+ew4fPoxpmiilePzxx9myZct9/7esypsu63vvvZfR2uZMoxdCCHFvOTF0I4QQIj1p9EIIkeOk0QshRI6TRi+EEDlOGr0QQuQ4afRCCJHjpNELIUSOk0YvhBA57v/opzZQDQIE8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainModel(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
