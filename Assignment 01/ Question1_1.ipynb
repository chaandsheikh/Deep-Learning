{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Shape of training features  (784, 60000)\n",
      "Shape of test features  (784, 10000)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "tr_x = tr_x.T\n",
    "te_x = te_x.T\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Defaultm the matrix has double variables. But the tensorFlow.MatMul to multiple matices, \n",
    "# we need the data in the matrices to be as TensorFlow float32 format. \n",
    "# We we don't case the datatype, we will get error which is given below\n",
    "# InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:MatMul] name: MatMul/\n",
    "\n",
    "\n",
    "tr_x = tf.cast(tr_x, tf.float32) \n",
    "te_x = tf.cast(te_x, tf.float32)\n",
    "tr_y = tf.cast(tr_y, tf.float32)\n",
    "te_y = tf.cast(te_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a coefficient for each of the features and a single bias value\n",
    "# Weight will be divided into approximatly same count of positive and negative weights as mean is kept as 0\n",
    "# Stddev is 0.05, to majorty of numbers between -0.05 to 0.05, and almost all of the values between -0.15 to 0.15\n",
    "# The distribution will follow the Gaussian cruve\n",
    "# Dafault datatype will be float32\n",
    "# We will make a matrix of 10 (classes) be 784 (features). The shape will be usefull during the matrix multiplication later\n",
    "w = tf.Variable(tf.random.normal(shape=[784,10], mean=0.0, stddev=0.05))\n",
    "\n",
    "# A single bias value is good enough, we will keep the value as 0 for now\n",
    "b = tf.Variable([0.])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is input data, feature values for all the instances, (training data -  60000 images)\n",
    "# X has dimension as features by number of instances\n",
    "# weight matrix w is created erlier,  matrix of shape 784X10\n",
    "# b is the bais\n",
    "\n",
    "def forwardPass(x, w, b):\n",
    "    w_T = tf.transpose(w)\n",
    "    y_pred = tf.matmul(w_T, x) + b \n",
    "    # Pipe the results through the softmax activiation function. \n",
    "    # pass the values to the softmax layer, the softmax layer to convert the values to probabilites\n",
    "    return softmax(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive values from a range, converts the range to 0 to 1; sum of all values will be 1\n",
    "# Returns the probabilites\n",
    "def softmax(y_pred):\n",
    "    y_pred_exp = tf.math.exp(y_pred)\n",
    "    summation = tf.reduce_sum(y_pred_exp, 0, keepdims=True) \n",
    "    return y_pred_exp / summation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    # Any values less than clip_value_min are set to clip_value_min. Any values greater than clip_value_max are set to clip_value_max.\n",
    "    # if the calue is less than 1e-10, than it will set to 1e-10, and if the value is more than 1.0, it will set to 1.0\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "\n",
    "    # Following is the formula for calculating the loss, and we will get loss for all the 10 classes\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=0)\n",
    "    \n",
    "    # Calculating the loss, by taking the mean of the losses\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(x, y, w, b):\n",
    "  # ForwardPass will internally class softmax, and we will get 10 probabilies for the image for fall under a class\n",
    "  y_pred_softmax = forwardPass(x,w, b)\n",
    "    \n",
    "  # Round the predictions by the logistical unit to either 1 or 0\n",
    "  # If the value will be more than 0.5, we will get a one, else we will set to 0\n",
    "  #predictions = tf.round(y_pred_softmax)\n",
    "    \n",
    "  # tf.equal will return a boolean array: True if prediction correct, False otherwise\n",
    "  # tf.cast converts the resulting boolean array to a numerical array \n",
    "  # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "  predictions_correct = tf.cast(tf.equal(tf.round(y_pred_softmax), y), tf.float32)\n",
    "      \n",
    "  # Finally, we just determine the mean value of predictions_correct (Acuracy)\n",
    "  return tf.reduce_mean(predictions_correct),y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(num_Iterations = 50):\n",
    "    # Lists using for storing the values per epoch for building the graph \n",
    "    trainingLosses= []\n",
    "    testLosses= []\n",
    "    trainingAccuracies = []\n",
    "    testAccuracies = []\n",
    "    \n",
    "    # Iterate our training loop\n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "      # Create an instance of GradientTape to monitor the forward pass\n",
    "      # and calcualte the gradients for each of the variables m and c\n",
    "      with tf.GradientTape() as tape:\n",
    "        y_pred = forwardPass(tr_x,w, b)\n",
    "        currentLoss = cross_entropy(tr_y, y_pred)\n",
    "      # Adding the valuses on the gradient tape, the tape will by the Adam optimizer later\n",
    "      gradients = tape.gradient(currentLoss, [w, b])\n",
    "        \n",
    "      # Calculations for training instances, accuracies and losses\n",
    "      tr_accuracy, y_pred_softmax = calculate_accuracy(tr_x, tr_y, w, b) #Accuracy on training instances\n",
    "      \n",
    "      # Calculations for Test instances, accuracies and losses\n",
    "      te_accuracy, y_pred_softmax = calculate_accuracy(te_x, te_y, w, b)\n",
    "      te_currentLoss = cross_entropy(te_y, y_pred_softmax)\n",
    "    \n",
    "      # Appending and print the information for training instances\n",
    "      trainingAccuracies.append(tr_accuracy.numpy())\n",
    "      trainingLosses.append(currentLoss.numpy())\n",
    "      print (\"Iteration \", i, \": train_loss = \",currentLoss.numpy(), \"  train_acc: \", tr_accuracy.numpy())\n",
    "      \n",
    "      # Appending and print the information for validation instances\n",
    "      testLosses.append(te_currentLoss.numpy())\n",
    "      testAccuracies.append(te_accuracy.numpy())\n",
    "      print (\"Iteration \", i, \": test_loss = \",te_currentLoss.numpy(), \"  test_acc: \", te_accuracy.numpy())\n",
    "      print(\"*\"*60)\n",
    "    \n",
    "      #Calling Adam optimizer the for updating the weights and trainfing the data\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.003).apply_gradients(zip(gradients, [w,b])) \n",
    "        \n",
    "    \n",
    "    plt.style.use(\"ggplot\") \n",
    "    plt.figure()\n",
    "    plt.plot(testLosses, label=\"Val Loss\")\n",
    "    plt.plot(trainingLosses, label=\"Train Loss\")\n",
    "    plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
    "    plt.plot(testAccuracies, label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : train_loss =  2.4586134   train_acc:  0.89867\n",
      "Iteration  0 : test_loss =  2.454957   test_acc:  0.8988\n",
      "************************************************************\n",
      "Iteration  1 : train_loss =  2.1348352   train_acc:  0.9\n",
      "Iteration  1 : test_loss =  2.130685   test_acc:  0.89999\n",
      "************************************************************\n",
      "Iteration  2 : train_loss =  1.9038943   train_acc:  0.900125\n",
      "Iteration  2 : test_loss =  1.9007626   test_acc:  0.90019\n",
      "************************************************************\n",
      "Iteration  3 : train_loss =  1.7377236   train_acc:  0.90007335\n",
      "Iteration  3 : test_loss =  1.737575   test_acc:  0.90015\n",
      "************************************************************\n",
      "Iteration  4 : train_loss =  1.5869443   train_acc:  0.9039083\n",
      "Iteration  4 : test_loss =  1.5871294   test_acc:  0.90412\n",
      "************************************************************\n",
      "Iteration  5 : train_loss =  1.4825854   train_acc:  0.90663\n",
      "Iteration  5 : test_loss =  1.4858421   test_acc:  0.90661\n",
      "************************************************************\n",
      "Iteration  6 : train_loss =  1.3749719   train_acc:  0.9141117\n",
      "Iteration  6 : test_loss =  1.3772826   test_acc:  0.91407\n",
      "************************************************************\n",
      "Iteration  7 : train_loss =  1.3231345   train_acc:  0.915085\n",
      "Iteration  7 : test_loss =  1.3285412   test_acc:  0.91476\n",
      "************************************************************\n",
      "Iteration  8 : train_loss =  1.2411078   train_acc:  0.92149\n",
      "Iteration  8 : test_loss =  1.2454959   test_acc:  0.92128\n",
      "************************************************************\n",
      "Iteration  9 : train_loss =  1.2129761   train_acc:  0.9231433\n",
      "Iteration  9 : test_loss =  1.2205111   test_acc:  0.92276\n",
      "************************************************************\n",
      "Iteration  10 : train_loss =  1.1358317   train_acc:  0.9286517\n",
      "Iteration  10 : test_loss =  1.1412898   test_acc:  0.92858\n",
      "************************************************************\n",
      "Iteration  11 : train_loss =  1.1265292   train_acc:  0.92829335\n",
      "Iteration  11 : test_loss =  1.1350988   test_acc:  0.92769\n",
      "************************************************************\n",
      "Iteration  12 : train_loss =  1.0652719   train_acc:  0.93254\n",
      "Iteration  12 : test_loss =  1.0724688   test_acc:  0.93213\n",
      "************************************************************\n",
      "Iteration  13 : train_loss =  1.072293   train_acc:  0.93100333\n",
      "Iteration  13 : test_loss =  1.0818841   test_acc:  0.93043\n",
      "************************************************************\n",
      "Iteration  14 : train_loss =  1.0135541   train_acc:  0.934765\n",
      "Iteration  14 : test_loss =  1.0213783   test_acc:  0.93452\n",
      "************************************************************\n",
      "Iteration  15 : train_loss =  1.02749   train_acc:  0.93382\n",
      "Iteration  15 : test_loss =  1.038009   test_acc:  0.93312\n",
      "************************************************************\n",
      "Iteration  16 : train_loss =  0.9756522   train_acc:  0.9362367\n",
      "Iteration  16 : test_loss =  0.9842539   test_acc:  0.93605\n",
      "************************************************************\n",
      "Iteration  17 : train_loss =  0.99582636   train_acc:  0.9355717\n",
      "Iteration  17 : test_loss =  1.0068616   test_acc:  0.93502\n",
      "************************************************************\n",
      "Iteration  18 : train_loss =  0.9450403   train_acc:  0.937555\n",
      "Iteration  18 : test_loss =  0.9540209   test_acc:  0.93761\n",
      "************************************************************\n",
      "Iteration  19 : train_loss =  0.9646613   train_acc:  0.93736833\n",
      "Iteration  19 : test_loss =  0.97630644   test_acc:  0.93686\n",
      "************************************************************\n",
      "Iteration  20 : train_loss =  0.91821074   train_acc:  0.9385783\n",
      "Iteration  20 : test_loss =  0.9277321   test_acc:  0.93843\n",
      "************************************************************\n",
      "Iteration  21 : train_loss =  0.94323736   train_acc:  0.93856\n",
      "Iteration  21 : test_loss =  0.9552794   test_acc:  0.93782\n",
      "************************************************************\n",
      "Iteration  22 : train_loss =  0.89726615   train_acc:  0.93941\n",
      "Iteration  22 : test_loss =  0.9071424   test_acc:  0.93911\n",
      "************************************************************\n",
      "Iteration  23 : train_loss =  0.92158437   train_acc:  0.93968666\n",
      "Iteration  23 : test_loss =  0.9341262   test_acc:  0.93895\n",
      "************************************************************\n",
      "Iteration  24 : train_loss =  0.878039   train_acc:  0.9403383\n",
      "Iteration  24 : test_loss =  0.8883671   test_acc:  0.94019\n",
      "************************************************************\n",
      "Iteration  25 : train_loss =  0.90646565   train_acc:  0.9404917\n",
      "Iteration  25 : test_loss =  0.9193276   test_acc:  0.93942\n",
      "************************************************************\n",
      "Iteration  26 : train_loss =  0.8634285   train_acc:  0.9409533\n",
      "Iteration  26 : test_loss =  0.87408596   test_acc:  0.94075\n",
      "************************************************************\n",
      "Iteration  27 : train_loss =  0.8913143   train_acc:  0.941225\n",
      "Iteration  27 : test_loss =  0.9046128   test_acc:  0.94036\n",
      "************************************************************\n",
      "Iteration  28 : train_loss =  0.8496508   train_acc:  0.94165\n",
      "Iteration  28 : test_loss =  0.8607086   test_acc:  0.9414\n",
      "************************************************************\n",
      "Iteration  29 : train_loss =  0.87945986   train_acc:  0.9418333\n",
      "Iteration  29 : test_loss =  0.89308673   test_acc:  0.94098\n",
      "************************************************************\n",
      "Iteration  30 : train_loss =  0.83836794   train_acc:  0.94215167\n",
      "Iteration  30 : test_loss =  0.849801   test_acc:  0.94197\n",
      "************************************************************\n",
      "Iteration  31 : train_loss =  0.86802214   train_acc:  0.9424483\n",
      "Iteration  31 : test_loss =  0.88205916   test_acc:  0.94161\n",
      "************************************************************\n",
      "Iteration  32 : train_loss =  0.8278249   train_acc:  0.9427367\n",
      "Iteration  32 : test_loss =  0.8396295   test_acc:  0.94249\n",
      "************************************************************\n",
      "Iteration  33 : train_loss =  0.8582538   train_acc:  0.94308335\n",
      "Iteration  33 : test_loss =  0.8726115   test_acc:  0.94204\n",
      "************************************************************\n",
      "Iteration  34 : train_loss =  0.81880224   train_acc:  0.943145\n",
      "Iteration  34 : test_loss =  0.830983   test_acc:  0.94292\n",
      "************************************************************\n",
      "Iteration  35 : train_loss =  0.84943134   train_acc:  0.94364834\n",
      "Iteration  35 : test_loss =  0.86415994   test_acc:  0.94249\n",
      "************************************************************\n",
      "Iteration  36 : train_loss =  0.81050974   train_acc:  0.9435883\n",
      "Iteration  36 : test_loss =  0.8230516   test_acc:  0.94327\n",
      "************************************************************\n",
      "Iteration  37 : train_loss =  0.8418691   train_acc:  0.94405335\n",
      "Iteration  37 : test_loss =  0.85692793   test_acc:  0.94286\n",
      "************************************************************\n",
      "Iteration  38 : train_loss =  0.80328035   train_acc:  0.9439667\n",
      "Iteration  38 : test_loss =  0.8161694   test_acc:  0.94379\n",
      "************************************************************\n",
      "Iteration  39 : train_loss =  0.8345367   train_acc:  0.9444183\n",
      "Iteration  39 : test_loss =  0.84992033   test_acc:  0.94314\n",
      "************************************************************\n",
      "Iteration  40 : train_loss =  0.79638344   train_acc:  0.944405\n",
      "Iteration  40 : test_loss =  0.80960625   test_acc:  0.94424\n",
      "************************************************************\n",
      "Iteration  41 : train_loss =  0.828203   train_acc:  0.944745\n",
      "Iteration  41 : test_loss =  0.8438597   test_acc:  0.94348\n",
      "************************************************************\n",
      "Iteration  42 : train_loss =  0.78996485   train_acc:  0.94474167\n",
      "Iteration  42 : test_loss =  0.80349797   test_acc:  0.94452\n",
      "************************************************************\n",
      "Iteration  43 : train_loss =  0.8217943   train_acc:  0.94509166\n",
      "Iteration  43 : test_loss =  0.83776337   test_acc:  0.94383\n",
      "************************************************************\n",
      "Iteration  44 : train_loss =  0.78398085   train_acc:  0.945125\n",
      "Iteration  44 : test_loss =  0.7978425   test_acc:  0.9448\n",
      "************************************************************\n",
      "Iteration  45 : train_loss =  0.81623995   train_acc:  0.94543666\n",
      "Iteration  45 : test_loss =  0.83245057   test_acc:  0.94393\n",
      "************************************************************\n",
      "Iteration  46 : train_loss =  0.7785351   train_acc:  0.94547665\n",
      "Iteration  46 : test_loss =  0.79267246   test_acc:  0.94502\n",
      "************************************************************\n",
      "Iteration  47 : train_loss =  0.81072915   train_acc:  0.9456483\n",
      "Iteration  47 : test_loss =  0.8271931   test_acc:  0.94431\n",
      "************************************************************\n",
      "Iteration  48 : train_loss =  0.7731944   train_acc:  0.9458017\n",
      "Iteration  48 : test_loss =  0.7876258   test_acc:  0.94531\n",
      "************************************************************\n",
      "Iteration  49 : train_loss =  0.8057631   train_acc:  0.94595164\n",
      "Iteration  49 : test_loss =  0.8224756   test_acc:  0.94465\n",
      "************************************************************\n",
      "Iteration  50 : train_loss =  0.7684737   train_acc:  0.9460883\n",
      "Iteration  50 : test_loss =  0.7831734   test_acc:  0.94556\n",
      "************************************************************\n",
      "Iteration  51 : train_loss =  0.8009931   train_acc:  0.94620836\n",
      "Iteration  51 : test_loss =  0.8179488   test_acc:  0.94493\n",
      "************************************************************\n",
      "Iteration  52 : train_loss =  0.763894   train_acc:  0.94636\n",
      "Iteration  52 : test_loss =  0.77887195   test_acc:  0.94587\n",
      "************************************************************\n",
      "Iteration  53 : train_loss =  0.79655015   train_acc:  0.94646335\n",
      "Iteration  53 : test_loss =  0.813752   test_acc:  0.94534\n",
      "************************************************************\n",
      "Iteration  54 : train_loss =  0.7596319   train_acc:  0.94662666\n",
      "Iteration  54 : test_loss =  0.77486974   test_acc:  0.94602\n",
      "************************************************************\n",
      "Iteration  55 : train_loss =  0.7923238   train_acc:  0.94668835\n",
      "Iteration  55 : test_loss =  0.8097514   test_acc:  0.94564\n",
      "************************************************************\n",
      "Iteration  56 : train_loss =  0.7557266   train_acc:  0.94684666\n",
      "Iteration  56 : test_loss =  0.7712093   test_acc:  0.94617\n",
      "************************************************************\n",
      "Iteration  57 : train_loss =  0.78836143   train_acc:  0.94689834\n",
      "Iteration  57 : test_loss =  0.80601066   test_acc:  0.94587\n",
      "************************************************************\n",
      "Iteration  58 : train_loss =  0.75181174   train_acc:  0.94709665\n",
      "Iteration  58 : test_loss =  0.76753634   test_acc:  0.94627\n",
      "************************************************************\n",
      "Iteration  59 : train_loss =  0.7846996   train_acc:  0.94710666\n",
      "Iteration  59 : test_loss =  0.80255085   test_acc:  0.94603\n",
      "************************************************************\n",
      "Iteration  60 : train_loss =  0.74828637   train_acc:  0.94730836\n",
      "Iteration  60 : test_loss =  0.76422834   test_acc:  0.94648\n",
      "************************************************************\n",
      "Iteration  61 : train_loss =  0.7812392   train_acc:  0.94726336\n",
      "Iteration  61 : test_loss =  0.79929847   test_acc:  0.94608\n",
      "************************************************************\n",
      "Iteration  62 : train_loss =  0.74480534   train_acc:  0.9475733\n",
      "Iteration  62 : test_loss =  0.76097333   test_acc:  0.94671\n",
      "************************************************************\n",
      "Iteration  63 : train_loss =  0.77789384   train_acc:  0.9474317\n",
      "Iteration  63 : test_loss =  0.7961641   test_acc:  0.94634\n",
      "************************************************************\n",
      "Iteration  64 : train_loss =  0.74163723   train_acc:  0.94774336\n",
      "Iteration  64 : test_loss =  0.7580198   test_acc:  0.94689\n",
      "************************************************************\n",
      "Iteration  65 : train_loss =  0.77470237   train_acc:  0.947655\n",
      "Iteration  65 : test_loss =  0.7931747   test_acc:  0.94645\n",
      "************************************************************\n",
      "Iteration  66 : train_loss =  0.73835886   train_acc:  0.947945\n",
      "Iteration  66 : test_loss =  0.75495344   test_acc:  0.94705\n",
      "************************************************************\n",
      "Iteration  67 : train_loss =  0.7717283   train_acc:  0.9478267\n",
      "Iteration  67 : test_loss =  0.79039454   test_acc:  0.9467\n",
      "************************************************************\n",
      "Iteration  68 : train_loss =  0.73557615   train_acc:  0.948125\n",
      "Iteration  68 : test_loss =  0.7523681   test_acc:  0.94716\n",
      "************************************************************\n",
      "Iteration  69 : train_loss =  0.7689039   train_acc:  0.948\n",
      "Iteration  69 : test_loss =  0.7877534   test_acc:  0.94691\n",
      "************************************************************\n",
      "Iteration  70 : train_loss =  0.73251206   train_acc:  0.9483567\n",
      "Iteration  70 : test_loss =  0.7494756   test_acc:  0.94724\n",
      "************************************************************\n",
      "Iteration  71 : train_loss =  0.7662194   train_acc:  0.94812167\n",
      "Iteration  71 : test_loss =  0.7852361   test_acc:  0.94701\n",
      "************************************************************\n",
      "Iteration  72 : train_loss =  0.7300453   train_acc:  0.9485033\n",
      "Iteration  72 : test_loss =  0.7471967   test_acc:  0.94744\n",
      "************************************************************\n",
      "Iteration  73 : train_loss =  0.76368076   train_acc:  0.948235\n",
      "Iteration  73 : test_loss =  0.78287053   test_acc:  0.94716\n",
      "************************************************************\n",
      "Iteration  74 : train_loss =  0.72733843   train_acc:  0.94869834\n",
      "Iteration  74 : test_loss =  0.7446465   test_acc:  0.9477\n",
      "************************************************************\n",
      "Iteration  75 : train_loss =  0.76123446   train_acc:  0.948375\n",
      "Iteration  75 : test_loss =  0.78057307   test_acc:  0.9473\n",
      "************************************************************\n",
      "Iteration  76 : train_loss =  0.72489506   train_acc:  0.94884664\n",
      "Iteration  76 : test_loss =  0.74235886   test_acc:  0.94787\n",
      "************************************************************\n",
      "Iteration  77 : train_loss =  0.75891745   train_acc:  0.94851834\n",
      "Iteration  77 : test_loss =  0.7784093   test_acc:  0.94753\n",
      "************************************************************\n",
      "Iteration  78 : train_loss =  0.7226124   train_acc:  0.9489767\n",
      "Iteration  78 : test_loss =  0.7402202   test_acc:  0.94796\n",
      "************************************************************\n",
      "Iteration  79 : train_loss =  0.75665665   train_acc:  0.9486433\n",
      "Iteration  79 : test_loss =  0.7762903   test_acc:  0.94753\n",
      "************************************************************\n",
      "Iteration  80 : train_loss =  0.7202595   train_acc:  0.9491583\n",
      "Iteration  80 : test_loss =  0.7380211   test_acc:  0.94808\n",
      "************************************************************\n",
      "Iteration  81 : train_loss =  0.7544748   train_acc:  0.94879335\n",
      "Iteration  81 : test_loss =  0.77425367   test_acc:  0.94772\n",
      "************************************************************\n",
      "Iteration  82 : train_loss =  0.71810454   train_acc:  0.94928336\n",
      "Iteration  82 : test_loss =  0.73600566   test_acc:  0.94823\n",
      "************************************************************\n",
      "Iteration  83 : train_loss =  0.7524046   train_acc:  0.9489383\n",
      "Iteration  83 : test_loss =  0.7723243   test_acc:  0.94779\n",
      "************************************************************\n",
      "Iteration  84 : train_loss =  0.716056   train_acc:  0.9494233\n",
      "Iteration  84 : test_loss =  0.73410076   test_acc:  0.94837\n",
      "************************************************************\n",
      "Iteration  85 : train_loss =  0.7503876   train_acc:  0.94906336\n",
      "Iteration  85 : test_loss =  0.7704463   test_acc:  0.9479\n",
      "************************************************************\n",
      "Iteration  86 : train_loss =  0.7140141   train_acc:  0.94959\n",
      "Iteration  86 : test_loss =  0.7322034   test_acc:  0.94849\n",
      "************************************************************\n",
      "Iteration  87 : train_loss =  0.7484731   train_acc:  0.94918\n",
      "Iteration  87 : test_loss =  0.7686664   test_acc:  0.94803\n",
      "************************************************************\n",
      "Iteration  88 : train_loss =  0.71213645   train_acc:  0.94973\n",
      "Iteration  88 : test_loss =  0.730466   test_acc:  0.9486\n",
      "************************************************************\n",
      "Iteration  89 : train_loss =  0.7466548   train_acc:  0.9492817\n",
      "Iteration  89 : test_loss =  0.76698124   test_acc:  0.94816\n",
      "************************************************************\n",
      "Iteration  90 : train_loss =  0.71010995   train_acc:  0.949885\n",
      "Iteration  90 : test_loss =  0.72857755   test_acc:  0.94873\n",
      "************************************************************\n",
      "Iteration  91 : train_loss =  0.7448922   train_acc:  0.9493783\n",
      "Iteration  91 : test_loss =  0.76534694   test_acc:  0.94827\n",
      "************************************************************\n",
      "Iteration  92 : train_loss =  0.7084613   train_acc:  0.94999\n",
      "Iteration  92 : test_loss =  0.72707146   test_acc:  0.9488\n",
      "************************************************************\n",
      "Iteration  93 : train_loss =  0.74326664   train_acc:  0.94946\n",
      "Iteration  93 : test_loss =  0.7638503   test_acc:  0.94832\n",
      "************************************************************\n",
      "Iteration  94 : train_loss =  0.70662355   train_acc:  0.9501067\n",
      "Iteration  94 : test_loss =  0.725343   test_acc:  0.94881\n",
      "************************************************************\n",
      "Iteration  95 : train_loss =  0.74170923   train_acc:  0.9495467\n",
      "Iteration  95 : test_loss =  0.76239973   test_acc:  0.94841\n",
      "************************************************************\n",
      "Iteration  96 : train_loss =  0.70518047   train_acc:  0.95019\n",
      "Iteration  96 : test_loss =  0.72403735   test_acc:  0.9489\n",
      "************************************************************\n",
      "Iteration  97 : train_loss =  0.7402271   train_acc:  0.949625\n",
      "Iteration  97 : test_loss =  0.7610457   test_acc:  0.94849\n",
      "************************************************************\n",
      "Iteration  98 : train_loss =  0.70325273   train_acc:  0.95032334\n",
      "Iteration  98 : test_loss =  0.72222793   test_acc:  0.94896\n",
      "************************************************************\n",
      "Iteration  99 : train_loss =  0.7387407   train_acc:  0.9497\n",
      "Iteration  99 : test_loss =  0.7596747   test_acc:  0.94857\n",
      "************************************************************\n",
      "Iteration  100 : train_loss =  0.70194143   train_acc:  0.95043164\n",
      "Iteration  100 : test_loss =  0.7210508   test_acc:  0.94902\n",
      "************************************************************\n",
      "Iteration  101 : train_loss =  0.73734283   train_acc:  0.94978666\n",
      "Iteration  101 : test_loss =  0.758408   test_acc:  0.94867\n",
      "************************************************************\n",
      "Iteration  102 : train_loss =  0.7003757   train_acc:  0.95051\n",
      "Iteration  102 : test_loss =  0.7195811   test_acc:  0.94918\n",
      "************************************************************\n",
      "Iteration  103 : train_loss =  0.73603696   train_acc:  0.94986\n",
      "Iteration  103 : test_loss =  0.7571957   test_acc:  0.94874\n",
      "************************************************************\n",
      "Iteration  104 : train_loss =  0.69914365   train_acc:  0.950585\n",
      "Iteration  104 : test_loss =  0.7184749   test_acc:  0.94921\n",
      "************************************************************\n",
      "Iteration  105 : train_loss =  0.73469985   train_acc:  0.94993\n",
      "Iteration  105 : test_loss =  0.7559835   test_acc:  0.94878\n",
      "************************************************************\n",
      "Iteration  106 : train_loss =  0.6976486   train_acc:  0.95067\n",
      "Iteration  106 : test_loss =  0.71707547   test_acc:  0.94931\n",
      "************************************************************\n",
      "Iteration  107 : train_loss =  0.7334613   train_acc:  0.949995\n",
      "Iteration  107 : test_loss =  0.7548384   test_acc:  0.94887\n",
      "************************************************************\n",
      "Iteration  108 : train_loss =  0.6965278   train_acc:  0.950735\n",
      "Iteration  108 : test_loss =  0.7160762   test_acc:  0.94943\n",
      "************************************************************\n",
      "Iteration  109 : train_loss =  0.7323592   train_acc:  0.95006\n",
      "Iteration  109 : test_loss =  0.7538451   test_acc:  0.9489\n",
      "************************************************************\n",
      "Iteration  110 : train_loss =  0.69534314   train_acc:  0.9508517\n",
      "Iteration  110 : test_loss =  0.7149878   test_acc:  0.94946\n",
      "************************************************************\n",
      "Iteration  111 : train_loss =  0.7312649   train_acc:  0.950135\n",
      "Iteration  111 : test_loss =  0.75284696   test_acc:  0.94902\n",
      "************************************************************\n",
      "Iteration  112 : train_loss =  0.6943499   train_acc:  0.9509183\n",
      "Iteration  112 : test_loss =  0.7140989   test_acc:  0.94953\n",
      "************************************************************\n",
      "Iteration  113 : train_loss =  0.73023   train_acc:  0.95017666\n",
      "Iteration  113 : test_loss =  0.7519125   test_acc:  0.9491\n",
      "************************************************************\n",
      "Iteration  114 : train_loss =  0.69314444   train_acc:  0.9510033\n",
      "Iteration  114 : test_loss =  0.71297234   test_acc:  0.94964\n",
      "************************************************************\n",
      "Iteration  115 : train_loss =  0.7292319   train_acc:  0.95022833\n",
      "Iteration  115 : test_loss =  0.7509918   test_acc:  0.94912\n",
      "************************************************************\n",
      "Iteration  116 : train_loss =  0.692252   train_acc:  0.9510533\n",
      "Iteration  116 : test_loss =  0.71218204   test_acc:  0.94975\n",
      "************************************************************\n",
      "Iteration  117 : train_loss =  0.7282065   train_acc:  0.95029336\n",
      "Iteration  117 : test_loss =  0.75006825   test_acc:  0.94915\n",
      "************************************************************\n",
      "Iteration  118 : train_loss =  0.69126964   train_acc:  0.95113\n",
      "Iteration  118 : test_loss =  0.71128875   test_acc:  0.94982\n",
      "************************************************************\n",
      "Iteration  119 : train_loss =  0.727282   train_acc:  0.95034665\n",
      "Iteration  119 : test_loss =  0.7492289   test_acc:  0.94924\n",
      "************************************************************\n",
      "Iteration  120 : train_loss =  0.6904275   train_acc:  0.95118\n",
      "Iteration  120 : test_loss =  0.7105366   test_acc:  0.94983\n",
      "************************************************************\n",
      "Iteration  121 : train_loss =  0.7264205   train_acc:  0.950365\n",
      "Iteration  121 : test_loss =  0.7484535   test_acc:  0.94927\n",
      "************************************************************\n",
      "Iteration  122 : train_loss =  0.6896086   train_acc:  0.9512467\n",
      "Iteration  122 : test_loss =  0.70979685   test_acc:  0.94987\n",
      "************************************************************\n",
      "Iteration  123 : train_loss =  0.7256236   train_acc:  0.9503833\n",
      "Iteration  123 : test_loss =  0.74773145   test_acc:  0.94928\n",
      "************************************************************\n",
      "Iteration  124 : train_loss =  0.6888336   train_acc:  0.95130664\n",
      "Iteration  124 : test_loss =  0.7090981   test_acc:  0.94992\n",
      "************************************************************\n",
      "Iteration  125 : train_loss =  0.7248263   train_acc:  0.95042\n",
      "Iteration  125 : test_loss =  0.7470076   test_acc:  0.94934\n",
      "************************************************************\n",
      "Iteration  126 : train_loss =  0.68810797   train_acc:  0.95133835\n",
      "Iteration  126 : test_loss =  0.70844924   test_acc:  0.94999\n",
      "************************************************************\n",
      "Iteration  127 : train_loss =  0.7240692   train_acc:  0.9504517\n",
      "Iteration  127 : test_loss =  0.7463269   test_acc:  0.94933\n",
      "************************************************************\n",
      "Iteration  128 : train_loss =  0.6874036   train_acc:  0.951395\n",
      "Iteration  128 : test_loss =  0.70781463   test_acc:  0.95001\n",
      "************************************************************\n",
      "Iteration  129 : train_loss =  0.7233166   train_acc:  0.9504833\n",
      "Iteration  129 : test_loss =  0.7456405   test_acc:  0.94932\n",
      "************************************************************\n",
      "Iteration  130 : train_loss =  0.68672824   train_acc:  0.95143664\n",
      "Iteration  130 : test_loss =  0.70720613   test_acc:  0.95013\n",
      "************************************************************\n",
      "Iteration  131 : train_loss =  0.7226503   train_acc:  0.950525\n",
      "Iteration  131 : test_loss =  0.7450364   test_acc:  0.94935\n",
      "************************************************************\n",
      "Iteration  132 : train_loss =  0.6860387   train_acc:  0.95149666\n",
      "Iteration  132 : test_loss =  0.70658255   test_acc:  0.95018\n",
      "************************************************************\n",
      "Iteration  133 : train_loss =  0.7219374   train_acc:  0.95056665\n",
      "Iteration  133 : test_loss =  0.74438816   test_acc:  0.94937\n",
      "************************************************************\n",
      "Iteration  134 : train_loss =  0.68535715   train_acc:  0.95155334\n",
      "Iteration  134 : test_loss =  0.70596915   test_acc:  0.95018\n",
      "************************************************************\n",
      "Iteration  135 : train_loss =  0.72126275   train_acc:  0.95061666\n",
      "Iteration  135 : test_loss =  0.74378073   test_acc:  0.94942\n",
      "************************************************************\n",
      "Iteration  136 : train_loss =  0.68478715   train_acc:  0.95158833\n",
      "Iteration  136 : test_loss =  0.70546526   test_acc:  0.95021\n",
      "************************************************************\n",
      "Iteration  137 : train_loss =  0.72063255   train_acc:  0.95064664\n",
      "Iteration  137 : test_loss =  0.74321014   test_acc:  0.9494\n",
      "************************************************************\n",
      "Iteration  138 : train_loss =  0.68419254   train_acc:  0.95164835\n",
      "Iteration  138 : test_loss =  0.70493734   test_acc:  0.95029\n",
      "************************************************************\n",
      "Iteration  139 : train_loss =  0.7200312   train_acc:  0.95068\n",
      "Iteration  139 : test_loss =  0.7426749   test_acc:  0.94944\n",
      "************************************************************\n",
      "Iteration  140 : train_loss =  0.683656   train_acc:  0.9516683\n",
      "Iteration  140 : test_loss =  0.7044626   test_acc:  0.95034\n",
      "************************************************************\n",
      "Iteration  141 : train_loss =  0.71947074   train_acc:  0.9507167\n",
      "Iteration  141 : test_loss =  0.74217623   test_acc:  0.9495\n",
      "************************************************************\n",
      "Iteration  142 : train_loss =  0.6831469   train_acc:  0.95170665\n",
      "Iteration  142 : test_loss =  0.70401925   test_acc:  0.9504\n",
      "************************************************************\n",
      "Iteration  143 : train_loss =  0.7189238   train_acc:  0.9507617\n",
      "Iteration  143 : test_loss =  0.7416926   test_acc:  0.94953\n",
      "************************************************************\n",
      "Iteration  144 : train_loss =  0.68262774   train_acc:  0.95176834\n",
      "Iteration  144 : test_loss =  0.70356154   test_acc:  0.95044\n",
      "************************************************************\n",
      "Iteration  145 : train_loss =  0.7183804   train_acc:  0.95080334\n",
      "Iteration  145 : test_loss =  0.74121016   test_acc:  0.94956\n",
      "************************************************************\n",
      "Iteration  146 : train_loss =  0.6821362   train_acc:  0.951815\n",
      "Iteration  146 : test_loss =  0.7031345   test_acc:  0.95052\n",
      "************************************************************\n",
      "Iteration  147 : train_loss =  0.71785575   train_acc:  0.95084834\n",
      "Iteration  147 : test_loss =  0.74074954   test_acc:  0.94966\n",
      "************************************************************\n",
      "Iteration  148 : train_loss =  0.68162817   train_acc:  0.951845\n",
      "Iteration  148 : test_loss =  0.70268965   test_acc:  0.95057\n",
      "************************************************************\n",
      "Iteration  149 : train_loss =  0.717299   train_acc:  0.95088\n",
      "Iteration  149 : test_loss =  0.74025416   test_acc:  0.94968\n",
      "************************************************************\n",
      "Iteration  150 : train_loss =  0.68115455   train_acc:  0.9518567\n",
      "Iteration  150 : test_loss =  0.7022812   test_acc:  0.95056\n",
      "************************************************************\n",
      "Iteration  151 : train_loss =  0.7167979   train_acc:  0.950915\n",
      "Iteration  151 : test_loss =  0.73982114   test_acc:  0.94974\n",
      "************************************************************\n",
      "Iteration  152 : train_loss =  0.6806936   train_acc:  0.95190334\n",
      "Iteration  152 : test_loss =  0.70188516   test_acc:  0.95059\n",
      "************************************************************\n",
      "Iteration  153 : train_loss =  0.7162897   train_acc:  0.95092666\n",
      "Iteration  153 : test_loss =  0.73937553   test_acc:  0.9498\n",
      "************************************************************\n",
      "Iteration  154 : train_loss =  0.68024933   train_acc:  0.95193166\n",
      "Iteration  154 : test_loss =  0.7015058   test_acc:  0.95062\n",
      "************************************************************\n",
      "Iteration  155 : train_loss =  0.71580833   train_acc:  0.9509683\n",
      "Iteration  155 : test_loss =  0.7389599   test_acc:  0.94987\n",
      "************************************************************\n",
      "Iteration  156 : train_loss =  0.67981356   train_acc:  0.95194834\n",
      "Iteration  156 : test_loss =  0.70113623   test_acc:  0.95067\n",
      "************************************************************\n",
      "Iteration  157 : train_loss =  0.71532893   train_acc:  0.95100164\n",
      "Iteration  157 : test_loss =  0.7385447   test_acc:  0.94988\n",
      "************************************************************\n",
      "Iteration  158 : train_loss =  0.6793891   train_acc:  0.95197\n",
      "Iteration  158 : test_loss =  0.7007778   test_acc:  0.9507\n",
      "************************************************************\n",
      "Iteration  159 : train_loss =  0.71486443   train_acc:  0.95104\n",
      "Iteration  159 : test_loss =  0.73814446   test_acc:  0.94989\n",
      "************************************************************\n",
      "Iteration  160 : train_loss =  0.67897487   train_acc:  0.9520017\n",
      "Iteration  160 : test_loss =  0.70042914   test_acc:  0.95068\n",
      "************************************************************\n",
      "Iteration  161 : train_loss =  0.7144055   train_acc:  0.951065\n",
      "Iteration  161 : test_loss =  0.73774856   test_acc:  0.94993\n",
      "************************************************************\n",
      "Iteration  162 : train_loss =  0.67857325   train_acc:  0.95201665\n",
      "Iteration  162 : test_loss =  0.70009387   test_acc:  0.95067\n",
      "************************************************************\n",
      "Iteration  163 : train_loss =  0.7139598   train_acc:  0.951075\n",
      "Iteration  163 : test_loss =  0.7373677   test_acc:  0.94994\n",
      "************************************************************\n",
      "Iteration  164 : train_loss =  0.67818296   train_acc:  0.9520233\n",
      "Iteration  164 : test_loss =  0.69976985   test_acc:  0.95069\n",
      "************************************************************\n",
      "Iteration  165 : train_loss =  0.7135262   train_acc:  0.951135\n",
      "Iteration  165 : test_loss =  0.7369981   test_acc:  0.95003\n",
      "************************************************************\n",
      "Iteration  166 : train_loss =  0.67780584   train_acc:  0.9520533\n",
      "Iteration  166 : test_loss =  0.699459   test_acc:  0.95075\n",
      "************************************************************\n",
      "Iteration  167 : train_loss =  0.71310115   train_acc:  0.951155\n",
      "Iteration  167 : test_loss =  0.7366387   test_acc:  0.95009\n",
      "************************************************************\n",
      "Iteration  168 : train_loss =  0.677442   train_acc:  0.95209664\n",
      "Iteration  168 : test_loss =  0.69916123   test_acc:  0.95073\n",
      "************************************************************\n",
      "Iteration  169 : train_loss =  0.7126808   train_acc:  0.95118\n",
      "Iteration  169 : test_loss =  0.7362845   test_acc:  0.95011\n",
      "************************************************************\n",
      "Iteration  170 : train_loss =  0.6770889   train_acc:  0.95211667\n",
      "Iteration  170 : test_loss =  0.6988744   test_acc:  0.95078\n",
      "************************************************************\n",
      "Iteration  171 : train_loss =  0.71226734   train_acc:  0.9512183\n",
      "Iteration  171 : test_loss =  0.7359372   test_acc:  0.95011\n",
      "************************************************************\n",
      "Iteration  172 : train_loss =  0.6767414   train_acc:  0.95212835\n",
      "Iteration  172 : test_loss =  0.6985927   test_acc:  0.95078\n",
      "************************************************************\n",
      "Iteration  173 : train_loss =  0.7118599   train_acc:  0.95125335\n",
      "Iteration  173 : test_loss =  0.7355956   test_acc:  0.95014\n",
      "************************************************************\n",
      "Iteration  174 : train_loss =  0.67640126   train_acc:  0.95215166\n",
      "Iteration  174 : test_loss =  0.69831705   test_acc:  0.9508\n",
      "************************************************************\n",
      "Iteration  175 : train_loss =  0.71145654   train_acc:  0.95126\n",
      "Iteration  175 : test_loss =  0.7352569   test_acc:  0.95015\n",
      "************************************************************\n",
      "Iteration  176 : train_loss =  0.6760634   train_acc:  0.95215666\n",
      "Iteration  176 : test_loss =  0.698042   test_acc:  0.95083\n",
      "************************************************************\n",
      "Iteration  177 : train_loss =  0.71105933   train_acc:  0.95127666\n",
      "Iteration  177 : test_loss =  0.73492223   test_acc:  0.95014\n",
      "************************************************************\n",
      "Iteration  178 : train_loss =  0.6757272   train_acc:  0.952185\n",
      "Iteration  178 : test_loss =  0.6977674   test_acc:  0.95086\n",
      "************************************************************\n",
      "Iteration  179 : train_loss =  0.7106677   train_acc:  0.95129\n",
      "Iteration  179 : test_loss =  0.7345916   test_acc:  0.95016\n",
      "************************************************************\n",
      "Iteration  180 : train_loss =  0.67540246   train_acc:  0.952225\n",
      "Iteration  180 : test_loss =  0.6975038   test_acc:  0.95091\n",
      "************************************************************\n",
      "Iteration  181 : train_loss =  0.71027863   train_acc:  0.951305\n",
      "Iteration  181 : test_loss =  0.73426366   test_acc:  0.95022\n",
      "************************************************************\n",
      "Iteration  182 : train_loss =  0.6750735   train_acc:  0.95225\n",
      "Iteration  182 : test_loss =  0.6972347   test_acc:  0.95092\n",
      "************************************************************\n",
      "Iteration  183 : train_loss =  0.70989674   train_acc:  0.95133\n",
      "Iteration  183 : test_loss =  0.73394084   test_acc:  0.95024\n",
      "************************************************************\n",
      "Iteration  184 : train_loss =  0.67475843   train_acc:  0.952285\n",
      "Iteration  184 : test_loss =  0.6969791   test_acc:  0.95095\n",
      "************************************************************\n",
      "Iteration  185 : train_loss =  0.7095177   train_acc:  0.95135\n",
      "Iteration  185 : test_loss =  0.7336211   test_acc:  0.95024\n",
      "************************************************************\n",
      "Iteration  186 : train_loss =  0.6744433   train_acc:  0.95230836\n",
      "Iteration  186 : test_loss =  0.6967224   test_acc:  0.95096\n",
      "************************************************************\n",
      "Iteration  187 : train_loss =  0.7091452   train_acc:  0.95137\n",
      "Iteration  187 : test_loss =  0.73330593   test_acc:  0.95028\n",
      "************************************************************\n",
      "Iteration  188 : train_loss =  0.6741357   train_acc:  0.95233166\n",
      "Iteration  188 : test_loss =  0.6964728   test_acc:  0.95096\n",
      "************************************************************\n",
      "Iteration  189 : train_loss =  0.70877737   train_acc:  0.951405\n",
      "Iteration  189 : test_loss =  0.73299426   test_acc:  0.95029\n",
      "************************************************************\n",
      "Iteration  190 : train_loss =  0.6738331   train_acc:  0.95235\n",
      "Iteration  190 : test_loss =  0.69622695   test_acc:  0.951\n",
      "************************************************************\n",
      "Iteration  191 : train_loss =  0.7084156   train_acc:  0.95142\n",
      "Iteration  191 : test_loss =  0.73268634   test_acc:  0.9503\n",
      "************************************************************\n",
      "Iteration  192 : train_loss =  0.6735367   train_acc:  0.95237\n",
      "Iteration  192 : test_loss =  0.69598556   test_acc:  0.95103\n",
      "************************************************************\n",
      "Iteration  193 : train_loss =  0.7080568   train_acc:  0.951445\n",
      "Iteration  193 : test_loss =  0.73238206   test_acc:  0.95032\n",
      "************************************************************\n",
      "Iteration  194 : train_loss =  0.6732502   train_acc:  0.95238835\n",
      "Iteration  194 : test_loss =  0.69575536   test_acc:  0.95108\n",
      "************************************************************\n",
      "Iteration  195 : train_loss =  0.7077025   train_acc:  0.951475\n",
      "Iteration  195 : test_loss =  0.7320829   test_acc:  0.95031\n",
      "************************************************************\n",
      "Iteration  196 : train_loss =  0.6729703   train_acc:  0.9524317\n",
      "Iteration  196 : test_loss =  0.6955316   test_acc:  0.95107\n",
      "************************************************************\n",
      "Iteration  197 : train_loss =  0.70735157   train_acc:  0.95150167\n",
      "Iteration  197 : test_loss =  0.73178804   test_acc:  0.95033\n",
      "************************************************************\n",
      "Iteration  198 : train_loss =  0.6726945   train_acc:  0.95244664\n",
      "Iteration  198 : test_loss =  0.6953119   test_acc:  0.95109\n",
      "************************************************************\n",
      "Iteration  199 : train_loss =  0.70700455   train_acc:  0.95151335\n",
      "Iteration  199 : test_loss =  0.73149717   test_acc:  0.95034\n",
      "************************************************************\n",
      "Iteration  200 : train_loss =  0.6724224   train_acc:  0.95246166\n",
      "Iteration  200 : test_loss =  0.6950959   test_acc:  0.9511\n",
      "************************************************************\n",
      "Iteration  201 : train_loss =  0.70666087   train_acc:  0.9515283\n",
      "Iteration  201 : test_loss =  0.7312099   test_acc:  0.95037\n",
      "************************************************************\n",
      "Iteration  202 : train_loss =  0.6721538   train_acc:  0.952485\n",
      "Iteration  202 : test_loss =  0.69488335   test_acc:  0.95113\n",
      "************************************************************\n",
      "Iteration  203 : train_loss =  0.7063209   train_acc:  0.951545\n",
      "Iteration  203 : test_loss =  0.73092586   test_acc:  0.95037\n",
      "************************************************************\n",
      "Iteration  204 : train_loss =  0.6718888   train_acc:  0.952505\n",
      "Iteration  204 : test_loss =  0.6946742   test_acc:  0.95113\n",
      "************************************************************\n",
      "Iteration  205 : train_loss =  0.70598465   train_acc:  0.95157665\n",
      "Iteration  205 : test_loss =  0.7306456   test_acc:  0.95035\n",
      "************************************************************\n",
      "Iteration  206 : train_loss =  0.6716279   train_acc:  0.95251834\n",
      "Iteration  206 : test_loss =  0.6944692   test_acc:  0.9512\n",
      "************************************************************\n",
      "Iteration  207 : train_loss =  0.7056522   train_acc:  0.9515967\n",
      "Iteration  207 : test_loss =  0.7303689   test_acc:  0.95036\n",
      "************************************************************\n",
      "Iteration  208 : train_loss =  0.67137253   train_acc:  0.9525333\n",
      "Iteration  208 : test_loss =  0.6942699   test_acc:  0.95121\n",
      "************************************************************\n",
      "Iteration  209 : train_loss =  0.7053245   train_acc:  0.9516133\n",
      "Iteration  209 : test_loss =  0.7300966   test_acc:  0.95037\n",
      "************************************************************\n",
      "Iteration  210 : train_loss =  0.67112446   train_acc:  0.95256\n",
      "Iteration  210 : test_loss =  0.6940776   test_acc:  0.95126\n",
      "************************************************************\n",
      "Iteration  211 : train_loss =  0.7050005   train_acc:  0.95162666\n",
      "Iteration  211 : test_loss =  0.7298287   test_acc:  0.95035\n",
      "************************************************************\n",
      "Iteration  212 : train_loss =  0.67088294   train_acc:  0.95257\n",
      "Iteration  212 : test_loss =  0.6938923   test_acc:  0.95136\n",
      "************************************************************\n",
      "Iteration  213 : train_loss =  0.7046811   train_acc:  0.9516517\n",
      "Iteration  213 : test_loss =  0.7295658   test_acc:  0.95037\n",
      "************************************************************\n",
      "Iteration  214 : train_loss =  0.6706448   train_acc:  0.95257664\n",
      "Iteration  214 : test_loss =  0.69371074   test_acc:  0.95136\n",
      "************************************************************\n",
      "Iteration  215 : train_loss =  0.7043677   train_acc:  0.9516633\n",
      "Iteration  215 : test_loss =  0.72930926   test_acc:  0.9504\n",
      "************************************************************\n",
      "Iteration  216 : train_loss =  0.67041016   train_acc:  0.95259\n",
      "Iteration  216 : test_loss =  0.69353265   test_acc:  0.95136\n",
      "************************************************************\n",
      "Iteration  217 : train_loss =  0.70406   train_acc:  0.951695\n",
      "Iteration  217 : test_loss =  0.7290587   test_acc:  0.95041\n",
      "************************************************************\n",
      "Iteration  218 : train_loss =  0.6701818   train_acc:  0.95260835\n",
      "Iteration  218 : test_loss =  0.69336146   test_acc:  0.95139\n",
      "************************************************************\n",
      "Iteration  219 : train_loss =  0.7037568   train_acc:  0.9517033\n",
      "Iteration  219 : test_loss =  0.7288135   test_acc:  0.95043\n",
      "************************************************************\n",
      "Iteration  220 : train_loss =  0.669956   train_acc:  0.9526233\n",
      "Iteration  220 : test_loss =  0.6931926   test_acc:  0.95137\n",
      "************************************************************\n",
      "Iteration  221 : train_loss =  0.70345753   train_acc:  0.95172\n",
      "Iteration  221 : test_loss =  0.7285717   test_acc:  0.95043\n",
      "************************************************************\n",
      "Iteration  222 : train_loss =  0.66972953   train_acc:  0.95264834\n",
      "Iteration  222 : test_loss =  0.69302297   test_acc:  0.95137\n",
      "************************************************************\n",
      "Iteration  223 : train_loss =  0.7031616   train_acc:  0.95175165\n",
      "Iteration  223 : test_loss =  0.728333   test_acc:  0.95044\n",
      "************************************************************\n",
      "Iteration  224 : train_loss =  0.66950727   train_acc:  0.95266336\n",
      "Iteration  224 : test_loss =  0.6928574   test_acc:  0.95139\n",
      "************************************************************\n",
      "Iteration  225 : train_loss =  0.70287025   train_acc:  0.95176333\n",
      "Iteration  225 : test_loss =  0.7280989   test_acc:  0.95047\n",
      "************************************************************\n",
      "Iteration  226 : train_loss =  0.6692863   train_acc:  0.95268166\n",
      "Iteration  226 : test_loss =  0.6926927   test_acc:  0.95142\n",
      "************************************************************\n",
      "Iteration  227 : train_loss =  0.70258754   train_acc:  0.95179\n",
      "Iteration  227 : test_loss =  0.7278727   test_acc:  0.95047\n",
      "************************************************************\n",
      "Iteration  228 : train_loss =  0.66906995   train_acc:  0.95269\n",
      "Iteration  228 : test_loss =  0.692532   test_acc:  0.95143\n",
      "************************************************************\n",
      "Iteration  229 : train_loss =  0.70234007   train_acc:  0.9517983\n",
      "Iteration  229 : test_loss =  0.7276784   test_acc:  0.95049\n",
      "************************************************************\n",
      "Iteration  230 : train_loss =  0.6688566   train_acc:  0.95270336\n",
      "Iteration  230 : test_loss =  0.692372   test_acc:  0.95143\n",
      "************************************************************\n",
      "Iteration  231 : train_loss =  0.7021025   train_acc:  0.95180833\n",
      "Iteration  231 : test_loss =  0.72749275   test_acc:  0.9505\n",
      "************************************************************\n",
      "Iteration  232 : train_loss =  0.6686464   train_acc:  0.95271\n",
      "Iteration  232 : test_loss =  0.6922136   test_acc:  0.95146\n",
      "************************************************************\n",
      "Iteration  233 : train_loss =  0.70186704   train_acc:  0.95182335\n",
      "Iteration  233 : test_loss =  0.72730887   test_acc:  0.9505\n",
      "************************************************************\n",
      "Iteration  234 : train_loss =  0.668439   train_acc:  0.95272166\n",
      "Iteration  234 : test_loss =  0.6920577   test_acc:  0.95149\n",
      "************************************************************\n",
      "Iteration  235 : train_loss =  0.7016335   train_acc:  0.95183\n",
      "Iteration  235 : test_loss =  0.72712636   test_acc:  0.9505\n",
      "************************************************************\n",
      "Iteration  236 : train_loss =  0.6682339   train_acc:  0.9527467\n",
      "Iteration  236 : test_loss =  0.69190365   test_acc:  0.9515\n",
      "************************************************************\n",
      "Iteration  237 : train_loss =  0.7014016   train_acc:  0.9518433\n",
      "Iteration  237 : test_loss =  0.72694504   test_acc:  0.95051\n",
      "************************************************************\n",
      "Iteration  238 : train_loss =  0.66803086   train_acc:  0.952755\n",
      "Iteration  238 : test_loss =  0.6917513   test_acc:  0.95152\n",
      "************************************************************\n",
      "Iteration  239 : train_loss =  0.7011713   train_acc:  0.95184666\n",
      "Iteration  239 : test_loss =  0.72676474   test_acc:  0.95053\n",
      "************************************************************\n",
      "Iteration  240 : train_loss =  0.6678306   train_acc:  0.9527817\n",
      "Iteration  240 : test_loss =  0.6916014   test_acc:  0.95156\n",
      "************************************************************\n",
      "Iteration  241 : train_loss =  0.70094335   train_acc:  0.95186335\n",
      "Iteration  241 : test_loss =  0.7265866   test_acc:  0.95054\n",
      "************************************************************\n",
      "Iteration  242 : train_loss =  0.6676333   train_acc:  0.952785\n",
      "Iteration  242 : test_loss =  0.69145393   test_acc:  0.95156\n",
      "************************************************************\n",
      "Iteration  243 : train_loss =  0.7007177   train_acc:  0.95188\n",
      "Iteration  243 : test_loss =  0.72641045   test_acc:  0.95056\n",
      "************************************************************\n",
      "Iteration  244 : train_loss =  0.6674391   train_acc:  0.9527983\n",
      "Iteration  244 : test_loss =  0.6913087   test_acc:  0.95155\n",
      "************************************************************\n",
      "Iteration  245 : train_loss =  0.700495   train_acc:  0.9519\n",
      "Iteration  245 : test_loss =  0.72623616   test_acc:  0.95055\n",
      "************************************************************\n",
      "Iteration  246 : train_loss =  0.66724753   train_acc:  0.9528217\n",
      "Iteration  246 : test_loss =  0.6911655   test_acc:  0.95156\n",
      "************************************************************\n",
      "Iteration  247 : train_loss =  0.7002746   train_acc:  0.951915\n",
      "Iteration  247 : test_loss =  0.7260634   test_acc:  0.95054\n",
      "************************************************************\n",
      "Iteration  248 : train_loss =  0.6670579   train_acc:  0.95283836\n",
      "Iteration  248 : test_loss =  0.6910241   test_acc:  0.95158\n",
      "************************************************************\n",
      "Iteration  249 : train_loss =  0.7000559   train_acc:  0.95192\n",
      "Iteration  249 : test_loss =  0.72589225   test_acc:  0.95055\n",
      "************************************************************\n",
      "Iteration  250 : train_loss =  0.6668711   train_acc:  0.952855\n",
      "Iteration  250 : test_loss =  0.6908852   test_acc:  0.95159\n",
      "************************************************************\n",
      "Iteration  251 : train_loss =  0.6998394   train_acc:  0.95194\n",
      "Iteration  251 : test_loss =  0.7257229   test_acc:  0.95055\n",
      "************************************************************\n",
      "Iteration  252 : train_loss =  0.66668814   train_acc:  0.95286334\n",
      "Iteration  252 : test_loss =  0.6907501   test_acc:  0.95162\n",
      "************************************************************\n",
      "Iteration  253 : train_loss =  0.699625   train_acc:  0.95194\n",
      "Iteration  253 : test_loss =  0.72555584   test_acc:  0.95058\n",
      "************************************************************\n",
      "Iteration  254 : train_loss =  0.6665087   train_acc:  0.9528767\n",
      "Iteration  254 : test_loss =  0.69061863   test_acc:  0.95166\n",
      "************************************************************\n",
      "Iteration  255 : train_loss =  0.6994127   train_acc:  0.9519567\n",
      "Iteration  255 : test_loss =  0.7253908   test_acc:  0.95058\n",
      "************************************************************\n",
      "Iteration  256 : train_loss =  0.6663313   train_acc:  0.9528817\n",
      "Iteration  256 : test_loss =  0.6904889   test_acc:  0.95166\n",
      "************************************************************\n",
      "Iteration  257 : train_loss =  0.69920236   train_acc:  0.95197666\n",
      "Iteration  257 : test_loss =  0.7252275   test_acc:  0.95061\n",
      "************************************************************\n",
      "Iteration  258 : train_loss =  0.66615623   train_acc:  0.95289165\n",
      "Iteration  258 : test_loss =  0.69036096   test_acc:  0.95165\n",
      "************************************************************\n",
      "Iteration  259 : train_loss =  0.6989941   train_acc:  0.951995\n",
      "Iteration  259 : test_loss =  0.72506565   test_acc:  0.95061\n",
      "************************************************************\n",
      "Iteration  260 : train_loss =  0.66598254   train_acc:  0.9528817\n",
      "Iteration  260 : test_loss =  0.6902342   test_acc:  0.95167\n",
      "************************************************************\n",
      "Iteration  261 : train_loss =  0.6987877   train_acc:  0.952005\n",
      "Iteration  261 : test_loss =  0.72490555   test_acc:  0.95061\n",
      "************************************************************\n",
      "Iteration  262 : train_loss =  0.6658105   train_acc:  0.95289\n",
      "Iteration  262 : test_loss =  0.6901087   test_acc:  0.95171\n",
      "************************************************************\n",
      "Iteration  263 : train_loss =  0.6985833   train_acc:  0.95202166\n",
      "Iteration  263 : test_loss =  0.72474676   test_acc:  0.95062\n",
      "************************************************************\n",
      "Iteration  264 : train_loss =  0.6656401   train_acc:  0.9528983\n",
      "Iteration  264 : test_loss =  0.6899844   test_acc:  0.95171\n",
      "************************************************************\n",
      "Iteration  265 : train_loss =  0.6983806   train_acc:  0.95202667\n",
      "Iteration  265 : test_loss =  0.72458977   test_acc:  0.95062\n",
      "************************************************************\n",
      "Iteration  266 : train_loss =  0.6654711   train_acc:  0.9529167\n",
      "Iteration  266 : test_loss =  0.68986136   test_acc:  0.95173\n",
      "************************************************************\n",
      "Iteration  267 : train_loss =  0.69817984   train_acc:  0.95203835\n",
      "Iteration  267 : test_loss =  0.72443396   test_acc:  0.95061\n",
      "************************************************************\n",
      "Iteration  268 : train_loss =  0.66530365   train_acc:  0.952925\n",
      "Iteration  268 : test_loss =  0.68973947   test_acc:  0.95177\n",
      "************************************************************\n",
      "Iteration  269 : train_loss =  0.6979807   train_acc:  0.95205\n",
      "Iteration  269 : test_loss =  0.7242796   test_acc:  0.95063\n",
      "************************************************************\n",
      "Iteration  270 : train_loss =  0.6651374   train_acc:  0.95294166\n",
      "Iteration  270 : test_loss =  0.6896185   test_acc:  0.95178\n",
      "************************************************************\n",
      "Iteration  271 : train_loss =  0.69778323   train_acc:  0.952065\n",
      "Iteration  271 : test_loss =  0.7241266   test_acc:  0.95063\n",
      "************************************************************\n",
      "Iteration  272 : train_loss =  0.66497266   train_acc:  0.95294666\n",
      "Iteration  272 : test_loss =  0.68949854   test_acc:  0.95182\n",
      "************************************************************\n",
      "Iteration  273 : train_loss =  0.69758725   train_acc:  0.95207334\n",
      "Iteration  273 : test_loss =  0.7239749   test_acc:  0.95064\n",
      "************************************************************\n",
      "Iteration  274 : train_loss =  0.6648091   train_acc:  0.9529567\n",
      "Iteration  274 : test_loss =  0.68937975   test_acc:  0.95181\n",
      "************************************************************\n",
      "Iteration  275 : train_loss =  0.6973928   train_acc:  0.95208836\n",
      "Iteration  275 : test_loss =  0.7238243   test_acc:  0.95064\n",
      "************************************************************\n",
      "Iteration  276 : train_loss =  0.66464674   train_acc:  0.95295835\n",
      "Iteration  276 : test_loss =  0.6892617   test_acc:  0.95181\n",
      "************************************************************\n",
      "Iteration  277 : train_loss =  0.69719994   train_acc:  0.9521\n",
      "Iteration  277 : test_loss =  0.7236751   test_acc:  0.95067\n",
      "************************************************************\n",
      "Iteration  278 : train_loss =  0.6644856   train_acc:  0.952965\n",
      "Iteration  278 : test_loss =  0.6891448   test_acc:  0.95182\n",
      "************************************************************\n",
      "Iteration  279 : train_loss =  0.6970086   train_acc:  0.95211\n",
      "Iteration  279 : test_loss =  0.72352725   test_acc:  0.9507\n",
      "************************************************************\n",
      "Iteration  280 : train_loss =  0.6643256   train_acc:  0.9529733\n",
      "Iteration  280 : test_loss =  0.6890284   test_acc:  0.95185\n",
      "************************************************************\n",
      "Iteration  281 : train_loss =  0.6968186   train_acc:  0.95212\n",
      "Iteration  281 : test_loss =  0.7233801   test_acc:  0.95071\n",
      "************************************************************\n",
      "Iteration  282 : train_loss =  0.6641667   train_acc:  0.952975\n",
      "Iteration  282 : test_loss =  0.68891287   test_acc:  0.95186\n",
      "************************************************************\n",
      "Iteration  283 : train_loss =  0.69663036   train_acc:  0.95213\n",
      "Iteration  283 : test_loss =  0.7232345   test_acc:  0.95071\n",
      "************************************************************\n",
      "Iteration  284 : train_loss =  0.66400886   train_acc:  0.95298165\n",
      "Iteration  284 : test_loss =  0.6887981   test_acc:  0.95186\n",
      "************************************************************\n",
      "Iteration  285 : train_loss =  0.6964435   train_acc:  0.9521433\n",
      "Iteration  285 : test_loss =  0.7230901   test_acc:  0.95072\n",
      "************************************************************\n",
      "Iteration  286 : train_loss =  0.6638522   train_acc:  0.953\n",
      "Iteration  286 : test_loss =  0.68868417   test_acc:  0.95187\n",
      "************************************************************\n",
      "Iteration  287 : train_loss =  0.69625807   train_acc:  0.95214665\n",
      "Iteration  287 : test_loss =  0.7229467   test_acc:  0.95073\n",
      "************************************************************\n",
      "Iteration  288 : train_loss =  0.6636966   train_acc:  0.95301664\n",
      "Iteration  288 : test_loss =  0.6885711   test_acc:  0.95188\n",
      "************************************************************\n",
      "Iteration  289 : train_loss =  0.6960741   train_acc:  0.9521483\n",
      "Iteration  289 : test_loss =  0.72280425   test_acc:  0.95074\n",
      "************************************************************\n",
      "Iteration  290 : train_loss =  0.663542   train_acc:  0.95303667\n",
      "Iteration  290 : test_loss =  0.6884584   test_acc:  0.95186\n",
      "************************************************************\n",
      "Iteration  291 : train_loss =  0.6958914   train_acc:  0.95216167\n",
      "Iteration  291 : test_loss =  0.7226633   test_acc:  0.95073\n",
      "************************************************************\n",
      "Iteration  292 : train_loss =  0.66338843   train_acc:  0.953045\n",
      "Iteration  292 : test_loss =  0.6883465   test_acc:  0.95187\n",
      "************************************************************\n",
      "Iteration  293 : train_loss =  0.69570994   train_acc:  0.95217335\n",
      "Iteration  293 : test_loss =  0.7225231   test_acc:  0.95073\n",
      "************************************************************\n",
      "Iteration  294 : train_loss =  0.66323596   train_acc:  0.9530467\n",
      "Iteration  294 : test_loss =  0.68823546   test_acc:  0.95189\n",
      "************************************************************\n",
      "Iteration  295 : train_loss =  0.6955297   train_acc:  0.95218164\n",
      "Iteration  295 : test_loss =  0.72238374   test_acc:  0.95074\n",
      "************************************************************\n",
      "Iteration  296 : train_loss =  0.6630843   train_acc:  0.95305\n",
      "Iteration  296 : test_loss =  0.6881252   test_acc:  0.95192\n",
      "************************************************************\n",
      "Iteration  297 : train_loss =  0.6953506   train_acc:  0.9521883\n",
      "Iteration  297 : test_loss =  0.7222455   test_acc:  0.95075\n",
      "************************************************************\n",
      "Iteration  298 : train_loss =  0.6629336   train_acc:  0.9530517\n",
      "Iteration  298 : test_loss =  0.6880157   test_acc:  0.95193\n",
      "************************************************************\n",
      "Iteration  299 : train_loss =  0.6951726   train_acc:  0.95219165\n",
      "Iteration  299 : test_loss =  0.7221081   test_acc:  0.95074\n",
      "************************************************************\n",
      "Iteration  300 : train_loss =  0.6627842   train_acc:  0.9530567\n",
      "Iteration  300 : test_loss =  0.68790686   test_acc:  0.95195\n",
      "************************************************************\n",
      "Iteration  301 : train_loss =  0.69499594   train_acc:  0.952205\n",
      "Iteration  301 : test_loss =  0.7219717   test_acc:  0.95076\n",
      "************************************************************\n",
      "Iteration  302 : train_loss =  0.6626353   train_acc:  0.95305836\n",
      "Iteration  302 : test_loss =  0.68779844   test_acc:  0.95196\n",
      "************************************************************\n",
      "Iteration  303 : train_loss =  0.6948202   train_acc:  0.95219666\n",
      "Iteration  303 : test_loss =  0.72183585   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  304 : train_loss =  0.66248804   train_acc:  0.95305836\n",
      "Iteration  304 : test_loss =  0.68769133   test_acc:  0.95197\n",
      "************************************************************\n",
      "Iteration  305 : train_loss =  0.6946456   train_acc:  0.95220333\n",
      "Iteration  305 : test_loss =  0.721701   test_acc:  0.95079\n",
      "************************************************************\n",
      "Iteration  306 : train_loss =  0.6623408   train_acc:  0.95305836\n",
      "Iteration  306 : test_loss =  0.68758416   test_acc:  0.95198\n",
      "************************************************************\n",
      "Iteration  307 : train_loss =  0.694472   train_acc:  0.95220834\n",
      "Iteration  307 : test_loss =  0.7215669   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  308 : train_loss =  0.66219574   train_acc:  0.9530633\n",
      "Iteration  308 : test_loss =  0.6874789   test_acc:  0.95199\n",
      "************************************************************\n",
      "Iteration  309 : train_loss =  0.69429946   train_acc:  0.952215\n",
      "Iteration  309 : test_loss =  0.721434   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  310 : train_loss =  0.66205   train_acc:  0.95306164\n",
      "Iteration  310 : test_loss =  0.6873725   test_acc:  0.952\n",
      "************************************************************\n",
      "Iteration  311 : train_loss =  0.69412786   train_acc:  0.95221835\n",
      "Iteration  311 : test_loss =  0.72130126   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  312 : train_loss =  0.6619083   train_acc:  0.9530733\n",
      "Iteration  312 : test_loss =  0.6872699   test_acc:  0.95203\n",
      "************************************************************\n",
      "Iteration  313 : train_loss =  0.693958   train_acc:  0.95222336\n",
      "Iteration  313 : test_loss =  0.7211706   test_acc:  0.95079\n",
      "************************************************************\n",
      "Iteration  314 : train_loss =  0.661763   train_acc:  0.9530967\n",
      "Iteration  314 : test_loss =  0.6871638   test_acc:  0.95201\n",
      "************************************************************\n",
      "Iteration  315 : train_loss =  0.6937883   train_acc:  0.95223\n",
      "Iteration  315 : test_loss =  0.72103894   test_acc:  0.9508\n",
      "************************************************************\n",
      "Iteration  316 : train_loss =  0.6616258   train_acc:  0.9531133\n",
      "Iteration  316 : test_loss =  0.6870648   test_acc:  0.95202\n",
      "************************************************************\n",
      "Iteration  317 : train_loss =  0.693624   train_acc:  0.95225\n",
      "Iteration  317 : test_loss =  0.7209125   test_acc:  0.95082\n",
      "************************************************************\n",
      "Iteration  318 : train_loss =  0.66149193   train_acc:  0.95312\n",
      "Iteration  318 : test_loss =  0.68696845   test_acc:  0.952\n",
      "************************************************************\n",
      "Iteration  319 : train_loss =  0.69346195   train_acc:  0.9522567\n",
      "Iteration  319 : test_loss =  0.7207876   test_acc:  0.95079\n",
      "************************************************************\n",
      "Iteration  320 : train_loss =  0.6613584   train_acc:  0.95314336\n",
      "Iteration  320 : test_loss =  0.6868719   test_acc:  0.95201\n",
      "************************************************************\n",
      "Iteration  321 : train_loss =  0.6933015   train_acc:  0.95226336\n",
      "Iteration  321 : test_loss =  0.7206636   test_acc:  0.95079\n",
      "************************************************************\n",
      "Iteration  322 : train_loss =  0.6612258   train_acc:  0.95315665\n",
      "Iteration  322 : test_loss =  0.68677586   test_acc:  0.95199\n",
      "************************************************************\n",
      "Iteration  323 : train_loss =  0.69314206   train_acc:  0.9522667\n",
      "Iteration  323 : test_loss =  0.7205404   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  324 : train_loss =  0.661094   train_acc:  0.95315665\n",
      "Iteration  324 : test_loss =  0.6866802   test_acc:  0.95198\n",
      "************************************************************\n",
      "Iteration  325 : train_loss =  0.69298357   train_acc:  0.95228\n",
      "Iteration  325 : test_loss =  0.720418   test_acc:  0.95078\n",
      "************************************************************\n",
      "Iteration  326 : train_loss =  0.66096294   train_acc:  0.9531633\n",
      "Iteration  326 : test_loss =  0.68658495   test_acc:  0.95198\n",
      "************************************************************\n",
      "Iteration  327 : train_loss =  0.69282603   train_acc:  0.952295\n",
      "Iteration  327 : test_loss =  0.72029626   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  328 : train_loss =  0.6608325   train_acc:  0.953175\n",
      "Iteration  328 : test_loss =  0.68649036   test_acc:  0.95202\n",
      "************************************************************\n",
      "Iteration  329 : train_loss =  0.6926694   train_acc:  0.95231\n",
      "Iteration  329 : test_loss =  0.7201752   test_acc:  0.95076\n",
      "************************************************************\n",
      "Iteration  330 : train_loss =  0.660703   train_acc:  0.9531817\n",
      "Iteration  330 : test_loss =  0.6863963   test_acc:  0.95205\n",
      "************************************************************\n",
      "Iteration  331 : train_loss =  0.6925135   train_acc:  0.95232165\n",
      "Iteration  331 : test_loss =  0.7200548   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  332 : train_loss =  0.660574   train_acc:  0.95318836\n",
      "Iteration  332 : test_loss =  0.68630266   test_acc:  0.95206\n",
      "************************************************************\n",
      "Iteration  333 : train_loss =  0.6923587   train_acc:  0.95234\n",
      "Iteration  333 : test_loss =  0.71993494   test_acc:  0.95075\n",
      "************************************************************\n",
      "Iteration  334 : train_loss =  0.66044587   train_acc:  0.9531983\n",
      "Iteration  334 : test_loss =  0.6862099   test_acc:  0.9521\n",
      "************************************************************\n",
      "Iteration  335 : train_loss =  0.69220436   train_acc:  0.95234334\n",
      "Iteration  335 : test_loss =  0.7198158   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  336 : train_loss =  0.6603185   train_acc:  0.9532033\n",
      "Iteration  336 : test_loss =  0.6861174   test_acc:  0.95209\n",
      "************************************************************\n",
      "Iteration  337 : train_loss =  0.6920512   train_acc:  0.9523567\n",
      "Iteration  337 : test_loss =  0.7196975   test_acc:  0.95078\n",
      "************************************************************\n",
      "Iteration  338 : train_loss =  0.6601918   train_acc:  0.95321\n",
      "Iteration  338 : test_loss =  0.6860257   test_acc:  0.95209\n",
      "************************************************************\n",
      "Iteration  339 : train_loss =  0.6918986   train_acc:  0.952365\n",
      "Iteration  339 : test_loss =  0.7195796   test_acc:  0.95078\n",
      "************************************************************\n",
      "Iteration  340 : train_loss =  0.660066   train_acc:  0.953225\n",
      "Iteration  340 : test_loss =  0.68593436   test_acc:  0.95209\n",
      "************************************************************\n",
      "Iteration  341 : train_loss =  0.691747   train_acc:  0.95237166\n",
      "Iteration  341 : test_loss =  0.71946234   test_acc:  0.95081\n",
      "************************************************************\n",
      "Iteration  342 : train_loss =  0.6599407   train_acc:  0.95322\n",
      "Iteration  342 : test_loss =  0.68584377   test_acc:  0.95209\n",
      "************************************************************\n",
      "Iteration  343 : train_loss =  0.6915961   train_acc:  0.9523817\n",
      "Iteration  343 : test_loss =  0.7193458   test_acc:  0.95082\n",
      "************************************************************\n",
      "Iteration  344 : train_loss =  0.65981627   train_acc:  0.9532217\n",
      "Iteration  344 : test_loss =  0.6857536   test_acc:  0.95213\n",
      "************************************************************\n",
      "Iteration  345 : train_loss =  0.69144595   train_acc:  0.95239836\n",
      "Iteration  345 : test_loss =  0.7192302   test_acc:  0.95084\n",
      "************************************************************\n",
      "Iteration  346 : train_loss =  0.6596927   train_acc:  0.95322835\n",
      "Iteration  346 : test_loss =  0.6856642   test_acc:  0.95212\n",
      "************************************************************\n",
      "Iteration  347 : train_loss =  0.6912969   train_acc:  0.95240164\n",
      "Iteration  347 : test_loss =  0.71911496   test_acc:  0.95084\n",
      "************************************************************\n",
      "Iteration  348 : train_loss =  0.65956986   train_acc:  0.95323664\n",
      "Iteration  348 : test_loss =  0.68557525   test_acc:  0.95213\n",
      "************************************************************\n",
      "Iteration  349 : train_loss =  0.6911484   train_acc:  0.95241\n",
      "Iteration  349 : test_loss =  0.7190004   test_acc:  0.95085\n",
      "************************************************************\n",
      "Iteration  350 : train_loss =  0.6594478   train_acc:  0.9532533\n",
      "Iteration  350 : test_loss =  0.6854872   test_acc:  0.95214\n",
      "************************************************************\n",
      "Iteration  351 : train_loss =  0.69100076   train_acc:  0.95242\n",
      "Iteration  351 : test_loss =  0.71888673   test_acc:  0.95086\n",
      "************************************************************\n",
      "Iteration  352 : train_loss =  0.6593267   train_acc:  0.953255\n",
      "Iteration  352 : test_loss =  0.6853997   test_acc:  0.95211\n",
      "************************************************************\n",
      "Iteration  353 : train_loss =  0.690854   train_acc:  0.952425\n",
      "Iteration  353 : test_loss =  0.7187736   test_acc:  0.95086\n",
      "************************************************************\n",
      "Iteration  354 : train_loss =  0.6592065   train_acc:  0.95326\n",
      "Iteration  354 : test_loss =  0.68531317   test_acc:  0.95211\n",
      "************************************************************\n",
      "Iteration  355 : train_loss =  0.6907082   train_acc:  0.95242333\n",
      "Iteration  355 : test_loss =  0.7186611   test_acc:  0.95086\n",
      "************************************************************\n",
      "Iteration  356 : train_loss =  0.6590873   train_acc:  0.9532717\n",
      "Iteration  356 : test_loss =  0.6852275   test_acc:  0.95213\n",
      "************************************************************\n",
      "Iteration  357 : train_loss =  0.6905631   train_acc:  0.95242834\n",
      "Iteration  357 : test_loss =  0.7185495   test_acc:  0.95085\n",
      "************************************************************\n",
      "Iteration  358 : train_loss =  0.65896946   train_acc:  0.95326334\n",
      "Iteration  358 : test_loss =  0.685143   test_acc:  0.95215\n",
      "************************************************************\n",
      "Iteration  359 : train_loss =  0.690419   train_acc:  0.95244\n",
      "Iteration  359 : test_loss =  0.7184388   test_acc:  0.95088\n",
      "************************************************************\n",
      "Iteration  360 : train_loss =  0.65885365   train_acc:  0.95327\n",
      "Iteration  360 : test_loss =  0.68506   test_acc:  0.95218\n",
      "************************************************************\n",
      "Iteration  361 : train_loss =  0.6902761   train_acc:  0.95244664\n",
      "Iteration  361 : test_loss =  0.7183289   test_acc:  0.95088\n",
      "************************************************************\n",
      "Iteration  362 : train_loss =  0.65874034   train_acc:  0.9532767\n",
      "Iteration  362 : test_loss =  0.6849797   test_acc:  0.95219\n",
      "************************************************************\n",
      "Iteration  363 : train_loss =  0.69013435   train_acc:  0.9524633\n",
      "Iteration  363 : test_loss =  0.71822023   test_acc:  0.95089\n",
      "************************************************************\n",
      "Iteration  364 : train_loss =  0.65863055   train_acc:  0.9532767\n",
      "Iteration  364 : test_loss =  0.6849027   test_acc:  0.95219\n",
      "************************************************************\n",
      "Iteration  365 : train_loss =  0.68999445   train_acc:  0.95246834\n",
      "Iteration  365 : test_loss =  0.7181133   test_acc:  0.9509\n",
      "************************************************************\n",
      "Iteration  366 : train_loss =  0.6585223   train_acc:  0.9532767\n",
      "Iteration  366 : test_loss =  0.68482745   test_acc:  0.95219\n",
      "************************************************************\n",
      "Iteration  367 : train_loss =  0.68985635   train_acc:  0.95247835\n",
      "Iteration  367 : test_loss =  0.7180084   test_acc:  0.9509\n",
      "************************************************************\n",
      "Iteration  368 : train_loss =  0.6584159   train_acc:  0.95328\n",
      "Iteration  368 : test_loss =  0.684754   test_acc:  0.95219\n",
      "************************************************************\n",
      "Iteration  369 : train_loss =  0.68971956   train_acc:  0.95249164\n",
      "Iteration  369 : test_loss =  0.7179051   test_acc:  0.9509\n",
      "************************************************************\n",
      "Iteration  370 : train_loss =  0.6583104   train_acc:  0.95328665\n",
      "Iteration  370 : test_loss =  0.6846817   test_acc:  0.95221\n",
      "************************************************************\n",
      "Iteration  371 : train_loss =  0.6895838   train_acc:  0.95249665\n",
      "Iteration  371 : test_loss =  0.7178025   test_acc:  0.9509\n",
      "************************************************************\n",
      "Iteration  372 : train_loss =  0.65820587   train_acc:  0.95328\n",
      "Iteration  372 : test_loss =  0.68460995   test_acc:  0.9522\n",
      "************************************************************\n",
      "Iteration  373 : train_loss =  0.68944937   train_acc:  0.95250666\n",
      "Iteration  373 : test_loss =  0.7177009   test_acc:  0.95089\n",
      "************************************************************\n",
      "Iteration  374 : train_loss =  0.65810204   train_acc:  0.95328164\n",
      "Iteration  374 : test_loss =  0.68453896   test_acc:  0.95221\n",
      "************************************************************\n",
      "Iteration  375 : train_loss =  0.68931574   train_acc:  0.9525167\n",
      "Iteration  375 : test_loss =  0.71760046   test_acc:  0.95092\n",
      "************************************************************\n",
      "Iteration  376 : train_loss =  0.6579991   train_acc:  0.95329\n",
      "Iteration  376 : test_loss =  0.6844689   test_acc:  0.95221\n",
      "************************************************************\n",
      "Iteration  377 : train_loss =  0.6891835   train_acc:  0.9525267\n",
      "Iteration  377 : test_loss =  0.7175007   test_acc:  0.95093\n",
      "************************************************************\n",
      "Iteration  378 : train_loss =  0.65789694   train_acc:  0.95330167\n",
      "Iteration  378 : test_loss =  0.6843991   test_acc:  0.95223\n",
      "************************************************************\n",
      "Iteration  379 : train_loss =  0.6890522   train_acc:  0.95252836\n",
      "Iteration  379 : test_loss =  0.7174022   test_acc:  0.95091\n",
      "************************************************************\n",
      "Iteration  380 : train_loss =  0.65779597   train_acc:  0.95330834\n",
      "Iteration  380 : test_loss =  0.68433034   test_acc:  0.95226\n",
      "************************************************************\n",
      "Iteration  381 : train_loss =  0.6889224   train_acc:  0.95253\n",
      "Iteration  381 : test_loss =  0.7173047   test_acc:  0.95092\n",
      "************************************************************\n",
      "Iteration  382 : train_loss =  0.65769607   train_acc:  0.95331\n",
      "Iteration  382 : test_loss =  0.6842623   test_acc:  0.95227\n",
      "************************************************************\n",
      "Iteration  383 : train_loss =  0.6887936   train_acc:  0.95253664\n",
      "Iteration  383 : test_loss =  0.71720785   test_acc:  0.95093\n",
      "************************************************************\n",
      "Iteration  384 : train_loss =  0.65759706   train_acc:  0.953315\n",
      "Iteration  384 : test_loss =  0.6841948   test_acc:  0.95226\n",
      "************************************************************\n",
      "Iteration  385 : train_loss =  0.6886663   train_acc:  0.9525433\n",
      "Iteration  385 : test_loss =  0.7171119   test_acc:  0.95094\n",
      "************************************************************\n",
      "Iteration  386 : train_loss =  0.657499   train_acc:  0.95331335\n",
      "Iteration  386 : test_loss =  0.68412787   test_acc:  0.95226\n",
      "************************************************************\n",
      "Iteration  387 : train_loss =  0.68854   train_acc:  0.95255333\n",
      "Iteration  387 : test_loss =  0.7170167   test_acc:  0.95097\n",
      "************************************************************\n",
      "Iteration  388 : train_loss =  0.6574017   train_acc:  0.953325\n",
      "Iteration  388 : test_loss =  0.68406165   test_acc:  0.95226\n",
      "************************************************************\n",
      "Iteration  389 : train_loss =  0.6884146   train_acc:  0.95255166\n",
      "Iteration  389 : test_loss =  0.71692204   test_acc:  0.95096\n",
      "************************************************************\n",
      "Iteration  390 : train_loss =  0.6573051   train_acc:  0.95332\n",
      "Iteration  390 : test_loss =  0.68399566   test_acc:  0.95225\n",
      "************************************************************\n",
      "Iteration  391 : train_loss =  0.6882898   train_acc:  0.95255\n",
      "Iteration  391 : test_loss =  0.7168279   test_acc:  0.95097\n",
      "************************************************************\n",
      "Iteration  392 : train_loss =  0.6572091   train_acc:  0.95333\n",
      "Iteration  392 : test_loss =  0.68393   test_acc:  0.95228\n",
      "************************************************************\n",
      "Iteration  393 : train_loss =  0.6881661   train_acc:  0.95257\n",
      "Iteration  393 : test_loss =  0.7167343   test_acc:  0.95097\n",
      "************************************************************\n",
      "Iteration  394 : train_loss =  0.65711385   train_acc:  0.953335\n",
      "Iteration  394 : test_loss =  0.6838648   test_acc:  0.95228\n",
      "************************************************************\n",
      "Iteration  395 : train_loss =  0.6880429   train_acc:  0.95258164\n",
      "Iteration  395 : test_loss =  0.7166413   test_acc:  0.95096\n",
      "************************************************************\n",
      "Iteration  396 : train_loss =  0.657019   train_acc:  0.95334\n",
      "Iteration  396 : test_loss =  0.6837999   test_acc:  0.95228\n",
      "************************************************************\n",
      "Iteration  397 : train_loss =  0.6879204   train_acc:  0.95258\n",
      "Iteration  397 : test_loss =  0.7165488   test_acc:  0.95096\n",
      "************************************************************\n",
      "Iteration  398 : train_loss =  0.6569246   train_acc:  0.9533517\n",
      "Iteration  398 : test_loss =  0.6837354   test_acc:  0.95228\n",
      "************************************************************\n",
      "Iteration  399 : train_loss =  0.6877985   train_acc:  0.9525883\n",
      "Iteration  399 : test_loss =  0.71645683   test_acc:  0.95097\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzde3gV9Z348fd3Zs4l9+QkJIEAclFRvCGCWESFEhDRIrUsbreP3VbdXarWUrf8xK5dtYpSlZV1H1mtUrbbbre1VkVr62rULq7UikW8YZFYQMItl5N7zm1mvr8/JjkhQgi5kdOTz+t5eJjM9TNzks/3MnO+o7TWGiGEEGnLGOoAhBBCDC5J9EIIkeYk0QshRJqTRC+EEGlOEr0QQqQ5SfRCCJHmrKEOoDv79+/v03ZFRUXU1tYOcDT9J3H1jsTVO6kaF6RubOkW16hRo7pdJjV6IYRIcz3W6Gtra3nkkUdoaGhAKUV5eTkLFy7sss6HH37I/fffT3FxMQAzZsxgyZIlAGzbto0NGzbgui5z585l8eLFg3AaQgghutNjojdNk2uuuYYJEyYQiURYuXIlZ599NqNHj+6y3umnn87KlSu7zHNdl/Xr13P77bdTWFjIbbfdxrRp047YVgghxODpMdEXFBRQUFAAQEZGBmVlZYTD4eNK1pWVlZSWllJSUgLAzJkz2bJliyR6IYYRrTWRSATXdVFKDXU4SYcOHSIWiw11GEc4VlxaawzDIBgM9upa9upmbHV1Nbt27eLkk08+YtnHH3/MihUrKCgo4JprrmHMmDGEw2EKCwuT6xQWFrJz586j7ruiooKKigoAVq9eTVFRUW9CS7Isq8/bDiaJq3ckrt5J1bgA6uvrCQaD+Hy+oQ7lCIFAYKhDOKpjxZVIJDAMo0tu7clxJ/poNMqaNWv42te+RmZmZpdl48ePZ926dQSDQbZu3coDDzzAww8/zNHGS+uuFCovL6e8vDz5c1/vhqfbnfTBJnH1jsTVe47jYJomtm0PdShdWJaVcjFBz3EppWhpaTkiv/b7qRvbtlmzZg0XXXQRM2bMOGJ5ZmYmwWAQgKlTp+I4Dk1NTRQWFlJXV5dcr66uLtkNJIQQom962wXWY6LXWvPoo49SVlbGFVdccdR1GhoakqVLZWUlruuSk5PDxIkTOXDgANXV1di2zebNm5k2bVqvAuyNn2/5lN/v2Ddo+xdCiL9EPXbd7Nixg02bNjF27FhWrFgBwJe//OVkM3H+/Pm8+eabvPTSS5imid/vZ/ny5SilME2Ta6+9llWrVuG6LnPmzGHMmDGDdjLPfFRPfM9uTlly8aAdQwjxl2XJkiXcdNNNzJ49Oznv8ccfZ/fu3axatarbbb73ve9xzjnnHNf8VNdjoj/ttNN48sknj7nOggULWLBgwVGXTZ06lalTp/Ytul4ytUvClfeoCCE6XXnllWzcuLFLot+4cSN33nnnkMV0oqXVN2Mt7WC7Qx2FECKVXH755VRUVCQfWdy7dy+HDh1ixowZrFy5kssuu4w5c+bw4IMP9mn/9fX1XHvttZSXl3PFFVewfft2AH7/+98zb9485s2bx/z582lpaeHQoUNcddVVzJs3j89//vP84Q9/GLDzPJaUHeumL0ztYkuNXoiU5f78cfTeXQO6TzVmPMZf/123y0OhEFOmTOF3v/sdl156KRs3bmTRokUopbj11lspKCjAcRyuvvpqtm/fzuTJk3t1/DVr1nDmmWfyox/9iP/7v//jW9/6Fi+//DKPPvoo9957L9OnT6e1tZVAIMBPf/pTLrnkEr71rW/hOA6RSKS/p39c0qpG79MutuR5IcRnLF68mI0bNwJet03HUCzPP/88l156KZdeeik7duzo9ns+x/LWW2/xpS99CYBZs2ZRX19PU1MT06dP56677mL9+vU0NjZiWRZTpkzhySefZM2aNXz00UdkZ2cP3EkeQ3rV6HGl60aIFHasmvdgWrBgAXfddRfvv/8+0WiUs846iz179vDYY4/xwgsvkJ+fz/Lly4lGo73ed3ffF7rpppuYO3cur776Kl/4whf4xS9+wQUXXMCvfvUrXnnlFb71rW+xbNky/uqv/mogTvGY0qpGbyE1eiHEkbKysvjc5z7HLbfckqzNt7S0kJGRQW5uLjU1Nbz22mt92vcFF1zA008/DcDmzZsJhULk5OSwe/duTj/9dG688UbOOeccKisrqaqqoqioiK985Sv89V//Ne+///6AneOxpFeNXmtJ9EKIo1q8eDHXX389//7v/w7AGWecwZlnnsmcOXMYO3Ys06dPP679fPWrX8WyvNR53nnn8YMf/IBbbrmF8vJygsEga9euBeCJJ55g8+bNGIbBqaeeypw5c9i4cSOPPvoolmWRlZXFv/7rvw7OyX6G0kdrd6SAvrx45Dvr/5ccC+7420sGIaL+SdWvqEtcvSNx9V7HEAip5i91CASAtra2I4aiGTYvHrGUxtapMzqeEEKkgvRK9EjXjRBCfFZaJXoTjY3U6IUQ4nBplegthSR6IYT4jPRK9Gjs9DolIYTot7TKipaSrhshhPistHqO3lJg67Qqu4QQ/RQOh7n66qsBqKmpwTRNQqEQSil+/etf4/f7u9323Xff5amnnuLuu+8+7uPNmDGD3/72t4RCoX7HPlDSKtGbChwliV4I0SkUCvHyyy8D3gBkWVlZLFu2LPm8um3byS9AfdY555zzFzf2/NGkVaK3DLBdSfRCiGNbvnw5oVCI9957j7POOotFixZxxx13EI1GCQaD/Mu//Asnn3wymzdv5tFHH+U///M/WbNmDfv27ePTTz9l3759XH/99Vx33XXHdbyqqipuueUWwuEwoVCIhx56iLKyMp5//nkeeughDMMgNzeXp59+mj/96U9861vfIh6Po7Xmhz/8IRMmTOjX+aZXoldgS41eiJT1xNuH2FXf+4HDjmV8QZDrp5X0ertPPvmEX/ziF5imSXNzM08//TSWZbFp0yZ+8IMf8Pjjjx+xTWVlJb/85S9pbW3loosu4qtf/So+n6/HY/3TP/0TS5YsYenSpfz85z/ne9/7Hj/60Y9Yu3Yt//Vf/8XIkSNpbGwE4Mc//jHXXXcdV111FfF4HMdxen1un5VmiV5hq9T7qrUQIvUsWrQoOTRDU1MTy5cvZ9euXSilSCQSR91m7ty5BAIBAoEARUVF1NTUHHPogQ5//OMfeeKJJwD40pe+xD333APAtGnT+Pa3v80XvvAFLrvssuS8tWvXcuDAAS677LJ+1+bhOBJ9bW0tjzzyCA0NDSilKC8vZ+HChV3Wef3115NjPQeDQa6//nrGjRsHwI033kgwGMQwDEzTZPXq1f0OujumIX30QqSyvtS8B8vhY8U88MADzJw5k/Xr17N3716WLFly1G0CgUBy2jTNPte2lfKeDvzBD37A1q1beeWVV5g/fz4vvfQSX/rSlzjnnHN45ZVX+MpXvsIDDzzArFmz+nScDj0metM0ueaaa5gwYQKRSISVK1dy9tlnM3r06OQ6xcXF3HnnnWRnZ/POO+/wwx/+kHvvvTe5/I477iA3N7dfgR4PSylsw0JrnbyQQgjRk+bmZkpLSwF6fEd2X0ybNo2NGzeyZMkSnn76ac4//3wAdu/enXyv9ssvv8z+/ftpa2vjpJNO4rrrrmPPnj189NFHg5/oCwoKKCgoACAjI4OysjLC4XCXRD9p0qTk9CmnnEJdXV2/guory1Rgg+04+Lq5iy6EEJ/1jW98g+XLl/PDH/6QCy+8sN/7Ky8vT1Y2v/CFL3D33Xdzyy238OijjyZvxgLcc8897Nq1C601s2bN4owzzmDdunU89dRTWJZFcXEx3/72t/sdT6+GKa6uruaOO+5gzZo1RwyR2eG5555j//79LFu2DPC6bjpelzVv3jzKy8uPul1FRQUVFRUArF69mng83qsTAdjwHxt5orGQl6+fSmbW0eMbKn/JQ6IOBYmrd1I1LvDyxrGeVRe9F4vFKCnp2g12rGt83NXeaDTKmjVr+NrXvtZtkv/ggw947bXX+P73v5+cd/fddxMKhWhsbOSee+5h1KhRR335bnl5eZdCoC9jazvtv+iHDlaTk3di3sV4vFJ1vHCJq3ckrt7TWqdkIZSqhePxxBWLxY74vPs9Hr1t26xZs4aLLrqIGTNmHHWdjvcvrlixgpycnOT8jm+H5eXlMX36dCorK4/nkH1iGao93qPfMRdCiOGox0SvtebRRx+lrKyMK6644qjr1NbW8uCDD3LTTTd1KVWi0SiRSCQ5/d577zF27NgBCv1IlumdjpNIvVJaCCGGSo9dNzt27GDTpk2MHTuWFStWAPDlL3852WyYP38+Tz31FC0tLcnnRDseo2xsbOTBBx8EvNeJzZo1iylTpgzWuXTW6BP9/4KBEEKkix4T/Wmnndbj40bLli1L3nw9XElJCQ888EDfo+sls71GL103QgjRKa2+XZTsurHdIY5ECCFSR3olequjRi999EIITzgcZt68ecybN48pU6Zw3nnnMW/ePD7/+c/3+Bj3u+++y/e+971eH/ODDz6grKyM3/3ud32MemCl1beKrPZxK2xb+uiFEJ6hGKb42Wef5fzzz+fZZ59l9uzZ/Ql/QKRZou+o0UvXjRCie4M5TLHWmhdeeIH//u//5qqrrkruE2DdunX86le/QinF5z//eb773e+ya9cuVq5cSV1dHaZpsn79+i4jDwyEtEz0jtTohUhJH2xto6lhYP8+c/NNzpza+2/CD9YwxVu2bGHMmDGMGzeOz33uc7z66qssXLiQV199lRdffJFf//rXZGRkUF9fD8A3v/lNbrzxRi677DKi0SiGMfA96umV6NubX/YAjN8shEhvgzVM8bPPPsuVV14JwJVXXslTTz3FwoULef3117n66qvJyMgAvHHEWlpaksMRgzf672B8YzfNEr1XEiYk0QuRkvpS8x4sgzFMseM4/OY3v+Gll17i4YcfRmtNfX09LS0tRx1VtxdDjfVLej1142uv0csXpoQQvTBQwxS//vrrTJ48mbfffps//OEPvPXWWyxcuJAXX3yRSy65hJ///OfJ0QLq6+vJyclh5MiRvPjii4A3hk1bW1v/T+gz0irR+/1eX1lc+uiFEL3wjW98g/vuu48rr7yyX6/ue/bZZ1mwYEGXeZdffjnPPvssc+bMYf78+Vx22WXMmzePRx99FICHH36Y9evXU15ezpVXXkl1dXW/zuVoejVM8Ym0f//+Xm9T/el+/u71Jm4sqmf+pZ8bhKj6LlVHF5S4ekfi6j3HcZJ94ankL3n0yra2tiNGEe736JV/KXwBbzzmuDxeKYQQSWmV6P0Br+smIV03QgiRlFaJ3td+RzzhpGRvlBBCDIm0SvSW33vqJi6JXgghktIq0Rumhd9JYLvSRy+EEB3SKtED+LRNXLrohRAiKQ0TvUMiNZ8YFUIMgSVLlhwxXPDjjz/Orbfeesxt3n333aMuq6ur46STTuInP/nJQIY5qNIz0UvPjRCi3ZVXXsnGjRu7zNu4cSNf/OIX+7S/559/nqlTpx6xz1TW41g3tbW1PPLIIzQ0NKCUory8nIULF3ZZR2vNhg0beOeddwgEAtxwww1MmDABgG3btrFhwwZc12Xu3LksXrx4cM6knV87JFzV84pCiGHh8ssv5/777ycWixEIBNi7dy+HDh1ixowZrFixgnfffZdoNMrll1/Od77znR73t3HjRv75n/+Zm266iQMHDjBy5EgAfvnLX/LYY48BcPrpp/Nv//Zv1NTUsHLlSvbs2QPAfffdx/Tp0wfvZLvRY6I3TZNrrrmGCRMmEIlEWLlyJWeffXaX8ZLfeecdDh48yMMPP8zOnTt54oknuPfee3Fdl/Xr13P77bdTWFjIbbfdxrRp0wZ8rOXD+XGJS8+NEClp06ZN1NTUDOg+R4wYwcUXX9zt8lAoxJQpU/jd737HpZdeysaNG1m0aBFKKW699VYKCgpwHIerr76a7du3M3ny5G73tW/fPqqrqzn33HO54ooreO655/iHf/gHduzYwcMPP8zGjRsJhULJIYi/973vccEFF7B+/Xocx6G1tXVAz/149dh1U1BQkKydZ2RkUFZWRjgc7rLO22+/zcUXX4xSilNPPZXW1lbq6+uprKyktLSUkpISLMti5syZbNmyZXDOpJ0PTVxLjV4I0Wnx4sXJrpaNGzcmexaef/55Lr30Ui699FJ27NjBzp07j7mf5557ji984QtA1y6hN954g8svv5xQKAR4ebNj/le/+lXAqzTn5uYO/Mkdh14NU1xdXc2uXbs4+eSTu8wPh8MUFRUlfy4sLCQcDhMOhyksLOwyv6cL2V8+5WIjiV6IVHSsmvdgWrBgAXfddRfvv/8+0WiUs846iz179vDYY4/xwgsvkJ+fz/Lly4lGo8fcz7PPPkttbS3PPPMMAIcOHeLPf/7zUYcgTiXHneij0Shr1qzha1/72hGD6RxtXDSlVLfzj6aiooKKigoAVq9e3aXg6I2A0rRqs8/bDxbLslIuJpC4ekvi6r3q6upu38l6ouTl5XHhhRfyj//4j1x11VVYlkVLSwuZmZmEQiFqa2t57bXXmDVrFpZloZTCNM0ucVdWVhKJRLo8jXP//ffz61//moULF/L1r3+db3zjG8mum4KCAi666CJ++tOf8g//8A84jkNbWxs5OTk9xtvT9ep48cnxOq6rb9s2a9as4aKLLmLGjBlHLC8sLOwycl5dXR0FBQXYtk1dXd0R84+mvLyc8vLy5M99HYmvo+sm1UbyS9XRBSWu3pG4ek9rnRKjRC5atIjrr7+edevWYds2Z5xxBmeccQYXXXQRY8eOZfr06TiOg23baK2T0x1+9atfsWDBgi7zFixYwA033MDNN9/MN7/5TRYvXoxhGJx55pmsXbuWu+66i//3//4fP/vZzzAMg/vuu49p06YdM87jGb0yFosd8Xkfa/TKHhO91ppHH32UsrIyrrjiiqOuM23aNF588UUuvPBCdu7cSWZmJgUFBeTm5nLgwAGqq6sJhUJs3ryZm2++uadD9ovPgISbdk+NCiH66bLLLmPfvn1d5q1du/ao6z711FNHzPvHf/zHI+ZNnjw5+Yz+0qVLWbp0aZflI0aMYMOGDX2MeOD0mOh37NjBpk2bGDt2LCtWrADgy1/+crI0mT9/Pueeey5bt27l5ptvxu/3c8MNNwDezYdrr72WVatW4bouc+bMYcyYMYN4OuA3IKFSb+xrIYQYKj0m+tNOO63HV2sppbj++uuPumzq1KlMnTq1b9H1gd+AuCR6IYRISrs+Dq9Gn3anJYQQSb19MWDaZUSfoUioob3DL4TolKqv7PtLZds2htG71J12GdFvGSQMK+WfaxViuAiFQlRVVRGLxVLqbzIQCBCLxYY6jCMcKy6tNYZhEAwGe7XP9Ev0poGrDGzHwTfEz+4KIbx7eBkZGUMdxhFS9ZHUwYgr7bpu/JZ3SolYYogjEUKI1JB2iT7QnuhTsUkmhBBDIe0SfYbfB0Cs7dhjVgghxHCRfok+4PXLRyNSoxdCCEjHRB8MABCLSqIXQghIy0TvByASlZuxQggB6ZjoM7znS+VmrBBCeNIv0Wd6XTfRmHwTTwghIA0TfWaW91KUaEISvRBCQDon+rgzxJEIIURqSLtEn9GR6G13iCMRQojUkHaJ3pcRxHQdYpLohRACSMNEr5Qi6MaJ2r0br1kIIdJV2iV6gICbIOpIohdCCEjTRB/UNjE3dca9FkKIodTjgO3r1q1j69at5OXlsWbNmiOWP/fcc7z++usAuK5LVVUV69evJzs7mxtvvJFgMIhhGJimyerVqwf+DI4ioB2i0kUvhBDAcST62bNns2DBAh555JGjLl+0aBGLFi0C4O233+aFF14gOzs7ufyOO+4gNzd3gMI9PkHlEE3PxooQQvRaj9lw8uTJXRL3sbzxxhtceOGF/Q6qv4K4RLUkeiGEgAF8lWAsFmPbtm1cd911XeavWrUKgHnz5lFeXt7t9hUVFVRUVACwevVqioqK+hSHZVlkWopax+rzPgaDZaVWPB0krt6RuHovVWMbTnENWKL/4x//yKRJk7rU/u+++25CoRCNjY3cc889jBo1ismTJx91+/Ly8i4FQV/fmVhUVIQPhyhmSr0Pcji9n3IgSFy9k6pxQerGlm5xjRo1qttlA9a/8cYbbzBr1qwu80KhEAB5eXlMnz6dysrKgTrcMQUNiCl5MbgQQsAAJfq2tja2b9/OtGnTkvOi0SiRSCQ5/d577zF27NiBOFyPAqYiavhPyLGEECLV9VjtXbt2Ldu3b6e5uZlly5axdOlSbNsbGXL+/PkAvPXWW5xzzjkEg8Hkdo2NjTz44IMAOI7DrFmzmDJlymCcwxEyLEXc9WHbDpZlnpBjCiFEquox0S9fvrzHncyePZvZs2d3mVdSUsIDDzzQ58D6I2CZEId4JIqVkzUkMQghRKpIy2cQgz7vtKLtXUdCCDGcpWWiD/i87ppoW3SIIxFCiKGXlok+6PcBEI3Ke2OFECItE31GwEv0sWhiiCMRQoihl5aJPhCQGr0QQnRIy0QfzAgAEI3JC8KFECItE30g6CX6SFy6boQQIi0TfUZ7jT4Wlxq9EEKkZaIPZGYAEE04QxyJEEIMvbRM9MGORG/La6aEECItE73ls7Bcm5gtLwgXQoi0TPQAQTdB1JFEL4QQaZvoM904bVKjF0KI9E30WTpOi5u2pyeEEMctbTNhFg4tWsaiF0KItE302YZL28C9ElcIIf5ipW0mzDY1LcjrBIUQIm0TfZZP0aqDPa8ohBBprsdEv27dOrZu3UpeXh5r1qw5YvmHH37I/fffT3FxMQAzZsxgyZIlAGzbto0NGzbgui5z585l8eLFAxx+97J9BnHHRywSIZCRccKOK4QQqabHRD979mwWLFjAI4880u06p59+OitXruwyz3Vd1q9fz+23305hYSG33XYb06ZNY/To0f2P+jhkByyIQktjsyR6IcSw1uPN2MmTJ5Odnd3rHVdWVlJaWkpJSQmWZTFz5ky2bNnSpyD7IivD659vbWo7YccUQohUNCB99B9//DErVqygoKCAa665hjFjxhAOhyksLEyuU1hYyM6dO7vdR0VFBRUVFQCsXr2aoqKiPsViWRZFRUUUjyiAgw5g9HlfA6kjrlQjcfWOxNV7qRrbcIqr34l+/PjxrFu3jmAwyNatW3nggQd4+OGH0frIb6UqpbrdT3l5OeXl5cmfa2tr+xRPUVERtbW1GHjHP1Rd2+d9DaSOuFKNxNU7ElfvpWps6RbXqFGjul3W7+foMzMzCQa9p1umTp2K4zg0NTVRWFhIXV1dcr26ujoKCgr6e7jjlpXj9cs3t8nrBIUQw1u/E31DQ0Oy9l5ZWYnruuTk5DBx4kQOHDhAdXU1tm2zefNmpk2b1u+Aj1d2rndfoVVeEC6EGOZ67LpZu3Yt27dvp7m5mWXLlrF06VJs23tz0/z583nzzTd56aWXME0Tv9/P8uXLUUphmibXXnstq1atwnVd5syZw5gxYwb9hDpk5eUAh2iV98YKIYa5HhP98uXLj7l8wYIFLFiw4KjLpk6dytSpU/sWWT/5LIugE6clIS8fEUIMb2n7zViALCdKq4xULIQY5tJ2UDOAbJ2gxe3+SR8hhBgO0jrRZ2HTqtP6FIUQokdpnQWzDYcW7RvqMIQQYkild6I3Na2GJHohxPCW3jdjLUWLDFUshBjm0rpGn+U3iJoBEjH5dqwQYvhK60SfHfAaLG2NTUMciRBCDJ20TvT5uZkAhA/V9bCmEEKkr7RO9KUlIQAOVoeHOBIhhBg6aZ3oS8pKAThULy8fEUIMX2md6HOyAmQ6UQ62yMBmQojhK60TvVKKEreNQ7Y51KEIIcSQSetED1BqJTik5OXgQojhK+0TfcgH9VY22pXhioUQw1PaJ/q8gEmbFSTR3DzUoQghxJBI/0Sf6QegMdwwxJEIIcTQSP9En+2NddPYIDV6IcTwlPaJPj8vC4DGpsgQRyKEEEOjx9Er161bx9atW8nLy2PNmjVHLH/99dfZuHEjAMFgkOuvv55x48YBcOONNxIMBjEMA9M0Wb169cBGfxzy8nOAMI2tkuiFEMNTj4l+9uzZLFiwgEceeeSoy4uLi7nzzjvJzs7mnXfe4Yc//CH33ntvcvkdd9xBbm7uwEXcS3mhfCBMY1tiyGIQQoih1GOinzx5MtXV1d0unzRpUnL6lFNOoa4utQYQywz6sFybxpgz1KEIIcSQGNAXj7z66quce+65XeatWrUKgHnz5lFeXt7tthUVFVRUVACwevVqioqK+hSDZVlHbJvvRmm03T7vcyAcLa5UIHH1jsTVe6ka23CKa8AS/QcffMBrr73G97///eS8u+++m1AoRGNjI/fccw+jRo1i8uTJR92+vLy8S0FQW1vbpziKioqO2HasFefjtkxq9u9D+QN92m9/HS2uVCBx9Y7E1XupGlu6xTVq1Khulw3IUzd79uzhscceY8WKFeTk5CTnh0LeMMF5eXlMnz6dysrKgThcr51Zmk1VZjENH344JMcXQoih1O9EX1tby4MPPshNN93UpUSJRqNEIpHk9HvvvcfYsWP7e7g+OePUMgA+rJJx6YUQw0+PXTdr165l+/btNDc3s2zZMpYuXYpte8P+zp8/n6eeeoqWlhaeeOIJgORjlI2NjTz44IMAOI7DrFmzmDJlyiCeSvdOKs0HajjYKuPdCCGGnx4T/fLly4+5fNmyZSxbtuyI+SUlJTzwwAN9j2wAZfhMgk6cBkcSvRBi+BnQp25SWZ4bpcFRQx2GEEKccGk/BEKHfOI0ankBiRBi+Bk+id50aGBoHq0UQoihNGwSfZ4PGswMtNZDHYoQQpxQwybR5wdMmn2ZOK2tQx2KEEKcUMMn0Wf50cqg6cP3hzoUIYQ4oYZNoh916kQAXnvtj0MciRBCnFjDJtFPOSnEeb5mnhp5IW6bdN8IIYaPYZPolVKclQttVgatNak3kJEQQgyWYZPoAYryvdcK1tbUD3EkQghx4gyvRF+UB0BNuGmIIxFCiBNneCX6Ym/Y5NoPP43blQEAACAASURBVEI3Nw5xNEIIcWIMq0Qfyg5iaIeKjIk0//bZoQ5HCCFOiGGV6E1D4SqTT3JG83ws9V4hJoQQg2FYJXqAL55eAEBdfIgDEUKIE2TYJfqvTS1hol1PnTNsRmgWQgxzwy7RA4ywHGoJ4v7PM0MdihBCDLrhmegzFFVZJfz+d2+ho5GhDkcIIQZVj/0X69atY+vWreTl5bFmzZojlmut2bBhA++88w6BQIAbbriBCRMmALBt2zY2bNiA67rMnTuXxYsXD/wZ9MGIgIII3H/m3/Jk9SECY8cNdUhCCDFoeqzRz549m+9+97vdLn/nnXc4ePAgDz/8MH//93+ffEm467qsX7+e7373uzz00EO88cYbVFVVDVzk/ZA7cWJy+uDOT9B2YgijEUKIwdVjop88eTLZ2dndLn/77be5+OKLUUpx6qmn0traSn19PZWVlZSWllJSUoJlWcycOZMtW7YMaPB9NfPkEVw0KgjAwZf+B/3zx4c4IiGEGDz9fvQkHA5TVNT5THphYSHhcJhwOExhYWGX+Tt37ux2PxUVFVRUVACwevXqLvvsDcuyjmvbFQvzeP2JLdx79rXcUPU/fKWPxxvouE40iat3JK7eS9XYhlNc/U70R3s1n1Kq2/ndKS8vp7y8PPlzbW3fRpgsKio6rm0Pj+9nIy/i0j4e73gdb1wnmsTVOxJX76VqbOkW16hRo7pd1u+nbgoLC7sEVVdXR0FBAYWFhdTV1R0xP1UcXugUt9TgPPx9tOsMYURCCDE4+p3op02bxqZNm9Ba8/HHH5OZmUlBQQETJ07kwIEDVFdXY9s2mzdvZtq0aQMR84C5ZeZITDQf553Er8MBqKsZ6pCEEGLAKX20PpbDrF27lu3bt9Pc3ExeXh5Lly7Ftm0A5s+fj9aa9evX8+677+L3+7nhhhuY2P5Uy9atW/nxj3+M67rMmTOHq6666rgD279/f59OqLfNnl/8+k1+1pgPwPrYaxQuuBw1amyfjj2QcQ2Wz37chYVF1NbUohRoDa4GQ3n/a40339VoDYahcFxve0OB43jLVfu00V5tcF1vHoDjuMnWk+tqDEN5x+mYdl1cF0xL4TjetGUZ5OTkUR+ux7QUtu2CBtMysBMOylCYhiKRcDFNAxTYCRvTNEFBIu5gWSagsW1vWmtwHAfTNHFdF9dxsXxm+zE1lmXi2A4aME0Dx3HRWmNZBomEi2EoDEORkZFFS0szRvvxreTxnWQsiYSDzzLRaGzbxWeZuK7Gdhx8Puuw41vYtoPWGp/PJJHwWpSWaWLbNkopDNMkYScwlIFpGiTiNqZlYCiDeMLGsryLHgxmEI1GQEPCdvD5TLSrsR0Xn887vmM7+PwWju3iao3PMknY3jF9lkk84R3TMg0StoNSKnlMwzIwDIUddzAtEwUkbBvL8np/7fZprTWO4+CzvPN0HJeCUAENjQ1oR+PrOGc0PssiYdsoFKZlkognMAwDwzSwEzaGaWAYRvKzVYYiEU8kj5mwbXwd04kEPr8f13FxXAe/z4ftOLiui8/nw7Ht9s/Tas9fivz8POobGjAMA9MwiCcSmKaJaRjE4gksy0QpRSJx2HHsBD7L1/7Z2vgsH652vXP2+ZLH91k+HMfG1Rx2nl7/e8K2MRSYpkUikcAwDUzDJB6PUzQixJlnjx3wrpseE/1QOVGJ/qU//plH/uQNfHPL9v9ierSKzDUbkss7fnGjkTiJhE0sZhOPJ3BsTTyeIB5P4LqaeDyObWu0q4knEmjXS3IJ25s2TYNoNOIlHNf7RcPVONpFuy5KGbjaxXUcXNdBaxdQaEC7Dq72kh0oQKPxtnO194eqUN46gNYOKAOtXbR2UCjAQKNBe39kqr0xp7WLxvH2oAzQuvPn9m20dtC47du46PalGrc9HkAflt21S0djUeN0Hgu3fTuAjnV0+z/V/j+fmRZi+DCUjzvuvL1Lt/fxOlaiH3YDvmitSSRcKj/eT3NTK/H6Ji5prcbUDu+PmcJ2dxLGv/8Y17Vx3HgyyQ0chVJm8idDGaj2pIxSGMrEMEwv6XoRo5SJoYz2ROolQKUMTMvAZ/igvdatDJ+X0g0DrTWGYWKYZnuN3KuVKsNsT/warTV+vx+tdfs/r8AxDK823FG4mKaFYRi4roNhGIBCaxfTNJPbdp02cNtr/h01aG/aSNbwO6YNw7sejutgmV4NynEcMjIyiMfjuK53HCA5rbVur4Uffpyu007ymCaO4xV2hmngth/fm3ZQhoGhVHssRnvrpL2lgErGpfGOmZ2VRSQS8WKxvHPT2quFe9PttUanvaZqmtiOnaw12rbTfhzV3sLwPlfH9lobSuHV/C0LV+tky0O7Gsd18VnWYa0N79wAcnJzaG5qBqWwLBO7vXZutk97tWMTu73Wqgzl1cJNCxQ4to1p+QCv5t+ldu73aqqudrEsC8dx0NprBdjtLQLLZ3qtrcOPaXg1cp/PRywew1BGeyvAaxEkW1vgtTZ83jl3aRF0nLProt2OFoGNprOmbCjvOifsRPt1NoknElimiWEYJGyvFaBQyVaAxvvdT8QTaDpbO257C8/n887T1e3HTHz2PPFaIe3naZoGiYSNaZgYpuG1PHze77Od+EzLx2eB9qY7rq3jOvh8fuKx+DEfWumrYZPoP91dwx/efIe68H7idtc3TPmUBcpCqwAJw08oHsUqLMEXyMAyLXx+Pz6fD9M08ft8WD4L0zTw+3z4fCaGqQgE/JiW94EHAhaGobB8BoGAH/BaGk1N3nEH44Psq1TpUvosiat3UjUuSN3YUjWuwZD2ib6utoUXf/sadfV7AJegP4+RJadSWlpKqDCX0tIQefnZfOkXlcltzgl/zD/Zu/F/bh5q4mkDEkcgEEipBC+EGD7SNtG7rstLL77Fx5VvAYoxZZO56OJpFI3IO+r6l52Sz293NgDwbuhUPnj3fzn9998l87GnT2DUQggx8NIm0TuOy9tvfURba5zmlkYOHNxNLN5Efu5oPve58zll0uhjbr/s/FJOM1t56E/euDffP+fvOKt+J3f93SKMf/kJKufoBYQQQqS6tEn0hqF46+3/RWvviZaAP4+p58zhggsntz9q17PR48rgT7uTP79fcAq/KZvJZY8/iHnJAtR5Fw5O8EIIMYjSJtErpbhq8dVMPGUMkUhrn/rDTy4MsnbhOJb/Zndy3hOnLKbo/f/gzMfXktXUgLpkAco4voJDCCFSQdokeoCyMYVkZ2cSjbb1eR/jC4IETUXU6XyOe/VZX6OsrZp/+9mDEK6FabNQJ008xl6EECJ1DMs3TPXk366YwN+e1XVcnn2ZxSy9+F6qX3sF955v4/7P0+iEjGMvhEh9kuiPojjbR/mkIszP9P7YhsU3z1/Bz8fNQz/1H7j3fQf31V8PTZBCCHGcJNF3Izdg8vTfnIb/M1cobvp4ctw8rv3c7eyrbSHx8ydw/m4Retsf0DUHhyZYIYQ4hrTqox8MDy0cz5tVLfxkW9eRLRsCuXxzxgpGttWw/KP/5uRHVqEANe9K1PwvQm4+ypByVAgx9CTR92B0XoAv5vg52Bzn5U8aj1h+IHMEt553Mxcdeofzaz/kwpc3ol/eCBmZqKv/DjVlBiqr+1cxCiHEYJNEfxxMQ3HTBSM5qyST/3qvlkMtR96Efb3kXF4vOZcnWw9ybeVzlEbqKPmPf/WGICsdTfQrf48uGy9fvBJCnHCS6HvhkvF5zDopl++/tpdtB4/+COferFLuOufvAbi86v9YsO/3hGqqYc0/eytYPtTSa1GnnQ05eajs3BMVvhBimJJE30umobhr7ljeO9jKCx/X8+belm7XfWH0LF4YPQuAv/nzi8yseY+CeBMZP3usc+T1mXNh8hRUIAhnngeGIX37QogBJYm+j84uzeLs0ixe3FnP/+xs4M/1sWOu/7MJC/jZhAUALNldwbS6j8iPN1O8+RXY/ErnazZOOhlVPBI1ax4UFEFWNio3f3BPRgiR1iTR99OCUwqYPT6PPQ0xbq/4lLjT85uRnhpXzlPjygG4oOZ9zqv7iCw7yvS67ag9n2DsqURveb1zg1Mmo8afCkqhzr8E/H7vqZ5MuckrhOiZJPoBELQMJhVl8Mu/nsSB5jgbPwonhzzuyZsjzuLNEWclf57UuJtRkVrK9/+BULyJgngz/p3b0Tu3A6D/55nOjcdORJWOhsxM1NSZ3ktbc/KgpMwrFKQLSAjBcSb6bdu2sWHDBlzXZe7cuSxevLjL8ueee47XX/dqoK7rUlVVxfr168nOzubGG28kGAy2v27LZPXq1QN/FilkZI6fZeeXcuXpIWpaE/zHO9V8Ej52t87hduSNY0feOF4rnZacd/GhrUxo3oehXWbWvI+hHfITrfDpJ+hPPwFA/+63R+5swiTUGVPBdaCoBDV2IsSiUDgCsnNBBmcTYljoMdG7rsv69eu5/fbbKSws5LbbbmPatGmMHt05vvuiRYtYtGgRAG+//TYvvPAC2dmd3Qp33HEHubnD6+mSkTl+Rub4+ZfLxvPncJSElcm//m8l+5rivd7XppKpbCqZCsCPTrkyOf+yfW8wItpAwIlzXt1HoBRF0QaMjh7/P+9A/3lHcv0jOpUys6krLsUtKUNHI6hTJnv3BerrUCefDsFMaG2CUWO999UaJiojs9fxCyGGVo+JvrKyktLSUkpKSgCYOXMmW7Zs6ZLoD/fGG29w4YUybvvhJoSCFBWFmJQ7gaqmGM1Rhyc/qGPrgdZ+7fe3ZZ3X+XG+mJyeEt5BQczr9jm7vhLbMCmO1lMUrUcBAbf9ewBtLdi7K2G39xpF/e5byX1owwDLB/GY1x0UzIC6ahh/qje/MYyadJZXerQ2wylnQCIGdgI17hR0awvKsqB0DDQ3QGYW5IegtQUysiAj09t3MNPrcmp/YbYQYuD1mOjD4TCFhYXJnwsLC9m5c+dR143FYmzbto3rrruuy/xVq1YBMG/ePMrLy4+6bUVFBRUVFQCsXr2aoqKi4zuDz7Asq8/bDqaOuDpCu2jyWJqiNjHb4al3D1DVEOF/K+uOrHX3wbbQpOT00yd9/ojlJZE6JjfuIqEsJjXtoTRSR4M/m1ObPiXLjmArk8J4E8TjmADNjd4/gE/+lNyPPrivc6d/fKNzPoBSaK29QsF1wTRRwQx0pBWVmY2RmYUTrsEsLMbIycP+9M+Y409FZWXj7N1F06Qz8fkDOFV78J12FiiFc7AK36SzwLFxwjX4TjkDHWlDtzZjTTgVt6kRHAdr9Dic+lqUz49ZXIoTrsXIzMLID+HU1WDk5mNkZeM21mPk5IE/gG5rxcjOAWWgXRvlDwIaZXb9E0n1369UlKqxDae4lNb6mLnl97//Pe+++y7Lli0DYNOmTVRWVnLttdcese7mzZvZtGkTK1euTM4Lh8OEQiEaGxu55557+PrXv87kyZN7DGz//v29PRcgdd/sfrxxfXCojdaEw59qIjy9PXwCIjuSoV1GttWQl2ilJpjP2fWVZNpR6v25nFP/MZbr0OzL5PTG3diGia1MxrQdImb48bkJcuwILgqfdk5s4EqB1qAMvOJGgWmCY3sFjmVBNOK1IiwLWpogN99bFq6BEaVgWnBoH5Sd5O3n4F4YP8nbR201gUlnEmtugqYG1EkT0a0tEIugysZBY9jbpmQk1FZDVjaERsDBKu//3HzYtwdKR3stmn17YMx4lM+HrtrT/o4DhT5Y5U07DrrmIGrsBIhF0U0NqFFjvRZULAbFI73WkobQxFMI7/oEfD7vOA1h7zyzsr3prGwIBKGpAbJyvPNvbfHmoyAehYxs0A7YtteCc1zQrveUl+t619i0QLu9evnOX/rf5InW17hGjRrV7bIea/SFhYXU1dUlf66rq6OgoOCo677xxhvMmjWry7xQKARAXl4e06dPp7Ky8rgS/XB1ZonXBz5jdA5/e24xTVGb2jabrQdaiTsub1W1sKuHZ/b7y1UG+7JK6KivvzLy/OSy/yuZkpw2tIPlOsRNPzmJVpTWRM0AIyO1RMwAAONa9tPozyY30UpZWw3VwQJKI3UUxho5mFHImNZD5CVa2J8xgvEt+8lwYhwKhhjXegBDuzT6sxnZVoOrDOKGj4J4E44yUWiCThyN6rwn0VFn0e1JCQ12+3Qi7v0DiBzWZdZY3zldfaBzeu+uzukd7ycnY3/43+S0rupcR3/0bs8Xthv6M/8DaKUABdr1utGU8hK/aYHCS8Y+v3eujktdIIBOxL0b7JbPK9ACga4J3bS8880LgWl4L9EpLPb2XXsIikd5UdQe8goj24b6Whg52ttfc5M33dzodbuVlHndeT6ft59D+71uvtx82P+p9wBAZja1B/biFI9CBQLoqt2o0ePAtLzp9hf46AN7USedDIk4uvaQ9+BApBXd2IAaM847ZjTqHT9c43X3FZV4n1lmttcteGCvd48pOwf27obSMq/A2rvLK7gtH1TtgrHeMVvqa9DFZd51qznoFdyRNmgIo8aMRzc3QrQNVToaXe/lQFVU4sUXCEB+IfrQflROLmTneecQGgEZGegDVajikd4xD+7zridAzQEYOabz2paWeQ9ItDR7y9ta0HMX9vl3qTs9JvqJEydy4MABqqurCYVCbN68mZtvvvmI9dra2ti+fTvf/OY3k/Oi0ShaazIyMohGo7z33nssWbJkYM8gzeUGLXKDFhNCQQD+5uwRNEZtXA2VdVF2NUSJ2ZrX9zQddQyeweQqk7jp1eyafVnJ+XuyRyanqzNCyektPexPaRdDaxzDxO8kMLVDxAqSk2jFch3qA7mUROpQaOoCeZzUcoCY6afFymRicxUtvgwcZXJSywHCgVwCToJRkRqqgyHy482MiNazP3MEIyN15MZbqMoq5qTWg2TaUaoyixnfsh+fa3Mgo4jxLfvQKOoCeYxtPYhtWLRaQUojYaKmH0cZFMSbiZk+lNZkOjFsZWJqB1O79Ptug9YkU39HbRq8lkWHROeNfR2NtC93Oud3zIPORAJey6NDXXXndPVhrej9n3ZOf/rnzunDbu53KSQ7uvFqD3XOqzmIBpz2/SULtH17OuNuf2oMQFd+1Dm9fVvn9LY3GQyfvUOmu5vuaCkeNt19QWx68x0bbbWnV7u9RamU99n4/d468ZjXygIv2fv9Xquq/PIBP9ceE71pmlx77bWsWrUK13WZM2cOY8aM4aWXXgJg/vz5ALz11lucc845BIPB5LaNjY08+OCDADiOw6xZs5gyZcqRBxG9khf0Prbpo7OZPtp7uumaKSNoS3hdJX+qiRCxXa8A2N1ES9xhX1Oc1oTb7T5TgVYGTnuGjJs+wAd0LUQOZXTeL6rMHZucfjvQ2Urcedj83lDaRaFxlYnl2hhaEzd9BO0YBi5tVga5cW/Ii1Yrg1C8kYSyiJoBiqNhmn1ZaKAkGqY6GCIn0cqIWAP7M0YwIhomP9HC3swSRrdVk2VH+DSrlPEt+/G7CT7NKmVicxWG1lRlFnNK814cZXAwo5CTm/YSM33U+3OZ0LKPZiuTNivISa0HqffnoFGMjNRSG8wn4MQpijVyKBgiN9FKbqKFAxlFFMYaybKjHMwIURytx+/aVAcLKImEUWjq/TmMiNbjKpNWK0go1kTCsIgbFnmJVuKGD1cpMu0oCcNCAX4njm2YGFpjtreijAG5y5RiDu/d1oe3Ho9WEB/WXWkfVijbh1XC4oc9eReLdp0/+ZRBeSihxz76oTJc++gHU11bAtvVxB3NH6paKAiaRJSfTTur8RmKynAMx9XkBU1a4i5R28VQ4Kbkb4g4nNIuWhldCitDOxhaYxsWPjeB0pq46SfoxFBaE7GCZNoRDO3S4ssiN96CQtPozyEUa0SjaPRnMSLaQMKwaLEyGBGtJ2IFiRs+RkTrqQ/kYmiHoqjXFZeTaKUg3szerBKKo2FyEm3syRrJmLZDBJ04u7NHMq5lP5br8mlWCRObq0ApqjJHcHJzFQllUZ0RYkJzFREzSL0/h/Et+2nyZxE1A5S1VlMXyMPApTQS5kBGIdl2hFCsiarMYopiDeQkWvk0aySjIjVk2DHvmK0HsFyHPVmlTGjxWh/7MouZ0LyPhGFRHSxgfMt+2qwAjb4cxrYepMmXRcz0MzJSS70/B4AR0QZqg3n4nQQF8WZqggVk2RFyEm1UBwvIS7SQYceoCRYQijViaYe6QB6FMe9hhiZfFoWxJmzDoM3MoCDeRNywiBs+EoYPd96VnPe1r3fpLj9ex+qjl0R/gvylxNUQsbEMRWPM4UBznLygyb6mOAea4+QGLD4JR2lLuGT6DHbURjANhc9Q7GmIkRc0cTTUR2yyfAaJ9kJFCgvxl0C1d7m5ysBof5DAVSam66AVaBSmdnGUgaldDO0SN/34nTiG1kStQJfWX1aiDYWmxZeVvIfV5M8mP94MWtMQyCUUa8DFoNmXRZYdQWdk8ZsbZxEe4EQvQyCILvIzvF+J7IBJWa4fgFMKM465jdaa1riLrTU+Q7G/OU7QMjCUYmddhKJMHwlXs6MmwqhcP61xh8pwlNG5fppiDnsaYpTl+qltsznUkqA020dtVFPfGqUo08f+5jiGUmT6DKpbE2T6DEylqI/aZPoMtIbWhEvQUrga4o7GMrzCxdXevUspZ0RPtOrseHJV51NFzmFPGNnK6PI/QNz0J6ejViA53err/HLh4d2PDe2tA4BwoHPAwiZ/NucVZ2AMQteNJHrRb0opsgOdfwyHFwwdhQXAuSOzOF4dLQ2tNY6GhKOxDEVr3EEDAUtR22rjMxUBy6CqMUZuwMQyFbvCMUqyfWi8G9Zj8vzEbM2u+ihj8gO0xh32NsYZm+enPupQ3ZKgLNdPTWuC5rhDcZZXuLgaCjIs9jTEyLAU2X6T3U02OT7IsAw+ro1QnO3HMmBn+3FcDZ82xBidGyDhag62xCnN9hF3NOGITSjDIu5oWuIOOQGTuK2JOy5Bn0HM9tKMZUDM1piGQgEJ1yu4tAZHg9GeB6QQSz+nFR//30hvSKIXKU0phaXAas9uHS0OgLH5nYVL6LD5o3M7a1WTijoLnY4b1/3RXRec1jrZPZVwve4qrSFquwQsA9vRtCa85B5JuLQlXPKDFs0xh6jtUpBhUR+xsV1NftCiujWBaUBuwKKqKUa2zyTTb7C7PkZhpoXfNPgkHGVUrh8FHIyZFFo2ttbsro8yLj9IxHbZ2xhjXH6A5rhXoI3NDxCO2DRGHUbl+KluTZBwXIqyfOxriuMzFPnthVuO3yTbb/BJfYwRmRYBy6CyzjumoeCTcJSxeQFcrflzvXecmO2ytynOuPYC9UBLgpNHZFPdFKEh6ngtt9YEMUdTnOXjUEscy1DkBU0ONCfI9htkWCYHWuLkBy18puJAc5yiTB+GgoMtCUZkWrhAXZtNYYaFozUNUYf89q7D5ph3nR1XE7VdMnwGCUdju5qAaRB3XO/3ylDEbBfLUCjV0RL0fs/sYxSuny1oYeAK29OKjt167itJ9EIMAKUUZvtfvWl0Nr0DVnsT3yLZ6sn0mXQ8O5RzWEuo4LDCqjjbl5w+vFU0vqDzqbbTRnQmhcMLoM+N6ewaSAVHKxxdrVF4SVRrjaEUttvRolFEEi6G4U23xV18psJQipa4Q0b7NW2OO2T7vYTeFHfID1rEbJfmmENhpkVr3CtQCzMtGqM2CVdTkGERbvOehhk/agR/2nsIv2mQG/DuReUETDJ9Bp82xCjK8uE3FbvqY8nPYE9DjDF5fhwX9jbGOCk/QNR2OdCc4KT8AM0xh7pIgtG5XoHalnAYme0VqK6GoiyLfY1xMnwGuUGTPQ0xCoIWWX6TT8JRTh8xOGNJSaIXQpxwHf3QloKOevHhBeThXYH+jM7+8AyfcdR1ki29gMmILK+QzOssE7sUokWZ3vKi3CC+ks6ukpE5nQXqmLzOVuHhhevhLcSOLzf22mHDhF142JPAF48bvIEfZcByIYRIc5LohRAizUmiF0KINCeJXggh0pwkeiGESHOS6IUQIs1JohdCiDQniV4IIdJcyo5eKYQQYmCkXY3+8PfVphKJq3ckrt5J1bggdWMbTnGlXaIXQgjRlSR6IYRIc+add95551AHMdAmTJgw1CEclcTVOxJX76RqXJC6sQ2XuORmrBBCpDnpuhFCiDQniV4IIdJc2rx4ZNu2bWzYsAHXdZk7dy6LFy8eslhuvPFGgsEghmFgmiarV6+mpaWFhx56iJqaGkaMGMG3v/1tsrP7/2q7nqxbt46tW7eSl5fHmjVrAI4ZyzPPPMOrr76KYRh8/etfZ8qUKScsrieffJJXXnmF3FzvBQxf/vKXmTp16gmNq7a2lkceeYSGhgaUUpSXl7Nw4cIhv2bdxTXU1ywej3PHHXdg2zaO43DBBRewdOnSIb9e3cU11Nerg+u6rFy5klAoxMqVKwf/euk04DiOvummm/TBgwd1IpHQ3/nOd/TevXuHLJ4bbrhBNzY2dpn3k5/8RD/zzDNaa62feeYZ/ZOf/OSExPLhhx/qTz75RN9yyy09xrJ37179ne98R8fjcX3o0CF90003acdxTlhcv/jFL/TGjRuPWPdExhUOh/Unn3yitda6ra1N33zzzXrv3r1Dfs26i2uor5nrujoSiWittU4kEvq2227TO3bsGPLr1V1cQ329Ojz//PN67dq1+r777tNaD/7fZFp03VRWVlJaWkpJSQmWZTFz5ky2bNky1GF1sWXLFi655BIALrnkkhMW3+TJk49oOXQXy5YtW5g5cyY+n4/i4mJKS0uprKw8YXF150TGVVBQkHziISMjg7KyMsLh8JBfs+7i6s6JikspRTDovWrPcRwcx0EpNeTXq7u4unMif8fq6urYunUrc+fO7XL8wbxeaZHow+EwhYWFyZ8LCwuP+UdwIqxatYpbb72Viv/f3vmDpNbGAfjpjLfhlNWS5FC5RA2BEkENETQ10BHi/wAAAx1JREFUiQRFg0M0FEREQVNLbSlJYThWW0tLU3AhHHKIiqiMgqI/iySZqElCot8QV74vPPfjwtVXDr9nUjnD48Pxd/Q9R87PnwAkk0nq6+uBrw9tKpVS5mbk8r2jxWKpeMeDgwPm5+fZ3Nzk/f1dqVcsFuPh4YH29vaqavZvL1DfLJ/Ps7CwwMTEBF1dXdjt9qroVcoL1Pfa2tpifHz8PweecvcyxRp9ocQVor87epeb5eVlLBYLyWSSlZUVmpublbn8CaU6VpKhoSHcbjcAu7u77OzsMDU1pcQrm83i8/nweDz8+GF8E+hKu333qoZmmqaxurpKJpPB6/Xy/PxsuK1qL9W9Tk9P0XWd1tZWIpHI/27/t7xM8Y2+oaGBeDxefB6Px4tHRxVYLBYAdF3H6XRyd3eHruskEgkAEolE8WSQCoxcvnd8e3srvpdKUFdXh6ZpaJrG4OAg9/f3SrxyuRw+n4/+/n56enqA6mhWyqtamgHU1tbS0dHB+fl5VfQq5aW61+3tLScnJ0xPT+P3+7m6umJ9fb3svUwx6Nva2ohGo8RiMXK5HOFwGIfDocQlm83y8fFRfHxxcYHNZsPhcBAKhQAIhUI4nU4lfoChi8PhIBwO8/n5SSwWIxqNFpcHKsGvHR3g+PiYlpaWinsVCgWCwSBWq5Xh4eHi66qbGXmpbpZKpchkMsDXlS6Xl5dYrVblvYy8VPcaGxsjGAwSCASYnZ2ls7OTmZmZsvcyzT9jz87O2N7eJp/PMzAwgMvlUuLx8vKC1+sFvk4C9fX14XK5SKfTrK2t8fr6SmNjI3NzcxW5vNLv93N9fU06nUbXdUZGRnA6nYYue3t7HB4eomkaHo+H7u7uinlFIhEeHx+pqamhqamJycnJ4i+zSnnd3NywtLSEzWYrLv+Njo5it9uVNjPyOjo6Utrs6emJQCBAPp+nUCjQ29uL2+3+7f6u0mtjY0P5PvaLSCTC/v4+i4uLZe9lmkEvCIIglMYUSzeCIAiCMTLoBUEQTI4MekEQBJMjg14QBMHkyKAXBEEwOTLoBUEQTI4MekEQBJPzD4KvVFEnXHR/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainModel(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
