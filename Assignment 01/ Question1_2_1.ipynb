{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Shape of training features  (784, 60000)\n",
      "Shape of test features  (784, 10000)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "tr_x = tr_x.T\n",
    "te_x = te_x.T\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_x = tf.cast(tr_x, tf.float32) \n",
    "te_x = tf.cast(te_x, tf.float32)\n",
    "tr_y = tf.cast(tr_y, tf.float32)\n",
    "te_y = tf.cast(te_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = tf.Variable(tf.random.normal(shape=[784,300], mean=0.0, stddev=0.05))\n",
    "w2 = tf.Variable(tf.random.normal(shape=[300,10], mean=0.0, stddev=0.05))\n",
    "b = tf.Variable([0.])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forwardPass(x, w1,w2, b):\n",
    "    w1 = tf.transpose(w1)\n",
    "    w2 = tf.transpose(w2)\n",
    "     \n",
    "    y_pred = tf.matmul(w1, x) + b \n",
    "    a1 = tf.maximum(y_pred, 1) \n",
    "    \n",
    "    y_pred = tf.matmul(w2, a1) + b \n",
    "    \n",
    "    return softmax(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(y_pred):\n",
    "    y_pred_exp = tf.math.exp(y_pred)\n",
    "    summation = tf.reduce_sum(y_pred_exp, 0, keepdims=True) \n",
    "    return y_pred_exp / summation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    \n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=0)\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(x, y, w1,w2, b):\n",
    "  \n",
    "  y_pred_softmax = forwardPass(x,w1,w2, b)\n",
    "  predictions_correct = tf.cast(tf.equal(tf.round(y_pred_softmax), y), tf.float32)\n",
    "  return tf.reduce_mean(predictions_correct),y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(num_Iterations = 50):\n",
    "   \n",
    "    trainingLosses= []\n",
    "    testLosses= []\n",
    "    trainingAccuracies = []\n",
    "    testAccuracies = []\n",
    "    \n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        y_pred = forwardPass(tr_x,w1,w2, b)\n",
    "        currentLoss = cross_entropy(tr_y, y_pred)\n",
    "      \n",
    "      gradients = tape.gradient(currentLoss, [w1,w2, b])\n",
    "        \n",
    "     \n",
    "      tr_accuracy, y_pred_softmax = calculate_accuracy(tr_x, tr_y, w1,w2, b)\n",
    "      te_accuracy, y_pred_softmax = calculate_accuracy(te_x, te_y, w1,w2, b)\n",
    "      te_currentLoss = cross_entropy(te_y, y_pred_softmax)\n",
    "    \n",
    "      # Appending and print the information for training instances\n",
    "      trainingAccuracies.append(tr_accuracy.numpy())\n",
    "      trainingLosses.append(currentLoss.numpy())\n",
    "      print (\"Iteration \", i, \": train_loss = \",currentLoss.numpy(), \"  train_acc: \", tr_accuracy.numpy())\n",
    "      \n",
    "      # Appending and print the information for validation instances\n",
    "      testLosses.append(te_currentLoss.numpy())\n",
    "      testAccuracies.append(te_accuracy.numpy())\n",
    "      print (\"Iteration \", i, \": test_loss = \",te_currentLoss.numpy(), \"  test_acc: \", te_accuracy.numpy())\n",
    "      print(\"*\"*60)\n",
    "    \n",
    "      #Calling Adam optimizer the for updating the weights and trainfing the data\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.001).apply_gradients(zip(gradients, [w1,w2,b])) \n",
    "        \n",
    "    \n",
    "    plt.style.use(\"ggplot\") \n",
    "    plt.figure()\n",
    "    plt.plot(testLosses, label=\"Val Loss\")\n",
    "    plt.plot(trainingLosses, label=\"Train Loss\")\n",
    "    plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
    "    plt.plot(testAccuracies, label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : train_loss =  2.6593006   train_acc:  0.9\n",
      "Iteration  0 : test_loss =  2.6596892   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  1 : train_loss =  2.3941011   train_acc:  0.9\n",
      "Iteration  1 : test_loss =  2.3949378   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  2 : train_loss =  2.2218187   train_acc:  0.9\n",
      "Iteration  2 : test_loss =  2.223196   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  3 : train_loss =  2.1110704   train_acc:  0.90001\n",
      "Iteration  3 : test_loss =  2.1137931   test_acc:  0.90003\n",
      "************************************************************\n",
      "Iteration  4 : train_loss =  2.0245855   train_acc:  0.9\n",
      "Iteration  4 : test_loss =  2.027584   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  5 : train_loss =  1.9129179   train_acc:  0.90098\n",
      "Iteration  5 : test_loss =  1.9171442   test_acc:  0.90079\n",
      "************************************************************\n",
      "Iteration  6 : train_loss =  1.8197373   train_acc:  0.9000033\n",
      "Iteration  6 : test_loss =  1.8248281   test_acc:  0.90001\n",
      "************************************************************\n",
      "Iteration  7 : train_loss =  1.7208339   train_acc:  0.90404665\n",
      "Iteration  7 : test_loss =  1.7258297   test_acc:  0.90373\n",
      "************************************************************\n",
      "Iteration  8 : train_loss =  1.6292727   train_acc:  0.9013467\n",
      "Iteration  8 : test_loss =  1.6346694   test_acc:  0.90131\n",
      "************************************************************\n",
      "Iteration  9 : train_loss =  1.5466467   train_acc:  0.91043836\n",
      "Iteration  9 : test_loss =  1.5517229   test_acc:  0.90997\n",
      "************************************************************\n",
      "Iteration  10 : train_loss =  1.481696   train_acc:  0.90559334\n",
      "Iteration  10 : test_loss =  1.4881274   test_acc:  0.90542\n",
      "************************************************************\n",
      "Iteration  11 : train_loss =  1.4227885   train_acc:  0.9142783\n",
      "Iteration  11 : test_loss =  1.4278421   test_acc:  0.91399\n",
      "************************************************************\n",
      "Iteration  12 : train_loss =  1.3768841   train_acc:  0.90895\n",
      "Iteration  12 : test_loss =  1.3839514   test_acc:  0.90856\n",
      "************************************************************\n",
      "Iteration  13 : train_loss =  1.326661   train_acc:  0.91868836\n",
      "Iteration  13 : test_loss =  1.3320248   test_acc:  0.91846\n",
      "************************************************************\n",
      "Iteration  14 : train_loss =  1.2862163   train_acc:  0.91227835\n",
      "Iteration  14 : test_loss =  1.2938057   test_acc:  0.91175\n",
      "************************************************************\n",
      "Iteration  15 : train_loss =  1.2486081   train_acc:  0.92315835\n",
      "Iteration  15 : test_loss =  1.2541428   test_acc:  0.92285\n",
      "************************************************************\n",
      "Iteration  16 : train_loss =  1.2139909   train_acc:  0.9170067\n",
      "Iteration  16 : test_loss =  1.221958   test_acc:  0.916\n",
      "************************************************************\n",
      "Iteration  17 : train_loss =  1.1853096   train_acc:  0.927055\n",
      "Iteration  17 : test_loss =  1.1908698   test_acc:  0.92673\n",
      "************************************************************\n",
      "Iteration  18 : train_loss =  1.158226   train_acc:  0.92224336\n",
      "Iteration  18 : test_loss =  1.1664169   test_acc:  0.92085\n",
      "************************************************************\n",
      "Iteration  19 : train_loss =  1.1446652   train_acc:  0.9292667\n",
      "Iteration  19 : test_loss =  1.1510305   test_acc:  0.92865\n",
      "************************************************************\n",
      "Iteration  20 : train_loss =  1.1475664   train_acc:  0.9246967\n",
      "Iteration  20 : test_loss =  1.1549679   test_acc:  0.92408\n",
      "************************************************************\n",
      "Iteration  21 : train_loss =  1.1200019   train_acc:  0.92978835\n",
      "Iteration  21 : test_loss =  1.1272722   test_acc:  0.92938\n",
      "************************************************************\n",
      "Iteration  22 : train_loss =  1.114739   train_acc:  0.92786\n",
      "Iteration  22 : test_loss =  1.1226765   test_acc:  0.92691\n",
      "************************************************************\n",
      "Iteration  23 : train_loss =  1.0971395   train_acc:  0.9311583\n",
      "Iteration  23 : test_loss =  1.1049603   test_acc:  0.93054\n",
      "************************************************************\n",
      "Iteration  24 : train_loss =  1.0865825   train_acc:  0.9300417\n",
      "Iteration  24 : test_loss =  1.0951471   test_acc:  0.92934\n",
      "************************************************************\n",
      "Iteration  25 : train_loss =  1.0760417   train_acc:  0.93216336\n",
      "Iteration  25 : test_loss =  1.0843422   test_acc:  0.93163\n",
      "************************************************************\n",
      "Iteration  26 : train_loss =  1.0613941   train_acc:  0.93195665\n",
      "Iteration  26 : test_loss =  1.070443   test_acc:  0.93105\n",
      "************************************************************\n",
      "Iteration  27 : train_loss =  1.0566384   train_acc:  0.932995\n",
      "Iteration  27 : test_loss =  1.0653226   test_acc:  0.93258\n",
      "************************************************************\n",
      "Iteration  28 : train_loss =  1.038517   train_acc:  0.93347836\n",
      "Iteration  28 : test_loss =  1.0479872   test_acc:  0.93269\n",
      "************************************************************\n",
      "Iteration  29 : train_loss =  1.0386906   train_acc:  0.93397\n",
      "Iteration  29 : test_loss =  1.04777   test_acc:  0.93351\n",
      "************************************************************\n",
      "Iteration  30 : train_loss =  1.0180119   train_acc:  0.93476\n",
      "Iteration  30 : test_loss =  1.0278809   test_acc:  0.93398\n",
      "************************************************************\n",
      "Iteration  31 : train_loss =  1.0221659   train_acc:  0.934695\n",
      "Iteration  31 : test_loss =  1.0316346   test_acc:  0.93424\n",
      "************************************************************\n",
      "Iteration  32 : train_loss =  0.99946123   train_acc:  0.93588\n",
      "Iteration  32 : test_loss =  1.0097363   test_acc:  0.9349\n",
      "************************************************************\n",
      "Iteration  33 : train_loss =  1.0068407   train_acc:  0.93533164\n",
      "Iteration  33 : test_loss =  1.0166883   test_acc:  0.93483\n",
      "************************************************************\n",
      "Iteration  34 : train_loss =  0.98256564   train_acc:  0.9368067\n",
      "Iteration  34 : test_loss =  0.99325734   test_acc:  0.93585\n",
      "************************************************************\n",
      "Iteration  35 : train_loss =  0.99277085   train_acc:  0.9359767\n",
      "Iteration  35 : test_loss =  1.0029839   test_acc:  0.93536\n",
      "************************************************************\n",
      "Iteration  36 : train_loss =  0.9671807   train_acc:  0.9378267\n",
      "Iteration  36 : test_loss =  0.9782289   test_acc:  0.93668\n",
      "************************************************************\n",
      "Iteration  37 : train_loss =  0.9797958   train_acc:  0.9365633\n",
      "Iteration  37 : test_loss =  0.99034417   test_acc:  0.93597\n",
      "************************************************************\n",
      "Iteration  38 : train_loss =  0.9532257   train_acc:  0.93872666\n",
      "Iteration  38 : test_loss =  0.96463525   test_acc:  0.93754\n",
      "************************************************************\n",
      "Iteration  39 : train_loss =  0.9676059   train_acc:  0.93716335\n",
      "Iteration  39 : test_loss =  0.9784486   test_acc:  0.9364\n",
      "************************************************************\n",
      "Iteration  40 : train_loss =  0.9402752   train_acc:  0.939465\n",
      "Iteration  40 : test_loss =  0.95202464   test_acc:  0.93849\n",
      "************************************************************\n",
      "Iteration  41 : train_loss =  0.9561638   train_acc:  0.93766165\n",
      "Iteration  41 : test_loss =  0.96730715   test_acc:  0.93689\n",
      "************************************************************\n",
      "Iteration  42 : train_loss =  0.92828894   train_acc:  0.940215\n",
      "Iteration  42 : test_loss =  0.94033515   test_acc:  0.93933\n",
      "************************************************************\n",
      "Iteration  43 : train_loss =  0.9454737   train_acc:  0.9381267\n",
      "Iteration  43 : test_loss =  0.95688   test_acc:  0.93729\n",
      "************************************************************\n",
      "Iteration  44 : train_loss =  0.9173726   train_acc:  0.9408517\n",
      "Iteration  44 : test_loss =  0.9296953   test_acc:  0.9401\n",
      "************************************************************\n",
      "Iteration  45 : train_loss =  0.93537337   train_acc:  0.9385717\n",
      "Iteration  45 : test_loss =  0.9470122   test_acc:  0.93776\n",
      "************************************************************\n",
      "Iteration  46 : train_loss =  0.9073   train_acc:  0.94144166\n",
      "Iteration  46 : test_loss =  0.91989005   test_acc:  0.9406\n",
      "************************************************************\n",
      "Iteration  47 : train_loss =  0.9256388   train_acc:  0.9390467\n",
      "Iteration  47 : test_loss =  0.9374904   test_acc:  0.93824\n",
      "************************************************************\n",
      "Iteration  48 : train_loss =  0.8979036   train_acc:  0.94195\n",
      "Iteration  48 : test_loss =  0.91077316   test_acc:  0.9412\n",
      "************************************************************\n",
      "Iteration  49 : train_loss =  0.91651314   train_acc:  0.93938166\n",
      "Iteration  49 : test_loss =  0.9285728   test_acc:  0.93876\n",
      "************************************************************\n",
      "Iteration  50 : train_loss =  0.8891062   train_acc:  0.94248664\n",
      "Iteration  50 : test_loss =  0.9022215   test_acc:  0.94165\n",
      "************************************************************\n",
      "Iteration  51 : train_loss =  0.9078818   train_acc:  0.9397283\n",
      "Iteration  51 : test_loss =  0.9201039   test_acc:  0.93921\n",
      "************************************************************\n",
      "Iteration  52 : train_loss =  0.88084024   train_acc:  0.94302166\n",
      "Iteration  52 : test_loss =  0.8941738   test_acc:  0.94217\n",
      "************************************************************\n",
      "Iteration  53 : train_loss =  0.89979935   train_acc:  0.94005835\n",
      "Iteration  53 : test_loss =  0.9121964   test_acc:  0.93946\n",
      "************************************************************\n",
      "Iteration  54 : train_loss =  0.87294453   train_acc:  0.943545\n",
      "Iteration  54 : test_loss =  0.88654006   test_acc:  0.94259\n",
      "************************************************************\n",
      "Iteration  55 : train_loss =  0.8921442   train_acc:  0.9404233\n",
      "Iteration  55 : test_loss =  0.9046868   test_acc:  0.9397\n",
      "************************************************************\n",
      "Iteration  56 : train_loss =  0.86538523   train_acc:  0.94404835\n",
      "Iteration  56 : test_loss =  0.8791883   test_acc:  0.94297\n",
      "************************************************************\n",
      "Iteration  57 : train_loss =  0.8849048   train_acc:  0.94075835\n",
      "Iteration  57 : test_loss =  0.8975988   test_acc:  0.94004\n",
      "************************************************************\n",
      "Iteration  58 : train_loss =  0.8581176   train_acc:  0.94447666\n",
      "Iteration  58 : test_loss =  0.8720527   test_acc:  0.94355\n",
      "************************************************************\n",
      "Iteration  59 : train_loss =  0.87801534   train_acc:  0.9410267\n",
      "Iteration  59 : test_loss =  0.8908623   test_acc:  0.94038\n",
      "************************************************************\n",
      "Iteration  60 : train_loss =  0.85114616   train_acc:  0.94496834\n",
      "Iteration  60 : test_loss =  0.865259   test_acc:  0.94403\n",
      "************************************************************\n",
      "Iteration  61 : train_loss =  0.87153405   train_acc:  0.941265\n",
      "Iteration  61 : test_loss =  0.88453674   test_acc:  0.94071\n",
      "************************************************************\n",
      "Iteration  62 : train_loss =  0.8446284   train_acc:  0.9454333\n",
      "Iteration  62 : test_loss =  0.8589311   test_acc:  0.94458\n",
      "************************************************************\n",
      "Iteration  63 : train_loss =  0.86551565   train_acc:  0.94157\n",
      "Iteration  63 : test_loss =  0.878684   test_acc:  0.94105\n",
      "************************************************************\n",
      "Iteration  64 : train_loss =  0.8386276   train_acc:  0.9458867\n",
      "Iteration  64 : test_loss =  0.85311913   test_acc:  0.94489\n",
      "************************************************************\n",
      "Iteration  65 : train_loss =  0.8598865   train_acc:  0.94180167\n",
      "Iteration  65 : test_loss =  0.8732087   test_acc:  0.94128\n",
      "************************************************************\n",
      "Iteration  66 : train_loss =  0.8328683   train_acc:  0.946275\n",
      "Iteration  66 : test_loss =  0.84753406   test_acc:  0.9455\n",
      "************************************************************\n",
      "Iteration  67 : train_loss =  0.85459524   train_acc:  0.94202834\n",
      "Iteration  67 : test_loss =  0.8680686   test_acc:  0.94144\n",
      "************************************************************\n",
      "Iteration  68 : train_loss =  0.827417   train_acc:  0.94664335\n",
      "Iteration  68 : test_loss =  0.8422711   test_acc:  0.94593\n",
      "************************************************************\n",
      "Iteration  69 : train_loss =  0.8495444   train_acc:  0.942225\n",
      "Iteration  69 : test_loss =  0.8631628   test_acc:  0.9417\n",
      "************************************************************\n",
      "Iteration  70 : train_loss =  0.82220775   train_acc:  0.9470083\n",
      "Iteration  70 : test_loss =  0.8372251   test_acc:  0.94624\n",
      "************************************************************\n",
      "Iteration  71 : train_loss =  0.8446351   train_acc:  0.942455\n",
      "Iteration  71 : test_loss =  0.8583836   test_acc:  0.94201\n",
      "************************************************************\n",
      "Iteration  72 : train_loss =  0.8172441   train_acc:  0.9473467\n",
      "Iteration  72 : test_loss =  0.83242756   test_acc:  0.94661\n",
      "************************************************************\n",
      "Iteration  73 : train_loss =  0.8400401   train_acc:  0.9426517\n",
      "Iteration  73 : test_loss =  0.85391915   test_acc:  0.94214\n",
      "************************************************************\n",
      "Iteration  74 : train_loss =  0.81254005   train_acc:  0.94769\n",
      "Iteration  74 : test_loss =  0.82789904   test_acc:  0.94689\n",
      "************************************************************\n",
      "Iteration  75 : train_loss =  0.8356298   train_acc:  0.9428833\n",
      "Iteration  75 : test_loss =  0.8496453   test_acc:  0.94238\n",
      "************************************************************\n",
      "Iteration  76 : train_loss =  0.8080353   train_acc:  0.9479617\n",
      "Iteration  76 : test_loss =  0.8235492   test_acc:  0.94707\n",
      "************************************************************\n",
      "Iteration  77 : train_loss =  0.83136415   train_acc:  0.943135\n",
      "Iteration  77 : test_loss =  0.8454877   test_acc:  0.94255\n",
      "************************************************************\n",
      "Iteration  78 : train_loss =  0.8037816   train_acc:  0.94821835\n",
      "Iteration  78 : test_loss =  0.8194292   test_acc:  0.94734\n",
      "************************************************************\n",
      "Iteration  79 : train_loss =  0.8271956   train_acc:  0.94332665\n",
      "Iteration  79 : test_loss =  0.8414039   test_acc:  0.94282\n",
      "************************************************************\n",
      "Iteration  80 : train_loss =  0.7995677   train_acc:  0.94846\n",
      "Iteration  80 : test_loss =  0.8153525   test_acc:  0.94745\n",
      "************************************************************\n",
      "Iteration  81 : train_loss =  0.82332027   train_acc:  0.94350165\n",
      "Iteration  81 : test_loss =  0.8376504   test_acc:  0.94299\n",
      "************************************************************\n",
      "Iteration  82 : train_loss =  0.79556847   train_acc:  0.94870335\n",
      "Iteration  82 : test_loss =  0.8114497   test_acc:  0.9477\n",
      "************************************************************\n",
      "Iteration  83 : train_loss =  0.8195746   train_acc:  0.94367164\n",
      "Iteration  83 : test_loss =  0.8340115   test_acc:  0.94315\n",
      "************************************************************\n",
      "Iteration  84 : train_loss =  0.7917029   train_acc:  0.94893664\n",
      "Iteration  84 : test_loss =  0.8077273   test_acc:  0.94801\n",
      "************************************************************\n",
      "Iteration  85 : train_loss =  0.81595767   train_acc:  0.94386333\n",
      "Iteration  85 : test_loss =  0.8305029   test_acc:  0.94333\n",
      "************************************************************\n",
      "Iteration  86 : train_loss =  0.78797877   train_acc:  0.94916165\n",
      "Iteration  86 : test_loss =  0.804125   test_acc:  0.94825\n",
      "************************************************************\n",
      "Iteration  87 : train_loss =  0.8124725   train_acc:  0.94405\n",
      "Iteration  87 : test_loss =  0.82712424   test_acc:  0.94351\n",
      "************************************************************\n",
      "Iteration  88 : train_loss =  0.7843737   train_acc:  0.9494283\n",
      "Iteration  88 : test_loss =  0.8006496   test_acc:  0.94842\n",
      "************************************************************\n",
      "Iteration  89 : train_loss =  0.8090981   train_acc:  0.94426334\n",
      "Iteration  89 : test_loss =  0.8238426   test_acc:  0.94377\n",
      "************************************************************\n",
      "Iteration  90 : train_loss =  0.7809134   train_acc:  0.94962\n",
      "Iteration  90 : test_loss =  0.7973873   test_acc:  0.94861\n",
      "************************************************************\n",
      "Iteration  91 : train_loss =  0.8058658   train_acc:  0.9444583\n",
      "Iteration  91 : test_loss =  0.82071894   test_acc:  0.9439\n",
      "************************************************************\n",
      "Iteration  92 : train_loss =  0.77753466   train_acc:  0.94986665\n",
      "Iteration  92 : test_loss =  0.7940629   test_acc:  0.94889\n",
      "************************************************************\n",
      "Iteration  93 : train_loss =  0.8027216   train_acc:  0.94461\n",
      "Iteration  93 : test_loss =  0.8176762   test_acc:  0.94403\n",
      "************************************************************\n",
      "Iteration  94 : train_loss =  0.77430344   train_acc:  0.95003664\n",
      "Iteration  94 : test_loss =  0.790965   test_acc:  0.94899\n",
      "************************************************************\n",
      "Iteration  95 : train_loss =  0.799681   train_acc:  0.94483\n",
      "Iteration  95 : test_loss =  0.8147259   test_acc:  0.94426\n",
      "************************************************************\n",
      "Iteration  96 : train_loss =  0.77117723   train_acc:  0.95027167\n",
      "Iteration  96 : test_loss =  0.7880205   test_acc:  0.94915\n",
      "************************************************************\n",
      "Iteration  97 : train_loss =  0.7967535   train_acc:  0.9449883\n",
      "Iteration  97 : test_loss =  0.8118857   test_acc:  0.94452\n",
      "************************************************************\n",
      "Iteration  98 : train_loss =  0.76812774   train_acc:  0.950475\n",
      "Iteration  98 : test_loss =  0.78510076   test_acc:  0.94926\n",
      "************************************************************\n",
      "Iteration  99 : train_loss =  0.7943309   train_acc:  0.94509834\n",
      "Iteration  99 : test_loss =  0.809583   test_acc:  0.94457\n",
      "************************************************************\n",
      "Iteration  100 : train_loss =  0.7651483   train_acc:  0.95070165\n",
      "Iteration  100 : test_loss =  0.78220135   test_acc:  0.94945\n",
      "************************************************************\n",
      "Iteration  101 : train_loss =  0.7913422   train_acc:  0.94526166\n",
      "Iteration  101 : test_loss =  0.8066837   test_acc:  0.94483\n",
      "************************************************************\n",
      "Iteration  102 : train_loss =  0.76226276   train_acc:  0.9508683\n",
      "Iteration  102 : test_loss =  0.7794174   test_acc:  0.94954\n",
      "************************************************************\n",
      "Iteration  103 : train_loss =  0.7885827   train_acc:  0.94546664\n",
      "Iteration  103 : test_loss =  0.80401736   test_acc:  0.94492\n",
      "************************************************************\n",
      "Iteration  104 : train_loss =  0.7594599   train_acc:  0.95102334\n",
      "Iteration  104 : test_loss =  0.77671456   test_acc:  0.94968\n",
      "************************************************************\n",
      "Iteration  105 : train_loss =  0.7859401   train_acc:  0.9456133\n",
      "Iteration  105 : test_loss =  0.8014602   test_acc:  0.94506\n",
      "************************************************************\n",
      "Iteration  106 : train_loss =  0.7567197   train_acc:  0.9511783\n",
      "Iteration  106 : test_loss =  0.7740905   test_acc:  0.94981\n",
      "************************************************************\n",
      "Iteration  107 : train_loss =  0.7833914   train_acc:  0.94577336\n",
      "Iteration  107 : test_loss =  0.79899395   test_acc:  0.9452\n",
      "************************************************************\n",
      "Iteration  108 : train_loss =  0.75404817   train_acc:  0.95138335\n",
      "Iteration  108 : test_loss =  0.77153456   test_acc:  0.95003\n",
      "************************************************************\n",
      "Iteration  109 : train_loss =  0.7808784   train_acc:  0.9459233\n",
      "Iteration  109 : test_loss =  0.79654366   test_acc:  0.94532\n",
      "************************************************************\n",
      "Iteration  110 : train_loss =  0.75145566   train_acc:  0.9515167\n",
      "Iteration  110 : test_loss =  0.7690824   test_acc:  0.95012\n",
      "************************************************************\n",
      "Iteration  111 : train_loss =  0.7784114   train_acc:  0.9460433\n",
      "Iteration  111 : test_loss =  0.7941561   test_acc:  0.94556\n",
      "************************************************************\n",
      "Iteration  112 : train_loss =  0.74889946   train_acc:  0.95167667\n",
      "Iteration  112 : test_loss =  0.766637   test_acc:  0.95014\n",
      "************************************************************\n",
      "Iteration  113 : train_loss =  0.77601874   train_acc:  0.94618165\n",
      "Iteration  113 : test_loss =  0.7918475   test_acc:  0.94566\n",
      "************************************************************\n",
      "Iteration  114 : train_loss =  0.7463948   train_acc:  0.95181835\n",
      "Iteration  114 : test_loss =  0.76423305   test_acc:  0.95023\n",
      "************************************************************\n",
      "Iteration  115 : train_loss =  0.7736948   train_acc:  0.946335\n",
      "Iteration  115 : test_loss =  0.78959835   test_acc:  0.94581\n",
      "************************************************************\n",
      "Iteration  116 : train_loss =  0.7439318   train_acc:  0.95194334\n",
      "Iteration  116 : test_loss =  0.76187134   test_acc:  0.9503\n",
      "************************************************************\n",
      "Iteration  117 : train_loss =  0.7713982   train_acc:  0.9464617\n",
      "Iteration  117 : test_loss =  0.7873801   test_acc:  0.94603\n",
      "************************************************************\n",
      "Iteration  118 : train_loss =  0.7415208   train_acc:  0.95207\n",
      "Iteration  118 : test_loss =  0.7595494   test_acc:  0.95039\n",
      "************************************************************\n",
      "Iteration  119 : train_loss =  0.76916057   train_acc:  0.94659835\n",
      "Iteration  119 : test_loss =  0.78521997   test_acc:  0.94619\n",
      "************************************************************\n",
      "Iteration  120 : train_loss =  0.73915666   train_acc:  0.95217335\n",
      "Iteration  120 : test_loss =  0.75727254   test_acc:  0.9505\n",
      "************************************************************\n",
      "Iteration  121 : train_loss =  0.7669668   train_acc:  0.9467533\n",
      "Iteration  121 : test_loss =  0.7831139   test_acc:  0.94636\n",
      "************************************************************\n",
      "Iteration  122 : train_loss =  0.7368342   train_acc:  0.95230836\n",
      "Iteration  122 : test_loss =  0.7550582   test_acc:  0.95064\n",
      "************************************************************\n",
      "Iteration  123 : train_loss =  0.7648148   train_acc:  0.94687164\n",
      "Iteration  123 : test_loss =  0.7810628   test_acc:  0.94647\n",
      "************************************************************\n",
      "Iteration  124 : train_loss =  0.7345733   train_acc:  0.952435\n",
      "Iteration  124 : test_loss =  0.75292784   test_acc:  0.95078\n",
      "************************************************************\n",
      "Iteration  125 : train_loss =  0.7626839   train_acc:  0.94698334\n",
      "Iteration  125 : test_loss =  0.77904236   test_acc:  0.94656\n",
      "************************************************************\n",
      "Iteration  126 : train_loss =  0.73236215   train_acc:  0.95253664\n",
      "Iteration  126 : test_loss =  0.7508302   test_acc:  0.95093\n",
      "************************************************************\n",
      "Iteration  127 : train_loss =  0.7606067   train_acc:  0.9471167\n",
      "Iteration  127 : test_loss =  0.7770417   test_acc:  0.94666\n",
      "************************************************************\n",
      "Iteration  128 : train_loss =  0.73018587   train_acc:  0.9526783\n",
      "Iteration  128 : test_loss =  0.7487991   test_acc:  0.95102\n",
      "************************************************************\n",
      "Iteration  129 : train_loss =  0.7591419   train_acc:  0.94719833\n",
      "Iteration  129 : test_loss =  0.77568865   test_acc:  0.94669\n",
      "************************************************************\n",
      "Iteration  130 : train_loss =  0.7280797   train_acc:  0.9527983\n",
      "Iteration  130 : test_loss =  0.746775   test_acc:  0.95114\n",
      "************************************************************\n",
      "Iteration  131 : train_loss =  0.75703615   train_acc:  0.94733\n",
      "Iteration  131 : test_loss =  0.77366006   test_acc:  0.94686\n",
      "************************************************************\n",
      "Iteration  132 : train_loss =  0.72602624   train_acc:  0.9529383\n",
      "Iteration  132 : test_loss =  0.74479973   test_acc:  0.9512\n",
      "************************************************************\n",
      "Iteration  133 : train_loss =  0.75500274   train_acc:  0.94746834\n",
      "Iteration  133 : test_loss =  0.77169883   test_acc:  0.94695\n",
      "************************************************************\n",
      "Iteration  134 : train_loss =  0.7240003   train_acc:  0.9530417\n",
      "Iteration  134 : test_loss =  0.74284714   test_acc:  0.95132\n",
      "************************************************************\n",
      "Iteration  135 : train_loss =  0.75302726   train_acc:  0.94759166\n",
      "Iteration  135 : test_loss =  0.7698074   test_acc:  0.94707\n",
      "************************************************************\n",
      "Iteration  136 : train_loss =  0.7220301   train_acc:  0.95317835\n",
      "Iteration  136 : test_loss =  0.7409841   test_acc:  0.95138\n",
      "************************************************************\n",
      "Iteration  137 : train_loss =  0.75107276   train_acc:  0.94767666\n",
      "Iteration  137 : test_loss =  0.76794714   test_acc:  0.94731\n",
      "************************************************************\n",
      "Iteration  138 : train_loss =  0.72009546   train_acc:  0.95329833\n",
      "Iteration  138 : test_loss =  0.73913723   test_acc:  0.95145\n",
      "************************************************************\n",
      "Iteration  139 : train_loss =  0.74917835   train_acc:  0.947795\n",
      "Iteration  139 : test_loss =  0.7661409   test_acc:  0.94738\n",
      "************************************************************\n",
      "Iteration  140 : train_loss =  0.7182131   train_acc:  0.9533783\n",
      "Iteration  140 : test_loss =  0.7373254   test_acc:  0.95158\n",
      "************************************************************\n",
      "Iteration  141 : train_loss =  0.7473206   train_acc:  0.9479167\n",
      "Iteration  141 : test_loss =  0.76437765   test_acc:  0.94744\n",
      "************************************************************\n",
      "Iteration  142 : train_loss =  0.7163563   train_acc:  0.9535\n",
      "Iteration  142 : test_loss =  0.73556083   test_acc:  0.95165\n",
      "************************************************************\n",
      "Iteration  143 : train_loss =  0.7455284   train_acc:  0.94803333\n",
      "Iteration  143 : test_loss =  0.76267207   test_acc:  0.94758\n",
      "************************************************************\n",
      "Iteration  144 : train_loss =  0.71454805   train_acc:  0.95358664\n",
      "Iteration  144 : test_loss =  0.73384714   test_acc:  0.95171\n",
      "************************************************************\n",
      "Iteration  145 : train_loss =  0.7437542   train_acc:  0.94814\n",
      "Iteration  145 : test_loss =  0.76098704   test_acc:  0.94763\n",
      "************************************************************\n",
      "Iteration  146 : train_loss =  0.71278805   train_acc:  0.953725\n",
      "Iteration  146 : test_loss =  0.73217314   test_acc:  0.9518\n",
      "************************************************************\n",
      "Iteration  147 : train_loss =  0.74200183   train_acc:  0.94826\n",
      "Iteration  147 : test_loss =  0.759323   test_acc:  0.94775\n",
      "************************************************************\n",
      "Iteration  148 : train_loss =  0.7110436   train_acc:  0.9538083\n",
      "Iteration  148 : test_loss =  0.73051655   test_acc:  0.95189\n",
      "************************************************************\n",
      "Iteration  149 : train_loss =  0.7402328   train_acc:  0.94839\n",
      "Iteration  149 : test_loss =  0.7576422   test_acc:  0.94784\n",
      "************************************************************\n",
      "Iteration  150 : train_loss =  0.70928067   train_acc:  0.95391\n",
      "Iteration  150 : test_loss =  0.72885334   test_acc:  0.95201\n",
      "************************************************************\n",
      "Iteration  151 : train_loss =  0.73861   train_acc:  0.9484983\n",
      "Iteration  151 : test_loss =  0.7561275   test_acc:  0.94798\n",
      "************************************************************\n",
      "Iteration  152 : train_loss =  0.7076253   train_acc:  0.9539883\n",
      "Iteration  152 : test_loss =  0.7272886   test_acc:  0.95208\n",
      "************************************************************\n",
      "Iteration  153 : train_loss =  0.73693544   train_acc:  0.94859\n",
      "Iteration  153 : test_loss =  0.7545378   test_acc:  0.94808\n",
      "************************************************************\n",
      "Iteration  154 : train_loss =  0.7061124   train_acc:  0.9540617\n",
      "Iteration  154 : test_loss =  0.72587687   test_acc:  0.95222\n",
      "************************************************************\n",
      "Iteration  155 : train_loss =  0.7352986   train_acc:  0.948685\n",
      "Iteration  155 : test_loss =  0.75299644   test_acc:  0.94816\n",
      "************************************************************\n",
      "Iteration  156 : train_loss =  0.70450604   train_acc:  0.9541467\n",
      "Iteration  156 : test_loss =  0.7243481   test_acc:  0.95228\n",
      "************************************************************\n",
      "Iteration  157 : train_loss =  0.7336555   train_acc:  0.94882\n",
      "Iteration  157 : test_loss =  0.75144106   test_acc:  0.94823\n",
      "************************************************************\n",
      "Iteration  158 : train_loss =  0.7029142   train_acc:  0.9542633\n",
      "Iteration  158 : test_loss =  0.722848   test_acc:  0.95239\n",
      "************************************************************\n",
      "Iteration  159 : train_loss =  0.7320586   train_acc:  0.94891334\n",
      "Iteration  159 : test_loss =  0.7499263   test_acc:  0.94824\n",
      "************************************************************\n",
      "Iteration  160 : train_loss =  0.701363   train_acc:  0.95432\n",
      "Iteration  160 : test_loss =  0.721379   test_acc:  0.95247\n",
      "************************************************************\n",
      "Iteration  161 : train_loss =  0.7304639   train_acc:  0.949015\n",
      "Iteration  161 : test_loss =  0.74841994   test_acc:  0.94828\n",
      "************************************************************\n",
      "Iteration  162 : train_loss =  0.6998262   train_acc:  0.954385\n",
      "Iteration  162 : test_loss =  0.719925   test_acc:  0.95259\n",
      "************************************************************\n",
      "Iteration  163 : train_loss =  0.72889674   train_acc:  0.94910836\n",
      "Iteration  163 : test_loss =  0.74694306   test_acc:  0.94834\n",
      "************************************************************\n",
      "Iteration  164 : train_loss =  0.6983056   train_acc:  0.95444834\n",
      "Iteration  164 : test_loss =  0.71849495   test_acc:  0.9527\n",
      "************************************************************\n",
      "Iteration  165 : train_loss =  0.72737134   train_acc:  0.9491983\n",
      "Iteration  165 : test_loss =  0.7455127   test_acc:  0.94832\n",
      "************************************************************\n",
      "Iteration  166 : train_loss =  0.69680107   train_acc:  0.9545467\n",
      "Iteration  166 : test_loss =  0.71709836   test_acc:  0.95279\n",
      "************************************************************\n",
      "Iteration  167 : train_loss =  0.72585595   train_acc:  0.949285\n",
      "Iteration  167 : test_loss =  0.74409795   test_acc:  0.94843\n",
      "************************************************************\n",
      "Iteration  168 : train_loss =  0.6952972   train_acc:  0.95459664\n",
      "Iteration  168 : test_loss =  0.71568567   test_acc:  0.95288\n",
      "************************************************************\n",
      "Iteration  169 : train_loss =  0.7243572   train_acc:  0.949365\n",
      "Iteration  169 : test_loss =  0.7426749   test_acc:  0.9485\n",
      "************************************************************\n",
      "Iteration  170 : train_loss =  0.6939257   train_acc:  0.9546833\n",
      "Iteration  170 : test_loss =  0.7144148   test_acc:  0.95298\n",
      "************************************************************\n",
      "Iteration  171 : train_loss =  0.72289586   train_acc:  0.9494683\n",
      "Iteration  171 : test_loss =  0.7412949   test_acc:  0.94856\n",
      "************************************************************\n",
      "Iteration  172 : train_loss =  0.6925884   train_acc:  0.954775\n",
      "Iteration  172 : test_loss =  0.713134   test_acc:  0.95299\n",
      "************************************************************\n",
      "Iteration  173 : train_loss =  0.7214857   train_acc:  0.94956\n",
      "Iteration  173 : test_loss =  0.73998606   test_acc:  0.94869\n",
      "************************************************************\n",
      "Iteration  174 : train_loss =  0.691197   train_acc:  0.95485836\n",
      "Iteration  174 : test_loss =  0.711797   test_acc:  0.95303\n",
      "************************************************************\n",
      "Iteration  175 : train_loss =  0.72009873   train_acc:  0.9496317\n",
      "Iteration  175 : test_loss =  0.7386695   test_acc:  0.94873\n",
      "************************************************************\n",
      "Iteration  176 : train_loss =  0.6898343   train_acc:  0.95494336\n",
      "Iteration  176 : test_loss =  0.7105263   test_acc:  0.95308\n",
      "************************************************************\n",
      "Iteration  177 : train_loss =  0.718698   train_acc:  0.94974\n",
      "Iteration  177 : test_loss =  0.7373533   test_acc:  0.9488\n",
      "************************************************************\n",
      "Iteration  178 : train_loss =  0.68856555   train_acc:  0.9550083\n",
      "Iteration  178 : test_loss =  0.70935154   test_acc:  0.95311\n",
      "************************************************************\n",
      "Iteration  179 : train_loss =  0.7173594   train_acc:  0.94982666\n",
      "Iteration  179 : test_loss =  0.7360923   test_acc:  0.94891\n",
      "************************************************************\n",
      "Iteration  180 : train_loss =  0.68720883   train_acc:  0.9550767\n",
      "Iteration  180 : test_loss =  0.70807034   test_acc:  0.95319\n",
      "************************************************************\n",
      "Iteration  181 : train_loss =  0.7160268   train_acc:  0.9499417\n",
      "Iteration  181 : test_loss =  0.7348469   test_acc:  0.94903\n",
      "************************************************************\n",
      "Iteration  182 : train_loss =  0.6858839   train_acc:  0.95518833\n",
      "Iteration  182 : test_loss =  0.70681804   test_acc:  0.95322\n",
      "************************************************************\n",
      "Iteration  183 : train_loss =  0.7146872   train_acc:  0.95\n",
      "Iteration  183 : test_loss =  0.7335779   test_acc:  0.94912\n",
      "************************************************************\n",
      "Iteration  184 : train_loss =  0.68466115   train_acc:  0.95526\n",
      "Iteration  184 : test_loss =  0.70569706   test_acc:  0.95332\n",
      "************************************************************\n",
      "Iteration  185 : train_loss =  0.71340805   train_acc:  0.95008\n",
      "Iteration  185 : test_loss =  0.73240155   test_acc:  0.9492\n",
      "************************************************************\n",
      "Iteration  186 : train_loss =  0.68336284   train_acc:  0.95533335\n",
      "Iteration  186 : test_loss =  0.70446247   test_acc:  0.95334\n",
      "************************************************************\n",
      "Iteration  187 : train_loss =  0.71213084   train_acc:  0.95015335\n",
      "Iteration  187 : test_loss =  0.73118645   test_acc:  0.94929\n",
      "************************************************************\n",
      "Iteration  188 : train_loss =  0.6821865   train_acc:  0.955375\n",
      "Iteration  188 : test_loss =  0.7033777   test_acc:  0.95339\n",
      "************************************************************\n",
      "Iteration  189 : train_loss =  0.71087986   train_acc:  0.95022166\n",
      "Iteration  189 : test_loss =  0.73004436   test_acc:  0.94938\n",
      "************************************************************\n",
      "Iteration  190 : train_loss =  0.68093765   train_acc:  0.9554383\n",
      "Iteration  190 : test_loss =  0.70221466   test_acc:  0.95353\n",
      "************************************************************\n",
      "Iteration  191 : train_loss =  0.7096124   train_acc:  0.9503033\n",
      "Iteration  191 : test_loss =  0.7288361   test_acc:  0.94948\n",
      "************************************************************\n",
      "Iteration  192 : train_loss =  0.6797371   train_acc:  0.95549\n",
      "Iteration  192 : test_loss =  0.7011078   test_acc:  0.95354\n",
      "************************************************************\n",
      "Iteration  193 : train_loss =  0.708401   train_acc:  0.95036167\n",
      "Iteration  193 : test_loss =  0.72771287   test_acc:  0.94957\n",
      "************************************************************\n",
      "Iteration  194 : train_loss =  0.6786168   train_acc:  0.95553833\n",
      "Iteration  194 : test_loss =  0.7000692   test_acc:  0.95359\n",
      "************************************************************\n",
      "Iteration  195 : train_loss =  0.70720154   train_acc:  0.95045\n",
      "Iteration  195 : test_loss =  0.72657347   test_acc:  0.9496\n",
      "************************************************************\n",
      "Iteration  196 : train_loss =  0.6773948   train_acc:  0.95560664\n",
      "Iteration  196 : test_loss =  0.69892126   test_acc:  0.95369\n",
      "************************************************************\n",
      "Iteration  197 : train_loss =  0.70602626   train_acc:  0.95050836\n",
      "Iteration  197 : test_loss =  0.7254794   test_acc:  0.94971\n",
      "************************************************************\n",
      "Iteration  198 : train_loss =  0.6763055   train_acc:  0.95566\n",
      "Iteration  198 : test_loss =  0.6979264   test_acc:  0.95376\n",
      "************************************************************\n",
      "Iteration  199 : train_loss =  0.70485115   train_acc:  0.950565\n",
      "Iteration  199 : test_loss =  0.724374   test_acc:  0.94981\n",
      "************************************************************\n",
      "Iteration  200 : train_loss =  0.6751332   train_acc:  0.95572335\n",
      "Iteration  200 : test_loss =  0.6968245   test_acc:  0.95379\n",
      "************************************************************\n",
      "Iteration  201 : train_loss =  0.703688   train_acc:  0.9506583\n",
      "Iteration  201 : test_loss =  0.7232824   test_acc:  0.94986\n",
      "************************************************************\n",
      "Iteration  202 : train_loss =  0.6740827   train_acc:  0.9557717\n",
      "Iteration  202 : test_loss =  0.6958583   test_acc:  0.95383\n",
      "************************************************************\n",
      "Iteration  203 : train_loss =  0.70254874   train_acc:  0.9507083\n",
      "Iteration  203 : test_loss =  0.7222113   test_acc:  0.94995\n",
      "************************************************************\n",
      "Iteration  204 : train_loss =  0.6729459   train_acc:  0.95580333\n",
      "Iteration  204 : test_loss =  0.6947857   test_acc:  0.95383\n",
      "************************************************************\n",
      "Iteration  205 : train_loss =  0.7013941   train_acc:  0.95079\n",
      "Iteration  205 : test_loss =  0.721121   test_acc:  0.95003\n",
      "************************************************************\n",
      "Iteration  206 : train_loss =  0.6719147   train_acc:  0.95584667\n",
      "Iteration  206 : test_loss =  0.693834   test_acc:  0.95389\n",
      "************************************************************\n",
      "Iteration  207 : train_loss =  0.70028985   train_acc:  0.950845\n",
      "Iteration  207 : test_loss =  0.7200921   test_acc:  0.9501\n",
      "************************************************************\n",
      "Iteration  208 : train_loss =  0.6708258   train_acc:  0.9559067\n",
      "Iteration  208 : test_loss =  0.692804   test_acc:  0.95396\n",
      "************************************************************\n",
      "Iteration  209 : train_loss =  0.69916356   train_acc:  0.95094\n",
      "Iteration  209 : test_loss =  0.71902907   test_acc:  0.95016\n",
      "************************************************************\n",
      "Iteration  210 : train_loss =  0.6697265   train_acc:  0.955955\n",
      "Iteration  210 : test_loss =  0.6917734   test_acc:  0.954\n",
      "************************************************************\n",
      "Iteration  211 : train_loss =  0.69808626   train_acc:  0.95100164\n",
      "Iteration  211 : test_loss =  0.71801883   test_acc:  0.95023\n",
      "************************************************************\n",
      "Iteration  212 : train_loss =  0.66873515   train_acc:  0.95598835\n",
      "Iteration  212 : test_loss =  0.6908462   test_acc:  0.95403\n",
      "************************************************************\n",
      "Iteration  213 : train_loss =  0.697018   train_acc:  0.9510717\n",
      "Iteration  213 : test_loss =  0.7170143   test_acc:  0.95032\n",
      "************************************************************\n",
      "Iteration  214 : train_loss =  0.66765445   train_acc:  0.956045\n",
      "Iteration  214 : test_loss =  0.6898195   test_acc:  0.95405\n",
      "************************************************************\n",
      "Iteration  215 : train_loss =  0.6959534   train_acc:  0.95112\n",
      "Iteration  215 : test_loss =  0.71602476   test_acc:  0.95031\n",
      "************************************************************\n",
      "Iteration  216 : train_loss =  0.66668946   train_acc:  0.95607334\n",
      "Iteration  216 : test_loss =  0.68893313   test_acc:  0.95412\n",
      "************************************************************\n",
      "Iteration  217 : train_loss =  0.6949048   train_acc:  0.95119\n",
      "Iteration  217 : test_loss =  0.71505183   test_acc:  0.95042\n",
      "************************************************************\n",
      "Iteration  218 : train_loss =  0.6656422   train_acc:  0.95609665\n",
      "Iteration  218 : test_loss =  0.6879463   test_acc:  0.95412\n",
      "************************************************************\n",
      "Iteration  219 : train_loss =  0.6938479   train_acc:  0.951245\n",
      "Iteration  219 : test_loss =  0.7140618   test_acc:  0.95046\n",
      "************************************************************\n",
      "Iteration  220 : train_loss =  0.664622   train_acc:  0.9561617\n",
      "Iteration  220 : test_loss =  0.68699163   test_acc:  0.95416\n",
      "************************************************************\n",
      "Iteration  221 : train_loss =  0.69283104   train_acc:  0.95129\n",
      "Iteration  221 : test_loss =  0.71310985   test_acc:  0.95053\n",
      "************************************************************\n",
      "Iteration  222 : train_loss =  0.6636922   train_acc:  0.95621336\n",
      "Iteration  222 : test_loss =  0.6861361   test_acc:  0.95414\n",
      "************************************************************\n",
      "Iteration  223 : train_loss =  0.69182914   train_acc:  0.951375\n",
      "Iteration  223 : test_loss =  0.71217805   test_acc:  0.95054\n",
      "************************************************************\n",
      "Iteration  224 : train_loss =  0.66270846   train_acc:  0.95624167\n",
      "Iteration  224 : test_loss =  0.68521076   test_acc:  0.9542\n",
      "************************************************************\n",
      "Iteration  225 : train_loss =  0.6908362   train_acc:  0.9514167\n",
      "Iteration  225 : test_loss =  0.71124643   test_acc:  0.95058\n",
      "************************************************************\n",
      "Iteration  226 : train_loss =  0.6618059   train_acc:  0.956295\n",
      "Iteration  226 : test_loss =  0.6843753   test_acc:  0.95427\n",
      "************************************************************\n",
      "Iteration  227 : train_loss =  0.689854   train_acc:  0.95148\n",
      "Iteration  227 : test_loss =  0.71032727   test_acc:  0.95067\n",
      "************************************************************\n",
      "Iteration  228 : train_loss =  0.6609046   train_acc:  0.95634\n",
      "Iteration  228 : test_loss =  0.68353623   test_acc:  0.95429\n",
      "************************************************************\n",
      "Iteration  229 : train_loss =  0.6888873   train_acc:  0.95154\n",
      "Iteration  229 : test_loss =  0.7094312   test_acc:  0.95073\n",
      "************************************************************\n",
      "Iteration  230 : train_loss =  0.6599222   train_acc:  0.95638\n",
      "Iteration  230 : test_loss =  0.6826176   test_acc:  0.95431\n",
      "************************************************************\n",
      "Iteration  231 : train_loss =  0.6878958   train_acc:  0.95159\n",
      "Iteration  231 : test_loss =  0.70850194   test_acc:  0.95077\n",
      "************************************************************\n",
      "Iteration  232 : train_loss =  0.6590463   train_acc:  0.956435\n",
      "Iteration  232 : test_loss =  0.6818172   test_acc:  0.95432\n",
      "************************************************************\n",
      "Iteration  233 : train_loss =  0.68695045   train_acc:  0.9516633\n",
      "Iteration  233 : test_loss =  0.70763165   test_acc:  0.95081\n",
      "************************************************************\n",
      "Iteration  234 : train_loss =  0.65808725   train_acc:  0.95650166\n",
      "Iteration  234 : test_loss =  0.6809382   test_acc:  0.95436\n",
      "************************************************************\n",
      "Iteration  235 : train_loss =  0.68591803   train_acc:  0.951705\n",
      "Iteration  235 : test_loss =  0.7066582   test_acc:  0.9509\n",
      "************************************************************\n",
      "Iteration  236 : train_loss =  0.6572856   train_acc:  0.95655835\n",
      "Iteration  236 : test_loss =  0.6801718   test_acc:  0.95431\n",
      "************************************************************\n",
      "Iteration  237 : train_loss =  0.68497753   train_acc:  0.9517483\n",
      "Iteration  237 : test_loss =  0.70581037   test_acc:  0.95091\n",
      "************************************************************\n",
      "Iteration  238 : train_loss =  0.6563172   train_acc:  0.95661664\n",
      "Iteration  238 : test_loss =  0.6793664   test_acc:  0.95431\n",
      "************************************************************\n",
      "Iteration  239 : train_loss =  0.6843211   train_acc:  0.95178\n",
      "Iteration  239 : test_loss =  0.7052972   test_acc:  0.95095\n",
      "************************************************************\n",
      "Iteration  240 : train_loss =  0.65550345   train_acc:  0.95665\n",
      "Iteration  240 : test_loss =  0.67859954   test_acc:  0.95437\n",
      "************************************************************\n",
      "Iteration  241 : train_loss =  0.6834017   train_acc:  0.95185834\n",
      "Iteration  241 : test_loss =  0.7044453   test_acc:  0.95108\n",
      "************************************************************\n",
      "Iteration  242 : train_loss =  0.6546787   train_acc:  0.9566967\n",
      "Iteration  242 : test_loss =  0.67784894   test_acc:  0.95441\n",
      "************************************************************\n",
      "Iteration  243 : train_loss =  0.68249255   train_acc:  0.95190835\n",
      "Iteration  243 : test_loss =  0.70358986   test_acc:  0.95112\n",
      "************************************************************\n",
      "Iteration  244 : train_loss =  0.65388685   train_acc:  0.9567183\n",
      "Iteration  244 : test_loss =  0.67710763   test_acc:  0.95439\n",
      "************************************************************\n",
      "Iteration  245 : train_loss =  0.6816051   train_acc:  0.951985\n",
      "Iteration  245 : test_loss =  0.70278573   test_acc:  0.95125\n",
      "************************************************************\n",
      "Iteration  246 : train_loss =  0.65308475   train_acc:  0.95674\n",
      "Iteration  246 : test_loss =  0.67638373   test_acc:  0.95441\n",
      "************************************************************\n",
      "Iteration  247 : train_loss =  0.6807262   train_acc:  0.952035\n",
      "Iteration  247 : test_loss =  0.7019557   test_acc:  0.95129\n",
      "************************************************************\n",
      "Iteration  248 : train_loss =  0.65231407   train_acc:  0.9567533\n",
      "Iteration  248 : test_loss =  0.675668   test_acc:  0.95446\n",
      "************************************************************\n",
      "Iteration  249 : train_loss =  0.67977554   train_acc:  0.9520767\n",
      "Iteration  249 : test_loss =  0.7010959   test_acc:  0.95132\n",
      "************************************************************\n",
      "Iteration  250 : train_loss =  0.6514431   train_acc:  0.95680666\n",
      "Iteration  250 : test_loss =  0.6748807   test_acc:  0.95456\n",
      "************************************************************\n",
      "Iteration  251 : train_loss =  0.67892057   train_acc:  0.952115\n",
      "Iteration  251 : test_loss =  0.7003032   test_acc:  0.95134\n",
      "************************************************************\n",
      "Iteration  252 : train_loss =  0.65059   train_acc:  0.95685166\n",
      "Iteration  252 : test_loss =  0.67409384   test_acc:  0.9546\n",
      "************************************************************\n",
      "Iteration  253 : train_loss =  0.67797065   train_acc:  0.95216\n",
      "Iteration  253 : test_loss =  0.699406   test_acc:  0.95139\n",
      "************************************************************\n",
      "Iteration  254 : train_loss =  0.6498745   train_acc:  0.95687336\n",
      "Iteration  254 : test_loss =  0.67340815   test_acc:  0.95462\n",
      "************************************************************\n",
      "Iteration  255 : train_loss =  0.67713404   train_acc:  0.9522333\n",
      "Iteration  255 : test_loss =  0.698622   test_acc:  0.95142\n",
      "************************************************************\n",
      "Iteration  256 : train_loss =  0.6490306   train_acc:  0.95692164\n",
      "Iteration  256 : test_loss =  0.6726432   test_acc:  0.95463\n",
      "************************************************************\n",
      "Iteration  257 : train_loss =  0.676221   train_acc:  0.952275\n",
      "Iteration  257 : test_loss =  0.6977865   test_acc:  0.95147\n",
      "************************************************************\n",
      "Iteration  258 : train_loss =  0.64822656   train_acc:  0.95697665\n",
      "Iteration  258 : test_loss =  0.6718834   test_acc:  0.95467\n",
      "************************************************************\n",
      "Iteration  259 : train_loss =  0.67530364   train_acc:  0.95232165\n",
      "Iteration  259 : test_loss =  0.6969174   test_acc:  0.95153\n",
      "************************************************************\n",
      "Iteration  260 : train_loss =  0.64743453   train_acc:  0.957\n",
      "Iteration  260 : test_loss =  0.6711376   test_acc:  0.95473\n",
      "************************************************************\n",
      "Iteration  261 : train_loss =  0.67447877   train_acc:  0.9523683\n",
      "Iteration  261 : test_loss =  0.6961191   test_acc:  0.95163\n",
      "************************************************************\n",
      "Iteration  262 : train_loss =  0.6467062   train_acc:  0.95704836\n",
      "Iteration  262 : test_loss =  0.6704759   test_acc:  0.95466\n",
      "************************************************************\n",
      "Iteration  263 : train_loss =  0.6736777   train_acc:  0.952395\n",
      "Iteration  263 : test_loss =  0.69538635   test_acc:  0.95165\n",
      "************************************************************\n",
      "Iteration  264 : train_loss =  0.6460115   train_acc:  0.9570917\n",
      "Iteration  264 : test_loss =  0.6698254   test_acc:  0.95472\n",
      "************************************************************\n",
      "Iteration  265 : train_loss =  0.6728667   train_acc:  0.952435\n",
      "Iteration  265 : test_loss =  0.694618   test_acc:  0.9517\n",
      "************************************************************\n",
      "Iteration  266 : train_loss =  0.6453193   train_acc:  0.95713335\n",
      "Iteration  266 : test_loss =  0.669181   test_acc:  0.95473\n",
      "************************************************************\n",
      "Iteration  267 : train_loss =  0.6719832   train_acc:  0.95249\n",
      "Iteration  267 : test_loss =  0.6937981   test_acc:  0.95174\n",
      "************************************************************\n",
      "Iteration  268 : train_loss =  0.64455354   train_acc:  0.9571767\n",
      "Iteration  268 : test_loss =  0.6684604   test_acc:  0.95477\n",
      "************************************************************\n",
      "Iteration  269 : train_loss =  0.6711417   train_acc:  0.9525267\n",
      "Iteration  269 : test_loss =  0.69299364   test_acc:  0.9518\n",
      "************************************************************\n",
      "Iteration  270 : train_loss =  0.6437301   train_acc:  0.95722836\n",
      "Iteration  270 : test_loss =  0.6677369   test_acc:  0.95481\n",
      "************************************************************\n",
      "Iteration  271 : train_loss =  0.67067814   train_acc:  0.95252335\n",
      "Iteration  271 : test_loss =  0.69259256   test_acc:  0.95181\n",
      "************************************************************\n",
      "Iteration  272 : train_loss =  0.64296895   train_acc:  0.957265\n",
      "Iteration  272 : test_loss =  0.6670153   test_acc:  0.95479\n",
      "************************************************************\n",
      "Iteration  273 : train_loss =  0.6697721   train_acc:  0.95256\n",
      "Iteration  273 : test_loss =  0.6917319   test_acc:  0.95186\n",
      "************************************************************\n",
      "Iteration  274 : train_loss =  0.6422218   train_acc:  0.9572883\n",
      "Iteration  274 : test_loss =  0.66631263   test_acc:  0.95486\n",
      "************************************************************\n",
      "Iteration  275 : train_loss =  0.66887057   train_acc:  0.95263165\n",
      "Iteration  275 : test_loss =  0.6908793   test_acc:  0.95195\n",
      "************************************************************\n",
      "Iteration  276 : train_loss =  0.6414796   train_acc:  0.957335\n",
      "Iteration  276 : test_loss =  0.66562814   test_acc:  0.95493\n",
      "************************************************************\n",
      "Iteration  277 : train_loss =  0.6680846   train_acc:  0.95270336\n",
      "Iteration  277 : test_loss =  0.6901308   test_acc:  0.952\n",
      "************************************************************\n",
      "Iteration  278 : train_loss =  0.64083046   train_acc:  0.9573733\n",
      "Iteration  278 : test_loss =  0.6650326   test_acc:  0.95496\n",
      "************************************************************\n",
      "Iteration  279 : train_loss =  0.667302   train_acc:  0.9527517\n",
      "Iteration  279 : test_loss =  0.68938464   test_acc:  0.95199\n",
      "************************************************************\n",
      "Iteration  280 : train_loss =  0.6401856   train_acc:  0.9574\n",
      "Iteration  280 : test_loss =  0.6644371   test_acc:  0.95503\n",
      "************************************************************\n",
      "Iteration  281 : train_loss =  0.6665301   train_acc:  0.952815\n",
      "Iteration  281 : test_loss =  0.6886623   test_acc:  0.95201\n",
      "************************************************************\n",
      "Iteration  282 : train_loss =  0.6394807   train_acc:  0.95744\n",
      "Iteration  282 : test_loss =  0.6637877   test_acc:  0.95502\n",
      "************************************************************\n",
      "Iteration  283 : train_loss =  0.6659665   train_acc:  0.952835\n",
      "Iteration  283 : test_loss =  0.6881596   test_acc:  0.95208\n",
      "************************************************************\n",
      "Iteration  284 : train_loss =  0.6388539   train_acc:  0.957475\n",
      "Iteration  284 : test_loss =  0.66321534   test_acc:  0.95506\n",
      "************************************************************\n",
      "Iteration  285 : train_loss =  0.6652066   train_acc:  0.95288336\n",
      "Iteration  285 : test_loss =  0.6874415   test_acc:  0.95212\n",
      "************************************************************\n",
      "Iteration  286 : train_loss =  0.6382257   train_acc:  0.95750165\n",
      "Iteration  286 : test_loss =  0.662634   test_acc:  0.95507\n",
      "************************************************************\n",
      "Iteration  287 : train_loss =  0.6644642   train_acc:  0.9529333\n",
      "Iteration  287 : test_loss =  0.6867515   test_acc:  0.95214\n",
      "************************************************************\n",
      "Iteration  288 : train_loss =  0.63760376   train_acc:  0.95753664\n",
      "Iteration  288 : test_loss =  0.6620639   test_acc:  0.95507\n",
      "************************************************************\n",
      "Iteration  289 : train_loss =  0.66371095   train_acc:  0.95299\n",
      "Iteration  289 : test_loss =  0.6860476   test_acc:  0.95217\n",
      "************************************************************\n",
      "Iteration  290 : train_loss =  0.63698417   train_acc:  0.957575\n",
      "Iteration  290 : test_loss =  0.66149104   test_acc:  0.95509\n",
      "************************************************************\n",
      "Iteration  291 : train_loss =  0.6629109   train_acc:  0.95305336\n",
      "Iteration  291 : test_loss =  0.68530184   test_acc:  0.95222\n",
      "************************************************************\n",
      "Iteration  292 : train_loss =  0.6362948   train_acc:  0.95761335\n",
      "Iteration  292 : test_loss =  0.660846   test_acc:  0.95517\n",
      "************************************************************\n",
      "Iteration  293 : train_loss =  0.6620677   train_acc:  0.95309\n",
      "Iteration  293 : test_loss =  0.6845205   test_acc:  0.95221\n",
      "************************************************************\n",
      "Iteration  294 : train_loss =  0.6356117   train_acc:  0.95764166\n",
      "Iteration  294 : test_loss =  0.6602207   test_acc:  0.95519\n",
      "************************************************************\n",
      "Iteration  295 : train_loss =  0.6613473   train_acc:  0.95315\n",
      "Iteration  295 : test_loss =  0.68384326   test_acc:  0.95223\n",
      "************************************************************\n",
      "Iteration  296 : train_loss =  0.6349481   train_acc:  0.95766336\n",
      "Iteration  296 : test_loss =  0.6596031   test_acc:  0.95519\n",
      "************************************************************\n",
      "Iteration  297 : train_loss =  0.66051865   train_acc:  0.9532\n",
      "Iteration  297 : test_loss =  0.6830706   test_acc:  0.95227\n",
      "************************************************************\n",
      "Iteration  298 : train_loss =  0.6342757   train_acc:  0.9577017\n",
      "Iteration  298 : test_loss =  0.6589875   test_acc:  0.95527\n",
      "************************************************************\n",
      "Iteration  299 : train_loss =  0.6598063   train_acc:  0.95322835\n",
      "Iteration  299 : test_loss =  0.6823965   test_acc:  0.95231\n",
      "************************************************************\n",
      "Iteration  300 : train_loss =  0.6336767   train_acc:  0.95772666\n",
      "Iteration  300 : test_loss =  0.65844333   test_acc:  0.95531\n",
      "************************************************************\n",
      "Iteration  301 : train_loss =  0.65910566   train_acc:  0.95326835\n",
      "Iteration  301 : test_loss =  0.6817541   test_acc:  0.95234\n",
      "************************************************************\n",
      "Iteration  302 : train_loss =  0.6330829   train_acc:  0.95776\n",
      "Iteration  302 : test_loss =  0.65790945   test_acc:  0.95535\n",
      "************************************************************\n",
      "Iteration  303 : train_loss =  0.6584004   train_acc:  0.95331\n",
      "Iteration  303 : test_loss =  0.68110347   test_acc:  0.95244\n",
      "************************************************************\n",
      "Iteration  304 : train_loss =  0.6324898   train_acc:  0.95779\n",
      "Iteration  304 : test_loss =  0.65737504   test_acc:  0.95538\n",
      "************************************************************\n",
      "Iteration  305 : train_loss =  0.6577022   train_acc:  0.95334\n",
      "Iteration  305 : test_loss =  0.68045926   test_acc:  0.95249\n",
      "************************************************************\n",
      "Iteration  306 : train_loss =  0.6319041   train_acc:  0.957835\n",
      "Iteration  306 : test_loss =  0.65684247   test_acc:  0.9554\n",
      "************************************************************\n",
      "Iteration  307 : train_loss =  0.6570079   train_acc:  0.95339\n",
      "Iteration  307 : test_loss =  0.67981756   test_acc:  0.95253\n",
      "************************************************************\n",
      "Iteration  308 : train_loss =  0.631313   train_acc:  0.9578717\n",
      "Iteration  308 : test_loss =  0.6563075   test_acc:  0.95541\n",
      "************************************************************\n",
      "Iteration  309 : train_loss =  0.6562271   train_acc:  0.95344\n",
      "Iteration  309 : test_loss =  0.6791097   test_acc:  0.95252\n",
      "************************************************************\n",
      "Iteration  310 : train_loss =  0.6306615   train_acc:  0.9578983\n",
      "Iteration  310 : test_loss =  0.6557119   test_acc:  0.95547\n",
      "************************************************************\n",
      "Iteration  311 : train_loss =  0.65551543   train_acc:  0.9535033\n",
      "Iteration  311 : test_loss =  0.6784452   test_acc:  0.95253\n",
      "************************************************************\n",
      "Iteration  312 : train_loss =  0.6300224   train_acc:  0.95792836\n",
      "Iteration  312 : test_loss =  0.655125   test_acc:  0.95552\n",
      "************************************************************\n",
      "Iteration  313 : train_loss =  0.6547212   train_acc:  0.95355\n",
      "Iteration  313 : test_loss =  0.6777114   test_acc:  0.95257\n",
      "************************************************************\n",
      "Iteration  314 : train_loss =  0.6293825   train_acc:  0.95797336\n",
      "Iteration  314 : test_loss =  0.6545428   test_acc:  0.95558\n",
      "************************************************************\n",
      "Iteration  315 : train_loss =  0.6540426   train_acc:  0.953585\n",
      "Iteration  315 : test_loss =  0.67707634   test_acc:  0.95259\n",
      "************************************************************\n",
      "Iteration  316 : train_loss =  0.62881315   train_acc:  0.95802\n",
      "Iteration  316 : test_loss =  0.65402454   test_acc:  0.95557\n",
      "************************************************************\n",
      "Iteration  317 : train_loss =  0.6533669   train_acc:  0.9536383\n",
      "Iteration  317 : test_loss =  0.67645836   test_acc:  0.95261\n",
      "************************************************************\n",
      "Iteration  318 : train_loss =  0.62823653   train_acc:  0.95805335\n",
      "Iteration  318 : test_loss =  0.65350944   test_acc:  0.95551\n",
      "************************************************************\n",
      "Iteration  319 : train_loss =  0.6526948   train_acc:  0.953675\n",
      "Iteration  319 : test_loss =  0.6758345   test_acc:  0.95264\n",
      "************************************************************\n",
      "Iteration  320 : train_loss =  0.62766653   train_acc:  0.9580967\n",
      "Iteration  320 : test_loss =  0.6529942   test_acc:  0.95557\n",
      "************************************************************\n",
      "Iteration  321 : train_loss =  0.6520275   train_acc:  0.953725\n",
      "Iteration  321 : test_loss =  0.67521596   test_acc:  0.95269\n",
      "************************************************************\n",
      "Iteration  322 : train_loss =  0.62710744   train_acc:  0.95811665\n",
      "Iteration  322 : test_loss =  0.6524958   test_acc:  0.95558\n",
      "************************************************************\n",
      "Iteration  323 : train_loss =  0.65136445   train_acc:  0.95376664\n",
      "Iteration  323 : test_loss =  0.67461133   test_acc:  0.95275\n",
      "************************************************************\n",
      "Iteration  324 : train_loss =  0.6265507   train_acc:  0.95814335\n",
      "Iteration  324 : test_loss =  0.65199786   test_acc:  0.95561\n",
      "************************************************************\n",
      "Iteration  325 : train_loss =  0.65070474   train_acc:  0.95382667\n",
      "Iteration  325 : test_loss =  0.6740086   test_acc:  0.95277\n",
      "************************************************************\n",
      "Iteration  326 : train_loss =  0.62599546   train_acc:  0.9582\n",
      "Iteration  326 : test_loss =  0.65149486   test_acc:  0.95563\n",
      "************************************************************\n",
      "Iteration  327 : train_loss =  0.6500492   train_acc:  0.9538767\n",
      "Iteration  327 : test_loss =  0.67341083   test_acc:  0.95279\n",
      "************************************************************\n",
      "Iteration  328 : train_loss =  0.62545127   train_acc:  0.9582217\n",
      "Iteration  328 : test_loss =  0.65100926   test_acc:  0.95569\n",
      "************************************************************\n",
      "Iteration  329 : train_loss =  0.64932644   train_acc:  0.953925\n",
      "Iteration  329 : test_loss =  0.67275107   test_acc:  0.95284\n",
      "************************************************************\n",
      "Iteration  330 : train_loss =  0.6248356   train_acc:  0.958245\n",
      "Iteration  330 : test_loss =  0.65043104   test_acc:  0.95571\n",
      "************************************************************\n",
      "Iteration  331 : train_loss =  0.64868957   train_acc:  0.95397\n",
      "Iteration  331 : test_loss =  0.6721625   test_acc:  0.95287\n",
      "************************************************************\n",
      "Iteration  332 : train_loss =  0.6243004   train_acc:  0.9582833\n",
      "Iteration  332 : test_loss =  0.64996123   test_acc:  0.95571\n",
      "************************************************************\n",
      "Iteration  333 : train_loss =  0.6480534   train_acc:  0.954\n",
      "Iteration  333 : test_loss =  0.6715781   test_acc:  0.95296\n",
      "************************************************************\n",
      "Iteration  334 : train_loss =  0.62376237   train_acc:  0.9583117\n",
      "Iteration  334 : test_loss =  0.6494694   test_acc:  0.95577\n",
      "************************************************************\n",
      "Iteration  335 : train_loss =  0.6474171   train_acc:  0.954045\n",
      "Iteration  335 : test_loss =  0.6709967   test_acc:  0.953\n",
      "************************************************************\n",
      "Iteration  336 : train_loss =  0.6232319   train_acc:  0.958335\n",
      "Iteration  336 : test_loss =  0.648994   test_acc:  0.95583\n",
      "************************************************************\n",
      "Iteration  337 : train_loss =  0.6467096   train_acc:  0.9541117\n",
      "Iteration  337 : test_loss =  0.670351   test_acc:  0.95307\n",
      "************************************************************\n",
      "Iteration  338 : train_loss =  0.6226349   train_acc:  0.95837\n",
      "Iteration  338 : test_loss =  0.648437   test_acc:  0.95584\n",
      "************************************************************\n",
      "Iteration  339 : train_loss =  0.6459898   train_acc:  0.95415336\n",
      "Iteration  339 : test_loss =  0.669682   test_acc:  0.9531\n",
      "************************************************************\n",
      "Iteration  340 : train_loss =  0.62204194   train_acc:  0.95840335\n",
      "Iteration  340 : test_loss =  0.64788467   test_acc:  0.95584\n",
      "************************************************************\n",
      "Iteration  341 : train_loss =  0.64537317   train_acc:  0.95420164\n",
      "Iteration  341 : test_loss =  0.66910696   test_acc:  0.95314\n",
      "************************************************************\n",
      "Iteration  342 : train_loss =  0.6215289   train_acc:  0.95841664\n",
      "Iteration  342 : test_loss =  0.64741504   test_acc:  0.95589\n",
      "************************************************************\n",
      "Iteration  343 : train_loss =  0.64474833   train_acc:  0.95422333\n",
      "Iteration  343 : test_loss =  0.66854995   test_acc:  0.9532\n",
      "************************************************************\n",
      "Iteration  344 : train_loss =  0.62101954   train_acc:  0.95844835\n",
      "Iteration  344 : test_loss =  0.64697754   test_acc:  0.95589\n",
      "************************************************************\n",
      "Iteration  345 : train_loss =  0.6441385   train_acc:  0.9542633\n",
      "Iteration  345 : test_loss =  0.6679748   test_acc:  0.95324\n",
      "************************************************************\n",
      "Iteration  346 : train_loss =  0.6204931   train_acc:  0.95847166\n",
      "Iteration  346 : test_loss =  0.6465003   test_acc:  0.95592\n",
      "************************************************************\n",
      "Iteration  347 : train_loss =  0.6434573   train_acc:  0.9542933\n",
      "Iteration  347 : test_loss =  0.6673818   test_acc:  0.95327\n",
      "************************************************************\n",
      "Iteration  348 : train_loss =  0.61992615   train_acc:  0.95848835\n",
      "Iteration  348 : test_loss =  0.64600366   test_acc:  0.95595\n",
      "************************************************************\n",
      "Iteration  349 : train_loss =  0.6428561   train_acc:  0.95435834\n",
      "Iteration  349 : test_loss =  0.666808   test_acc:  0.95332\n",
      "************************************************************\n",
      "Iteration  350 : train_loss =  0.6194344   train_acc:  0.95851165\n",
      "Iteration  350 : test_loss =  0.6455294   test_acc:  0.95594\n",
      "************************************************************\n",
      "Iteration  351 : train_loss =  0.6421747   train_acc:  0.9543717\n",
      "Iteration  351 : test_loss =  0.6661912   test_acc:  0.95333\n",
      "************************************************************\n",
      "Iteration  352 : train_loss =  0.6188742   train_acc:  0.9585317\n",
      "Iteration  352 : test_loss =  0.64504373   test_acc:  0.95596\n",
      "************************************************************\n",
      "Iteration  353 : train_loss =  0.64156926   train_acc:  0.9544167\n",
      "Iteration  353 : test_loss =  0.66563046   test_acc:  0.95337\n",
      "************************************************************\n",
      "Iteration  354 : train_loss =  0.618355   train_acc:  0.95856667\n",
      "Iteration  354 : test_loss =  0.64458406   test_acc:  0.95599\n",
      "************************************************************\n",
      "Iteration  355 : train_loss =  0.6409779   train_acc:  0.954455\n",
      "Iteration  355 : test_loss =  0.6650843   test_acc:  0.95337\n",
      "************************************************************\n",
      "Iteration  356 : train_loss =  0.6178785   train_acc:  0.95859\n",
      "Iteration  356 : test_loss =  0.6441398   test_acc:  0.95598\n",
      "************************************************************\n",
      "Iteration  357 : train_loss =  0.6403929   train_acc:  0.954505\n",
      "Iteration  357 : test_loss =  0.6645581   test_acc:  0.95341\n",
      "************************************************************\n",
      "Iteration  358 : train_loss =  0.61738276   train_acc:  0.9586217\n",
      "Iteration  358 : test_loss =  0.64368856   test_acc:  0.95603\n",
      "************************************************************\n",
      "Iteration  359 : train_loss =  0.63980675   train_acc:  0.95456165\n",
      "Iteration  359 : test_loss =  0.6639949   test_acc:  0.95341\n",
      "************************************************************\n",
      "Iteration  360 : train_loss =  0.61688036   train_acc:  0.95865834\n",
      "Iteration  360 : test_loss =  0.6432585   test_acc:  0.95603\n",
      "************************************************************\n",
      "Iteration  361 : train_loss =  0.6392213   train_acc:  0.95461\n",
      "Iteration  361 : test_loss =  0.66345793   test_acc:  0.95344\n",
      "************************************************************\n",
      "Iteration  362 : train_loss =  0.61639196   train_acc:  0.95866835\n",
      "Iteration  362 : test_loss =  0.642807   test_acc:  0.95603\n",
      "************************************************************\n",
      "Iteration  363 : train_loss =  0.6385999   train_acc:  0.95462835\n",
      "Iteration  363 : test_loss =  0.6628604   test_acc:  0.95349\n",
      "************************************************************\n",
      "Iteration  364 : train_loss =  0.6158151   train_acc:  0.9586717\n",
      "Iteration  364 : test_loss =  0.6422672   test_acc:  0.95605\n",
      "************************************************************\n",
      "Iteration  365 : train_loss =  0.6379285   train_acc:  0.95464164\n",
      "Iteration  365 : test_loss =  0.6621734   test_acc:  0.95357\n",
      "************************************************************\n",
      "Iteration  366 : train_loss =  0.61525184   train_acc:  0.9587\n",
      "Iteration  366 : test_loss =  0.64175546   test_acc:  0.95608\n",
      "************************************************************\n",
      "Iteration  367 : train_loss =  0.63732105   train_acc:  0.9546817\n",
      "Iteration  367 : test_loss =  0.66156465   test_acc:  0.95357\n",
      "************************************************************\n",
      "Iteration  368 : train_loss =  0.614766   train_acc:  0.9587167\n",
      "Iteration  368 : test_loss =  0.64130974   test_acc:  0.95609\n",
      "************************************************************\n",
      "Iteration  369 : train_loss =  0.63673156   train_acc:  0.95466834\n",
      "Iteration  369 : test_loss =  0.6610655   test_acc:  0.95356\n",
      "************************************************************\n",
      "Iteration  370 : train_loss =  0.6142693   train_acc:  0.9587517\n",
      "Iteration  370 : test_loss =  0.6408913   test_acc:  0.95611\n",
      "************************************************************\n",
      "Iteration  371 : train_loss =  0.63615507   train_acc:  0.954695\n",
      "Iteration  371 : test_loss =  0.66057444   test_acc:  0.9536\n",
      "************************************************************\n",
      "Iteration  372 : train_loss =  0.6137819   train_acc:  0.95878166\n",
      "Iteration  372 : test_loss =  0.64047855   test_acc:  0.95613\n",
      "************************************************************\n",
      "Iteration  373 : train_loss =  0.6355717   train_acc:  0.95473\n",
      "Iteration  373 : test_loss =  0.66006523   test_acc:  0.95366\n",
      "************************************************************\n",
      "Iteration  374 : train_loss =  0.61331105   train_acc:  0.9588017\n",
      "Iteration  374 : test_loss =  0.6400581   test_acc:  0.95616\n",
      "************************************************************\n",
      "Iteration  375 : train_loss =  0.6349348   train_acc:  0.9547667\n",
      "Iteration  375 : test_loss =  0.65949374   test_acc:  0.95367\n",
      "************************************************************\n",
      "Iteration  376 : train_loss =  0.6127724   train_acc:  0.95882666\n",
      "Iteration  376 : test_loss =  0.6395902   test_acc:  0.95616\n",
      "************************************************************\n",
      "Iteration  377 : train_loss =  0.6342935   train_acc:  0.954795\n",
      "Iteration  377 : test_loss =  0.65891415   test_acc:  0.95366\n",
      "************************************************************\n",
      "Iteration  378 : train_loss =  0.6122526   train_acc:  0.9588583\n",
      "Iteration  378 : test_loss =  0.6391198   test_acc:  0.95621\n",
      "************************************************************\n",
      "Iteration  379 : train_loss =  0.6336546   train_acc:  0.9548183\n",
      "Iteration  379 : test_loss =  0.65832597   test_acc:  0.95368\n",
      "************************************************************\n",
      "Iteration  380 : train_loss =  0.61172134   train_acc:  0.9588817\n",
      "Iteration  380 : test_loss =  0.6386492   test_acc:  0.9562\n",
      "************************************************************\n",
      "Iteration  381 : train_loss =  0.6330915   train_acc:  0.9548517\n",
      "Iteration  381 : test_loss =  0.65780324   test_acc:  0.95369\n",
      "************************************************************\n",
      "Iteration  382 : train_loss =  0.6112573   train_acc:  0.9589133\n",
      "Iteration  382 : test_loss =  0.6382276   test_acc:  0.95621\n",
      "************************************************************\n",
      "Iteration  383 : train_loss =  0.6325189   train_acc:  0.954885\n",
      "Iteration  383 : test_loss =  0.65725976   test_acc:  0.95374\n",
      "************************************************************\n",
      "Iteration  384 : train_loss =  0.6107723   train_acc:  0.95893\n",
      "Iteration  384 : test_loss =  0.6377956   test_acc:  0.95619\n",
      "************************************************************\n",
      "Iteration  385 : train_loss =  0.6319495   train_acc:  0.9549183\n",
      "Iteration  385 : test_loss =  0.6567603   test_acc:  0.95378\n",
      "************************************************************\n",
      "Iteration  386 : train_loss =  0.6103087   train_acc:  0.95895165\n",
      "Iteration  386 : test_loss =  0.63737226   test_acc:  0.95624\n",
      "************************************************************\n",
      "Iteration  387 : train_loss =  0.6313885   train_acc:  0.95495\n",
      "Iteration  387 : test_loss =  0.6562486   test_acc:  0.95379\n",
      "************************************************************\n",
      "Iteration  388 : train_loss =  0.6098449   train_acc:  0.95897335\n",
      "Iteration  388 : test_loss =  0.63695496   test_acc:  0.95628\n",
      "************************************************************\n",
      "Iteration  389 : train_loss =  0.6308298   train_acc:  0.95498335\n",
      "Iteration  389 : test_loss =  0.65574783   test_acc:  0.95378\n",
      "************************************************************\n",
      "Iteration  390 : train_loss =  0.6093837   train_acc:  0.958995\n",
      "Iteration  390 : test_loss =  0.6365665   test_acc:  0.95632\n",
      "************************************************************\n",
      "Iteration  391 : train_loss =  0.63028526   train_acc:  0.955025\n",
      "Iteration  391 : test_loss =  0.65526485   test_acc:  0.9538\n",
      "************************************************************\n",
      "Iteration  392 : train_loss =  0.6089311   train_acc:  0.95903665\n",
      "Iteration  392 : test_loss =  0.6361613   test_acc:  0.95633\n",
      "************************************************************\n",
      "Iteration  393 : train_loss =  0.6297355   train_acc:  0.9550483\n",
      "Iteration  393 : test_loss =  0.65476143   test_acc:  0.95384\n",
      "************************************************************\n",
      "Iteration  394 : train_loss =  0.6084699   train_acc:  0.95908\n",
      "Iteration  394 : test_loss =  0.63577366   test_acc:  0.95636\n",
      "************************************************************\n",
      "Iteration  395 : train_loss =  0.6291993   train_acc:  0.95507336\n",
      "Iteration  395 : test_loss =  0.6542871   test_acc:  0.95385\n",
      "************************************************************\n",
      "Iteration  396 : train_loss =  0.60801965   train_acc:  0.959095\n",
      "Iteration  396 : test_loss =  0.63536835   test_acc:  0.95642\n",
      "************************************************************\n",
      "Iteration  397 : train_loss =  0.62864816   train_acc:  0.95508665\n",
      "Iteration  397 : test_loss =  0.6537828   test_acc:  0.95393\n",
      "************************************************************\n",
      "Iteration  398 : train_loss =  0.60755247   train_acc:  0.95911664\n",
      "Iteration  398 : test_loss =  0.6349845   test_acc:  0.95641\n",
      "************************************************************\n",
      "Iteration  399 : train_loss =  0.62805337   train_acc:  0.9551167\n",
      "Iteration  399 : test_loss =  0.65326875   test_acc:  0.95394\n",
      "************************************************************\n",
      "Iteration  400 : train_loss =  0.60707504   train_acc:  0.95915\n",
      "Iteration  400 : test_loss =  0.6345457   test_acc:  0.95647\n",
      "************************************************************\n",
      "Iteration  401 : train_loss =  0.62751967   train_acc:  0.95517164\n",
      "Iteration  401 : test_loss =  0.65275717   test_acc:  0.95397\n",
      "************************************************************\n",
      "Iteration  402 : train_loss =  0.6066315   train_acc:  0.95919335\n",
      "Iteration  402 : test_loss =  0.6341626   test_acc:  0.95649\n",
      "************************************************************\n",
      "Iteration  403 : train_loss =  0.62698686   train_acc:  0.9552133\n",
      "Iteration  403 : test_loss =  0.65226305   test_acc:  0.95398\n",
      "************************************************************\n",
      "Iteration  404 : train_loss =  0.60619676   train_acc:  0.95921165\n",
      "Iteration  404 : test_loss =  0.63376784   test_acc:  0.9565\n",
      "************************************************************\n",
      "Iteration  405 : train_loss =  0.6264564   train_acc:  0.9552233\n",
      "Iteration  405 : test_loss =  0.6517799   test_acc:  0.95399\n",
      "************************************************************\n",
      "Iteration  406 : train_loss =  0.605755   train_acc:  0.95922834\n",
      "Iteration  406 : test_loss =  0.6334043   test_acc:  0.95656\n",
      "************************************************************\n",
      "Iteration  407 : train_loss =  0.62593466   train_acc:  0.95524836\n",
      "Iteration  407 : test_loss =  0.6513135   test_acc:  0.95404\n",
      "************************************************************\n",
      "Iteration  408 : train_loss =  0.60531545   train_acc:  0.95925\n",
      "Iteration  408 : test_loss =  0.63301384   test_acc:  0.95663\n",
      "************************************************************\n",
      "Iteration  409 : train_loss =  0.6253526   train_acc:  0.9552867\n",
      "Iteration  409 : test_loss =  0.65079117   test_acc:  0.95408\n",
      "************************************************************\n",
      "Iteration  410 : train_loss =  0.6049029   train_acc:  0.959305\n",
      "Iteration  410 : test_loss =  0.6326565   test_acc:  0.95659\n",
      "************************************************************\n",
      "Iteration  411 : train_loss =  0.6248399   train_acc:  0.95532835\n",
      "Iteration  411 : test_loss =  0.6503051   test_acc:  0.95413\n",
      "************************************************************\n",
      "Iteration  412 : train_loss =  0.6044776   train_acc:  0.959315\n",
      "Iteration  412 : test_loss =  0.63226837   test_acc:  0.95667\n",
      "************************************************************\n",
      "Iteration  413 : train_loss =  0.6243292   train_acc:  0.95538336\n",
      "Iteration  413 : test_loss =  0.6498294   test_acc:  0.95414\n",
      "************************************************************\n",
      "Iteration  414 : train_loss =  0.6040413   train_acc:  0.95934\n",
      "Iteration  414 : test_loss =  0.63190496   test_acc:  0.95665\n",
      "************************************************************\n",
      "Iteration  415 : train_loss =  0.6238282   train_acc:  0.9554\n",
      "Iteration  415 : test_loss =  0.649391   test_acc:  0.95411\n",
      "************************************************************\n",
      "Iteration  416 : train_loss =  0.60362595   train_acc:  0.95937\n",
      "Iteration  416 : test_loss =  0.63153225   test_acc:  0.95669\n",
      "************************************************************\n",
      "Iteration  417 : train_loss =  0.62334013   train_acc:  0.95545167\n",
      "Iteration  417 : test_loss =  0.6489399   test_acc:  0.95414\n",
      "************************************************************\n",
      "Iteration  418 : train_loss =  0.6032064   train_acc:  0.95940334\n",
      "Iteration  418 : test_loss =  0.63116515   test_acc:  0.95672\n",
      "************************************************************\n",
      "Iteration  419 : train_loss =  0.622835   train_acc:  0.9554833\n",
      "Iteration  419 : test_loss =  0.64847356   test_acc:  0.95411\n",
      "************************************************************\n",
      "Iteration  420 : train_loss =  0.60278934   train_acc:  0.95943\n",
      "Iteration  420 : test_loss =  0.630792   test_acc:  0.95674\n",
      "************************************************************\n",
      "Iteration  421 : train_loss =  0.62232906   train_acc:  0.9555167\n",
      "Iteration  421 : test_loss =  0.64800864   test_acc:  0.95411\n",
      "************************************************************\n",
      "Iteration  422 : train_loss =  0.60236746   train_acc:  0.95945334\n",
      "Iteration  422 : test_loss =  0.63042796   test_acc:  0.95675\n",
      "************************************************************\n",
      "Iteration  423 : train_loss =  0.6218396   train_acc:  0.955545\n",
      "Iteration  423 : test_loss =  0.64757043   test_acc:  0.95412\n",
      "************************************************************\n",
      "Iteration  424 : train_loss =  0.6019715   train_acc:  0.9594833\n",
      "Iteration  424 : test_loss =  0.6300728   test_acc:  0.9568\n",
      "************************************************************\n",
      "Iteration  425 : train_loss =  0.6213471   train_acc:  0.95558\n",
      "Iteration  425 : test_loss =  0.64710844   test_acc:  0.95415\n",
      "************************************************************\n",
      "Iteration  426 : train_loss =  0.60155815   train_acc:  0.959525\n",
      "Iteration  426 : test_loss =  0.6297243   test_acc:  0.95681\n",
      "************************************************************\n",
      "Iteration  427 : train_loss =  0.6208615   train_acc:  0.95562\n",
      "Iteration  427 : test_loss =  0.646679   test_acc:  0.95415\n",
      "************************************************************\n",
      "Iteration  428 : train_loss =  0.60114306   train_acc:  0.95954\n",
      "Iteration  428 : test_loss =  0.6293732   test_acc:  0.95681\n",
      "************************************************************\n",
      "Iteration  429 : train_loss =  0.6203954   train_acc:  0.95562\n",
      "Iteration  429 : test_loss =  0.6462671   test_acc:  0.95418\n",
      "************************************************************\n",
      "Iteration  430 : train_loss =  0.60076225   train_acc:  0.95955664\n",
      "Iteration  430 : test_loss =  0.629028   test_acc:  0.95685\n",
      "************************************************************\n",
      "Iteration  431 : train_loss =  0.6199038   train_acc:  0.95567834\n",
      "Iteration  431 : test_loss =  0.6458003   test_acc:  0.95422\n",
      "************************************************************\n",
      "Iteration  432 : train_loss =  0.60037416   train_acc:  0.95957667\n",
      "Iteration  432 : test_loss =  0.6286745   test_acc:  0.95684\n",
      "************************************************************\n",
      "Iteration  433 : train_loss =  0.61942726   train_acc:  0.95570165\n",
      "Iteration  433 : test_loss =  0.64534676   test_acc:  0.95426\n",
      "************************************************************\n",
      "Iteration  434 : train_loss =  0.5999642   train_acc:  0.95957\n",
      "Iteration  434 : test_loss =  0.62833977   test_acc:  0.95686\n",
      "************************************************************\n",
      "Iteration  435 : train_loss =  0.61897016   train_acc:  0.95573336\n",
      "Iteration  435 : test_loss =  0.64496505   test_acc:  0.95429\n",
      "************************************************************\n",
      "Iteration  436 : train_loss =  0.5995919   train_acc:  0.95960665\n",
      "Iteration  436 : test_loss =  0.6280123   test_acc:  0.9569\n",
      "************************************************************\n",
      "Iteration  437 : train_loss =  0.61848754   train_acc:  0.95577\n",
      "Iteration  437 : test_loss =  0.64450103   test_acc:  0.95436\n",
      "************************************************************\n",
      "Iteration  438 : train_loss =  0.599205   train_acc:  0.95963\n",
      "Iteration  438 : test_loss =  0.62765634   test_acc:  0.95696\n",
      "************************************************************\n",
      "Iteration  439 : train_loss =  0.61802036   train_acc:  0.95579\n",
      "Iteration  439 : test_loss =  0.6440661   test_acc:  0.95436\n",
      "************************************************************\n",
      "Iteration  440 : train_loss =  0.5988037   train_acc:  0.9596483\n",
      "Iteration  440 : test_loss =  0.6273337   test_acc:  0.95698\n",
      "************************************************************\n",
      "Iteration  441 : train_loss =  0.61755794   train_acc:  0.9558217\n",
      "Iteration  441 : test_loss =  0.6436656   test_acc:  0.95438\n",
      "************************************************************\n",
      "Iteration  442 : train_loss =  0.59843165   train_acc:  0.959665\n",
      "Iteration  442 : test_loss =  0.6269918   test_acc:  0.95701\n",
      "************************************************************\n",
      "Iteration  443 : train_loss =  0.6170934   train_acc:  0.95585835\n",
      "Iteration  443 : test_loss =  0.6432178   test_acc:  0.95441\n",
      "************************************************************\n",
      "Iteration  444 : train_loss =  0.5980397   train_acc:  0.95966834\n",
      "Iteration  444 : test_loss =  0.6266493   test_acc:  0.95705\n",
      "************************************************************\n",
      "Iteration  445 : train_loss =  0.6166439   train_acc:  0.9558667\n",
      "Iteration  445 : test_loss =  0.64281565   test_acc:  0.95441\n",
      "************************************************************\n",
      "Iteration  446 : train_loss =  0.59768075   train_acc:  0.95972\n",
      "Iteration  446 : test_loss =  0.6263307   test_acc:  0.95703\n",
      "************************************************************\n",
      "Iteration  447 : train_loss =  0.6161702   train_acc:  0.955915\n",
      "Iteration  447 : test_loss =  0.64236873   test_acc:  0.95448\n",
      "************************************************************\n",
      "Iteration  448 : train_loss =  0.59730196   train_acc:  0.95975333\n",
      "Iteration  448 : test_loss =  0.6260018   test_acc:  0.95702\n",
      "************************************************************\n",
      "Iteration  449 : train_loss =  0.61571795   train_acc:  0.955935\n",
      "Iteration  449 : test_loss =  0.64196616   test_acc:  0.9545\n",
      "************************************************************\n",
      "Iteration  450 : train_loss =  0.59693503   train_acc:  0.95975167\n",
      "Iteration  450 : test_loss =  0.6257121   test_acc:  0.95705\n",
      "************************************************************\n",
      "Iteration  451 : train_loss =  0.61527187   train_acc:  0.95596\n",
      "Iteration  451 : test_loss =  0.6415652   test_acc:  0.9546\n",
      "************************************************************\n",
      "Iteration  452 : train_loss =  0.5965586   train_acc:  0.959785\n",
      "Iteration  452 : test_loss =  0.62538445   test_acc:  0.95708\n",
      "************************************************************\n",
      "Iteration  453 : train_loss =  0.61486536   train_acc:  0.955985\n",
      "Iteration  453 : test_loss =  0.6411926   test_acc:  0.95461\n",
      "************************************************************\n",
      "Iteration  454 : train_loss =  0.59619415   train_acc:  0.9598017\n",
      "Iteration  454 : test_loss =  0.6250537   test_acc:  0.95711\n",
      "************************************************************\n",
      "Iteration  455 : train_loss =  0.6144116   train_acc:  0.95601165\n",
      "Iteration  455 : test_loss =  0.6407716   test_acc:  0.95464\n",
      "************************************************************\n",
      "Iteration  456 : train_loss =  0.59582317   train_acc:  0.9598183\n",
      "Iteration  456 : test_loss =  0.6247168   test_acc:  0.95715\n",
      "************************************************************\n",
      "Iteration  457 : train_loss =  0.61395746   train_acc:  0.95604\n",
      "Iteration  457 : test_loss =  0.6403583   test_acc:  0.95464\n",
      "************************************************************\n",
      "Iteration  458 : train_loss =  0.5954792   train_acc:  0.95984334\n",
      "Iteration  458 : test_loss =  0.62441003   test_acc:  0.95716\n",
      "************************************************************\n",
      "Iteration  459 : train_loss =  0.61351055   train_acc:  0.9560817\n",
      "Iteration  459 : test_loss =  0.63993114   test_acc:  0.95472\n",
      "************************************************************\n",
      "Iteration  460 : train_loss =  0.59510535   train_acc:  0.95986\n",
      "Iteration  460 : test_loss =  0.6240751   test_acc:  0.95716\n",
      "************************************************************\n",
      "Iteration  461 : train_loss =  0.61307216   train_acc:  0.9561033\n",
      "Iteration  461 : test_loss =  0.6395348   test_acc:  0.95476\n",
      "************************************************************\n",
      "Iteration  462 : train_loss =  0.59476066   train_acc:  0.95988166\n",
      "Iteration  462 : test_loss =  0.6237668   test_acc:  0.95719\n",
      "************************************************************\n",
      "Iteration  463 : train_loss =  0.61263305   train_acc:  0.95613164\n",
      "Iteration  463 : test_loss =  0.63911456   test_acc:  0.95482\n",
      "************************************************************\n",
      "Iteration  464 : train_loss =  0.59439194   train_acc:  0.9599017\n",
      "Iteration  464 : test_loss =  0.62345076   test_acc:  0.9572\n",
      "************************************************************\n",
      "Iteration  465 : train_loss =  0.61220104   train_acc:  0.9561383\n",
      "Iteration  465 : test_loss =  0.6387358   test_acc:  0.95483\n",
      "************************************************************\n",
      "Iteration  466 : train_loss =  0.59404767   train_acc:  0.95992666\n",
      "Iteration  466 : test_loss =  0.62314224   test_acc:  0.95719\n",
      "************************************************************\n",
      "Iteration  467 : train_loss =  0.61176956   train_acc:  0.956165\n",
      "Iteration  467 : test_loss =  0.63833463   test_acc:  0.95488\n",
      "************************************************************\n",
      "Iteration  468 : train_loss =  0.5936866   train_acc:  0.9599467\n",
      "Iteration  468 : test_loss =  0.62283504   test_acc:  0.95722\n",
      "************************************************************\n",
      "Iteration  469 : train_loss =  0.61133933   train_acc:  0.95619667\n",
      "Iteration  469 : test_loss =  0.63794744   test_acc:  0.9549\n",
      "************************************************************\n",
      "Iteration  470 : train_loss =  0.59334505   train_acc:  0.95997834\n",
      "Iteration  470 : test_loss =  0.62252843   test_acc:  0.9572\n",
      "************************************************************\n",
      "Iteration  471 : train_loss =  0.6109065   train_acc:  0.95623\n",
      "Iteration  471 : test_loss =  0.63754004   test_acc:  0.95489\n",
      "************************************************************\n",
      "Iteration  472 : train_loss =  0.5929859   train_acc:  0.9599917\n",
      "Iteration  472 : test_loss =  0.622221   test_acc:  0.95722\n",
      "************************************************************\n",
      "Iteration  473 : train_loss =  0.6104837   train_acc:  0.95626\n",
      "Iteration  473 : test_loss =  0.637164   test_acc:  0.95489\n",
      "************************************************************\n",
      "Iteration  474 : train_loss =  0.5926427   train_acc:  0.9600033\n",
      "Iteration  474 : test_loss =  0.6219095   test_acc:  0.95726\n",
      "************************************************************\n",
      "Iteration  475 : train_loss =  0.6100585   train_acc:  0.95628834\n",
      "Iteration  475 : test_loss =  0.63676727   test_acc:  0.95493\n",
      "************************************************************\n",
      "Iteration  476 : train_loss =  0.5922978   train_acc:  0.96003\n",
      "Iteration  476 : test_loss =  0.6215966   test_acc:  0.95731\n",
      "************************************************************\n",
      "Iteration  477 : train_loss =  0.6095851   train_acc:  0.956315\n",
      "Iteration  477 : test_loss =  0.6363292   test_acc:  0.955\n",
      "************************************************************\n",
      "Iteration  478 : train_loss =  0.5919456   train_acc:  0.96006334\n",
      "Iteration  478 : test_loss =  0.621293   test_acc:  0.9573\n",
      "************************************************************\n",
      "Iteration  479 : train_loss =  0.6091805   train_acc:  0.95632166\n",
      "Iteration  479 : test_loss =  0.6359671   test_acc:  0.95499\n",
      "************************************************************\n",
      "Iteration  480 : train_loss =  0.59160453   train_acc:  0.9600767\n",
      "Iteration  480 : test_loss =  0.62097347   test_acc:  0.95733\n",
      "************************************************************\n",
      "Iteration  481 : train_loss =  0.6087578   train_acc:  0.956355\n",
      "Iteration  481 : test_loss =  0.6355828   test_acc:  0.95499\n",
      "************************************************************\n",
      "Iteration  482 : train_loss =  0.5912738   train_acc:  0.9601\n",
      "Iteration  482 : test_loss =  0.62066907   test_acc:  0.95742\n",
      "************************************************************\n",
      "Iteration  483 : train_loss =  0.60834384   train_acc:  0.9563767\n",
      "Iteration  483 : test_loss =  0.6352084   test_acc:  0.95506\n",
      "************************************************************\n",
      "Iteration  484 : train_loss =  0.59093475   train_acc:  0.96012\n",
      "Iteration  484 : test_loss =  0.62038815   test_acc:  0.95742\n",
      "************************************************************\n",
      "Iteration  485 : train_loss =  0.6079363   train_acc:  0.95640165\n",
      "Iteration  485 : test_loss =  0.63483274   test_acc:  0.9551\n",
      "************************************************************\n",
      "Iteration  486 : train_loss =  0.59059685   train_acc:  0.96013165\n",
      "Iteration  486 : test_loss =  0.62007666   test_acc:  0.95745\n",
      "************************************************************\n",
      "Iteration  487 : train_loss =  0.60752696   train_acc:  0.95644665\n",
      "Iteration  487 : test_loss =  0.6344494   test_acc:  0.95513\n",
      "************************************************************\n",
      "Iteration  488 : train_loss =  0.59025943   train_acc:  0.9601517\n",
      "Iteration  488 : test_loss =  0.61976594   test_acc:  0.95747\n",
      "************************************************************\n",
      "Iteration  489 : train_loss =  0.6071189   train_acc:  0.95649165\n",
      "Iteration  489 : test_loss =  0.6340626   test_acc:  0.95515\n",
      "************************************************************\n",
      "Iteration  490 : train_loss =  0.5899197   train_acc:  0.96017665\n",
      "Iteration  490 : test_loss =  0.619488   test_acc:  0.9575\n",
      "************************************************************\n",
      "Iteration  491 : train_loss =  0.6067233   train_acc:  0.9565\n",
      "Iteration  491 : test_loss =  0.6337262   test_acc:  0.95515\n",
      "************************************************************\n",
      "Iteration  492 : train_loss =  0.5895933   train_acc:  0.960185\n",
      "Iteration  492 : test_loss =  0.6191921   test_acc:  0.95751\n",
      "************************************************************\n",
      "Iteration  493 : train_loss =  0.6063188   train_acc:  0.95653665\n",
      "Iteration  493 : test_loss =  0.63336134   test_acc:  0.95518\n",
      "************************************************************\n",
      "Iteration  494 : train_loss =  0.58926356   train_acc:  0.960225\n",
      "Iteration  494 : test_loss =  0.6188944   test_acc:  0.95752\n",
      "************************************************************\n",
      "Iteration  495 : train_loss =  0.6059172   train_acc:  0.95655835\n",
      "Iteration  495 : test_loss =  0.63298726   test_acc:  0.9552\n",
      "************************************************************\n",
      "Iteration  496 : train_loss =  0.588939   train_acc:  0.96023333\n",
      "Iteration  496 : test_loss =  0.6186021   test_acc:  0.95752\n",
      "************************************************************\n",
      "Iteration  497 : train_loss =  0.6055154   train_acc:  0.95659\n",
      "Iteration  497 : test_loss =  0.63261503   test_acc:  0.9552\n",
      "************************************************************\n",
      "Iteration  498 : train_loss =  0.58861417   train_acc:  0.96025336\n",
      "Iteration  498 : test_loss =  0.6183164   test_acc:  0.95757\n",
      "************************************************************\n",
      "Iteration  499 : train_loss =  0.6051177   train_acc:  0.95663\n",
      "Iteration  499 : test_loss =  0.63223445   test_acc:  0.95524\n",
      "************************************************************\n",
      "Iteration  500 : train_loss =  0.58828646   train_acc:  0.9602517\n",
      "Iteration  500 : test_loss =  0.6180258   test_acc:  0.95761\n",
      "************************************************************\n",
      "Iteration  501 : train_loss =  0.60471964   train_acc:  0.9566583\n",
      "Iteration  501 : test_loss =  0.63188064   test_acc:  0.95522\n",
      "************************************************************\n",
      "Iteration  502 : train_loss =  0.58796626   train_acc:  0.960275\n",
      "Iteration  502 : test_loss =  0.61775947   test_acc:  0.95759\n",
      "************************************************************\n",
      "Iteration  503 : train_loss =  0.60432684   train_acc:  0.9566867\n",
      "Iteration  503 : test_loss =  0.6315343   test_acc:  0.95524\n",
      "************************************************************\n",
      "Iteration  504 : train_loss =  0.5876423   train_acc:  0.96029836\n",
      "Iteration  504 : test_loss =  0.6174746   test_acc:  0.95761\n",
      "************************************************************\n",
      "Iteration  505 : train_loss =  0.6039342   train_acc:  0.95668834\n",
      "Iteration  505 : test_loss =  0.63118243   test_acc:  0.95525\n",
      "************************************************************\n",
      "Iteration  506 : train_loss =  0.5873263   train_acc:  0.96031165\n",
      "Iteration  506 : test_loss =  0.6171865   test_acc:  0.9576\n",
      "************************************************************\n",
      "Iteration  507 : train_loss =  0.6035408   train_acc:  0.956705\n",
      "Iteration  507 : test_loss =  0.6308213   test_acc:  0.95526\n",
      "************************************************************\n",
      "Iteration  508 : train_loss =  0.58701456   train_acc:  0.96033\n",
      "Iteration  508 : test_loss =  0.61692053   test_acc:  0.95761\n",
      "************************************************************\n",
      "Iteration  509 : train_loss =  0.60315233   train_acc:  0.95673835\n",
      "Iteration  509 : test_loss =  0.6304547   test_acc:  0.95527\n",
      "************************************************************\n",
      "Iteration  510 : train_loss =  0.5866976   train_acc:  0.9603317\n",
      "Iteration  510 : test_loss =  0.61665666   test_acc:  0.95767\n",
      "************************************************************\n",
      "Iteration  511 : train_loss =  0.60276955   train_acc:  0.9567533\n",
      "Iteration  511 : test_loss =  0.6301106   test_acc:  0.95531\n",
      "************************************************************\n",
      "Iteration  512 : train_loss =  0.58638465   train_acc:  0.9603483\n",
      "Iteration  512 : test_loss =  0.61638635   test_acc:  0.95771\n",
      "************************************************************\n",
      "Iteration  513 : train_loss =  0.6023836   train_acc:  0.9567817\n",
      "Iteration  513 : test_loss =  0.6297584   test_acc:  0.95532\n",
      "************************************************************\n",
      "Iteration  514 : train_loss =  0.58607537   train_acc:  0.960365\n",
      "Iteration  514 : test_loss =  0.6161071   test_acc:  0.95779\n",
      "************************************************************\n",
      "Iteration  515 : train_loss =  0.6019992   train_acc:  0.956815\n",
      "Iteration  515 : test_loss =  0.6293959   test_acc:  0.95533\n",
      "************************************************************\n",
      "Iteration  516 : train_loss =  0.5857635   train_acc:  0.96038836\n",
      "Iteration  516 : test_loss =  0.6158451   test_acc:  0.9578\n",
      "************************************************************\n",
      "Iteration  517 : train_loss =  0.60162145   train_acc:  0.956825\n",
      "Iteration  517 : test_loss =  0.629058   test_acc:  0.95534\n",
      "************************************************************\n",
      "Iteration  518 : train_loss =  0.5854597   train_acc:  0.96041167\n",
      "Iteration  518 : test_loss =  0.6155697   test_acc:  0.9578\n",
      "************************************************************\n",
      "Iteration  519 : train_loss =  0.6012388   train_acc:  0.95685834\n",
      "Iteration  519 : test_loss =  0.628708   test_acc:  0.95532\n",
      "************************************************************\n",
      "Iteration  520 : train_loss =  0.5851555   train_acc:  0.96041834\n",
      "Iteration  520 : test_loss =  0.6153031   test_acc:  0.95781\n",
      "************************************************************\n",
      "Iteration  521 : train_loss =  0.6008602   train_acc:  0.95685834\n",
      "Iteration  521 : test_loss =  0.62838227   test_acc:  0.95536\n",
      "************************************************************\n",
      "Iteration  522 : train_loss =  0.5848565   train_acc:  0.9604317\n",
      "Iteration  522 : test_loss =  0.61504227   test_acc:  0.95782\n",
      "************************************************************\n",
      "Iteration  523 : train_loss =  0.6004803   train_acc:  0.9568833\n",
      "Iteration  523 : test_loss =  0.6280254   test_acc:  0.95539\n",
      "************************************************************\n",
      "Iteration  524 : train_loss =  0.5845552   train_acc:  0.96045333\n",
      "Iteration  524 : test_loss =  0.61478096   test_acc:  0.95785\n",
      "************************************************************\n",
      "Iteration  525 : train_loss =  0.6001023   train_acc:  0.956895\n",
      "Iteration  525 : test_loss =  0.6276831   test_acc:  0.9554\n",
      "************************************************************\n",
      "Iteration  526 : train_loss =  0.58426005   train_acc:  0.960475\n",
      "Iteration  526 : test_loss =  0.61453027   test_acc:  0.95785\n",
      "************************************************************\n",
      "Iteration  527 : train_loss =  0.5997286   train_acc:  0.95692\n",
      "Iteration  527 : test_loss =  0.6273319   test_acc:  0.95541\n",
      "************************************************************\n",
      "Iteration  528 : train_loss =  0.58396614   train_acc:  0.96048\n",
      "Iteration  528 : test_loss =  0.6142947   test_acc:  0.95788\n",
      "************************************************************\n",
      "Iteration  529 : train_loss =  0.5993608   train_acc:  0.956945\n",
      "Iteration  529 : test_loss =  0.62700075   test_acc:  0.95547\n",
      "************************************************************\n",
      "Iteration  530 : train_loss =  0.5836716   train_acc:  0.960505\n",
      "Iteration  530 : test_loss =  0.6140323   test_acc:  0.95788\n",
      "************************************************************\n",
      "Iteration  531 : train_loss =  0.59899247   train_acc:  0.95697665\n",
      "Iteration  531 : test_loss =  0.6266565   test_acc:  0.95545\n",
      "************************************************************\n",
      "Iteration  532 : train_loss =  0.58337593   train_acc:  0.96051335\n",
      "Iteration  532 : test_loss =  0.61378807   test_acc:  0.95787\n",
      "************************************************************\n",
      "Iteration  533 : train_loss =  0.5986276   train_acc:  0.95699835\n",
      "Iteration  533 : test_loss =  0.62632275   test_acc:  0.95546\n",
      "************************************************************\n",
      "Iteration  534 : train_loss =  0.58308065   train_acc:  0.960535\n",
      "Iteration  534 : test_loss =  0.61353296   test_acc:  0.9579\n",
      "************************************************************\n",
      "Iteration  535 : train_loss =  0.59826314   train_acc:  0.9570233\n",
      "Iteration  535 : test_loss =  0.62598234   test_acc:  0.9555\n",
      "************************************************************\n",
      "Iteration  536 : train_loss =  0.5827854   train_acc:  0.96055\n",
      "Iteration  536 : test_loss =  0.6132595   test_acc:  0.9579\n",
      "************************************************************\n",
      "Iteration  537 : train_loss =  0.59790015   train_acc:  0.95702666\n",
      "Iteration  537 : test_loss =  0.6256644   test_acc:  0.95551\n",
      "************************************************************\n",
      "Iteration  538 : train_loss =  0.5824985   train_acc:  0.96057165\n",
      "Iteration  538 : test_loss =  0.6130016   test_acc:  0.95795\n",
      "************************************************************\n",
      "Iteration  539 : train_loss =  0.5975352   train_acc:  0.957065\n",
      "Iteration  539 : test_loss =  0.62532824   test_acc:  0.95556\n",
      "************************************************************\n",
      "Iteration  540 : train_loss =  0.58221143   train_acc:  0.9605733\n",
      "Iteration  540 : test_loss =  0.6127605   test_acc:  0.95796\n",
      "************************************************************\n",
      "Iteration  541 : train_loss =  0.5971788   train_acc:  0.9570683\n",
      "Iteration  541 : test_loss =  0.6250123   test_acc:  0.95559\n",
      "************************************************************\n",
      "Iteration  542 : train_loss =  0.5819242   train_acc:  0.9606017\n",
      "Iteration  542 : test_loss =  0.6125073   test_acc:  0.95798\n",
      "************************************************************\n",
      "Iteration  543 : train_loss =  0.59681284   train_acc:  0.9570817\n",
      "Iteration  543 : test_loss =  0.62467676   test_acc:  0.95561\n",
      "************************************************************\n",
      "Iteration  544 : train_loss =  0.5816371   train_acc:  0.9606183\n",
      "Iteration  544 : test_loss =  0.6122548   test_acc:  0.95801\n",
      "************************************************************\n",
      "Iteration  545 : train_loss =  0.5964516   train_acc:  0.9571033\n",
      "Iteration  545 : test_loss =  0.6243545   test_acc:  0.95562\n",
      "************************************************************\n",
      "Iteration  546 : train_loss =  0.58135515   train_acc:  0.9606517\n",
      "Iteration  546 : test_loss =  0.61201113   test_acc:  0.95803\n",
      "************************************************************\n",
      "Iteration  547 : train_loss =  0.59608936   train_acc:  0.9571267\n",
      "Iteration  547 : test_loss =  0.62401074   test_acc:  0.95567\n",
      "************************************************************\n",
      "Iteration  548 : train_loss =  0.5810685   train_acc:  0.96066\n",
      "Iteration  548 : test_loss =  0.611761   test_acc:  0.95803\n",
      "************************************************************\n",
      "Iteration  549 : train_loss =  0.59572893   train_acc:  0.95714664\n",
      "Iteration  549 : test_loss =  0.6236848   test_acc:  0.95567\n",
      "************************************************************\n",
      "Iteration  550 : train_loss =  0.5807894   train_acc:  0.96067\n",
      "Iteration  550 : test_loss =  0.61152893   test_acc:  0.95802\n",
      "************************************************************\n",
      "Iteration  551 : train_loss =  0.5953802   train_acc:  0.9571667\n",
      "Iteration  551 : test_loss =  0.62334913   test_acc:  0.9557\n",
      "************************************************************\n",
      "Iteration  552 : train_loss =  0.5805047   train_acc:  0.96067\n",
      "Iteration  552 : test_loss =  0.61128396   test_acc:  0.95803\n",
      "************************************************************\n",
      "Iteration  553 : train_loss =  0.5950289   train_acc:  0.9572117\n",
      "Iteration  553 : test_loss =  0.62301767   test_acc:  0.95569\n",
      "************************************************************\n",
      "Iteration  554 : train_loss =  0.58022016   train_acc:  0.96067333\n",
      "Iteration  554 : test_loss =  0.611034   test_acc:  0.95804\n",
      "************************************************************\n",
      "Iteration  555 : train_loss =  0.5946783   train_acc:  0.957225\n",
      "Iteration  555 : test_loss =  0.62269807   test_acc:  0.95571\n",
      "************************************************************\n",
      "Iteration  556 : train_loss =  0.5799384   train_acc:  0.96068335\n",
      "Iteration  556 : test_loss =  0.6107886   test_acc:  0.95803\n",
      "************************************************************\n",
      "Iteration  557 : train_loss =  0.5943305   train_acc:  0.9572433\n",
      "Iteration  557 : test_loss =  0.62238103   test_acc:  0.9557\n",
      "************************************************************\n",
      "Iteration  558 : train_loss =  0.579656   train_acc:  0.960705\n",
      "Iteration  558 : test_loss =  0.61054236   test_acc:  0.95806\n",
      "************************************************************\n",
      "Iteration  559 : train_loss =  0.59398395   train_acc:  0.9572667\n",
      "Iteration  559 : test_loss =  0.62206024   test_acc:  0.95573\n",
      "************************************************************\n",
      "Iteration  560 : train_loss =  0.5793748   train_acc:  0.96071666\n",
      "Iteration  560 : test_loss =  0.61030275   test_acc:  0.95806\n",
      "************************************************************\n",
      "Iteration  561 : train_loss =  0.5936401   train_acc:  0.9572883\n",
      "Iteration  561 : test_loss =  0.62175167   test_acc:  0.95571\n",
      "************************************************************\n",
      "Iteration  562 : train_loss =  0.5790964   train_acc:  0.9607317\n",
      "Iteration  562 : test_loss =  0.6100581   test_acc:  0.95809\n",
      "************************************************************\n",
      "Iteration  563 : train_loss =  0.59329444   train_acc:  0.9573117\n",
      "Iteration  563 : test_loss =  0.62143123   test_acc:  0.95575\n",
      "************************************************************\n",
      "Iteration  564 : train_loss =  0.5788189   train_acc:  0.9607483\n",
      "Iteration  564 : test_loss =  0.6098208   test_acc:  0.95812\n",
      "************************************************************\n",
      "Iteration  565 : train_loss =  0.59295106   train_acc:  0.9573333\n",
      "Iteration  565 : test_loss =  0.62111956   test_acc:  0.95577\n",
      "************************************************************\n",
      "Iteration  566 : train_loss =  0.5785432   train_acc:  0.960775\n",
      "Iteration  566 : test_loss =  0.6095816   test_acc:  0.95814\n",
      "************************************************************\n",
      "Iteration  567 : train_loss =  0.59260964   train_acc:  0.95736\n",
      "Iteration  567 : test_loss =  0.62081003   test_acc:  0.95578\n",
      "************************************************************\n",
      "Iteration  568 : train_loss =  0.57827294   train_acc:  0.9607817\n",
      "Iteration  568 : test_loss =  0.6093416   test_acc:  0.95814\n",
      "************************************************************\n",
      "Iteration  569 : train_loss =  0.5922865   train_acc:  0.95735\n",
      "Iteration  569 : test_loss =  0.62053055   test_acc:  0.95577\n",
      "************************************************************\n",
      "Iteration  570 : train_loss =  0.5780056   train_acc:  0.960795\n",
      "Iteration  570 : test_loss =  0.60910136   test_acc:  0.95818\n",
      "************************************************************\n",
      "Iteration  571 : train_loss =  0.5919463   train_acc:  0.9573733\n",
      "Iteration  571 : test_loss =  0.6202272   test_acc:  0.9558\n",
      "************************************************************\n",
      "Iteration  572 : train_loss =  0.5777452   train_acc:  0.96080834\n",
      "Iteration  572 : test_loss =  0.60887665   test_acc:  0.9582\n",
      "************************************************************\n",
      "Iteration  573 : train_loss =  0.59160835   train_acc:  0.95741\n",
      "Iteration  573 : test_loss =  0.619911   test_acc:  0.95582\n",
      "************************************************************\n",
      "Iteration  574 : train_loss =  0.5774785   train_acc:  0.96082\n",
      "Iteration  574 : test_loss =  0.6086422   test_acc:  0.95822\n",
      "************************************************************\n",
      "Iteration  575 : train_loss =  0.59127176   train_acc:  0.95742667\n",
      "Iteration  575 : test_loss =  0.61960626   test_acc:  0.95584\n",
      "************************************************************\n",
      "Iteration  576 : train_loss =  0.577217   train_acc:  0.96084166\n",
      "Iteration  576 : test_loss =  0.60841596   test_acc:  0.95825\n",
      "************************************************************\n",
      "Iteration  577 : train_loss =  0.59093606   train_acc:  0.9574467\n",
      "Iteration  577 : test_loss =  0.61929464   test_acc:  0.95582\n",
      "************************************************************\n",
      "Iteration  578 : train_loss =  0.57695746   train_acc:  0.96086836\n",
      "Iteration  578 : test_loss =  0.6081879   test_acc:  0.95825\n",
      "************************************************************\n",
      "Iteration  579 : train_loss =  0.5906021   train_acc:  0.9574817\n",
      "Iteration  579 : test_loss =  0.6189909   test_acc:  0.95585\n",
      "************************************************************\n",
      "Iteration  580 : train_loss =  0.5766954   train_acc:  0.96087664\n",
      "Iteration  580 : test_loss =  0.60796064   test_acc:  0.95824\n",
      "************************************************************\n",
      "Iteration  581 : train_loss =  0.59026873   train_acc:  0.95749664\n",
      "Iteration  581 : test_loss =  0.61868054   test_acc:  0.95587\n",
      "************************************************************\n",
      "Iteration  582 : train_loss =  0.5764375   train_acc:  0.960905\n",
      "Iteration  582 : test_loss =  0.6077364   test_acc:  0.95825\n",
      "************************************************************\n",
      "Iteration  583 : train_loss =  0.5899368   train_acc:  0.95751166\n",
      "Iteration  583 : test_loss =  0.6183822   test_acc:  0.95589\n",
      "************************************************************\n",
      "Iteration  584 : train_loss =  0.5761766   train_acc:  0.96093166\n",
      "Iteration  584 : test_loss =  0.6075071   test_acc:  0.95826\n",
      "************************************************************\n",
      "Iteration  585 : train_loss =  0.58960706   train_acc:  0.9575383\n",
      "Iteration  585 : test_loss =  0.61808205   test_acc:  0.9559\n",
      "************************************************************\n",
      "Iteration  586 : train_loss =  0.57592046   train_acc:  0.96094\n",
      "Iteration  586 : test_loss =  0.60728604   test_acc:  0.95828\n",
      "************************************************************\n",
      "Iteration  587 : train_loss =  0.5892766   train_acc:  0.957555\n",
      "Iteration  587 : test_loss =  0.61778116   test_acc:  0.95593\n",
      "************************************************************\n",
      "Iteration  588 : train_loss =  0.5756642   train_acc:  0.960955\n",
      "Iteration  588 : test_loss =  0.6070628   test_acc:  0.95833\n",
      "************************************************************\n",
      "Iteration  589 : train_loss =  0.58894944   train_acc:  0.95757836\n",
      "Iteration  589 : test_loss =  0.61748654   test_acc:  0.95595\n",
      "************************************************************\n",
      "Iteration  590 : train_loss =  0.57540655   train_acc:  0.96097165\n",
      "Iteration  590 : test_loss =  0.6068463   test_acc:  0.95833\n",
      "************************************************************\n",
      "Iteration  591 : train_loss =  0.58861893   train_acc:  0.957615\n",
      "Iteration  591 : test_loss =  0.61717206   test_acc:  0.95598\n",
      "************************************************************\n",
      "Iteration  592 : train_loss =  0.5751537   train_acc:  0.96099\n",
      "Iteration  592 : test_loss =  0.60663235   test_acc:  0.95834\n",
      "************************************************************\n",
      "Iteration  593 : train_loss =  0.5882921   train_acc:  0.957655\n",
      "Iteration  593 : test_loss =  0.6168652   test_acc:  0.95599\n",
      "************************************************************\n",
      "Iteration  594 : train_loss =  0.5748918   train_acc:  0.961005\n",
      "Iteration  594 : test_loss =  0.606402   test_acc:  0.95832\n",
      "************************************************************\n",
      "Iteration  595 : train_loss =  0.58796525   train_acc:  0.957665\n",
      "Iteration  595 : test_loss =  0.6165751   test_acc:  0.95604\n",
      "************************************************************\n",
      "Iteration  596 : train_loss =  0.574633   train_acc:  0.96102333\n",
      "Iteration  596 : test_loss =  0.60617644   test_acc:  0.95833\n",
      "************************************************************\n",
      "Iteration  597 : train_loss =  0.5876401   train_acc:  0.9577017\n",
      "Iteration  597 : test_loss =  0.6162841   test_acc:  0.95607\n",
      "************************************************************\n",
      "Iteration  598 : train_loss =  0.5743748   train_acc:  0.96103334\n",
      "Iteration  598 : test_loss =  0.60595304   test_acc:  0.95835\n",
      "************************************************************\n",
      "Iteration  599 : train_loss =  0.58731526   train_acc:  0.9577283\n",
      "Iteration  599 : test_loss =  0.6159876   test_acc:  0.95609\n",
      "************************************************************\n",
      "Iteration  600 : train_loss =  0.57411903   train_acc:  0.96104\n",
      "Iteration  600 : test_loss =  0.60572976   test_acc:  0.95838\n",
      "************************************************************\n",
      "Iteration  601 : train_loss =  0.58699185   train_acc:  0.9577467\n",
      "Iteration  601 : test_loss =  0.6156943   test_acc:  0.95611\n",
      "************************************************************\n",
      "Iteration  602 : train_loss =  0.57386583   train_acc:  0.9610467\n",
      "Iteration  602 : test_loss =  0.60551065   test_acc:  0.95838\n",
      "************************************************************\n",
      "Iteration  603 : train_loss =  0.5866707   train_acc:  0.957765\n",
      "Iteration  603 : test_loss =  0.61539763   test_acc:  0.95611\n",
      "************************************************************\n",
      "Iteration  604 : train_loss =  0.57361233   train_acc:  0.9610633\n",
      "Iteration  604 : test_loss =  0.605287   test_acc:  0.95839\n",
      "************************************************************\n",
      "Iteration  605 : train_loss =  0.58634776   train_acc:  0.9577733\n",
      "Iteration  605 : test_loss =  0.6150981   test_acc:  0.95612\n",
      "************************************************************\n",
      "Iteration  606 : train_loss =  0.5733591   train_acc:  0.9610817\n",
      "Iteration  606 : test_loss =  0.6050682   test_acc:  0.95842\n",
      "************************************************************\n",
      "Iteration  607 : train_loss =  0.58602756   train_acc:  0.95778835\n",
      "Iteration  607 : test_loss =  0.6148083   test_acc:  0.95611\n",
      "************************************************************\n",
      "Iteration  608 : train_loss =  0.5731088   train_acc:  0.9610933\n",
      "Iteration  608 : test_loss =  0.60485   test_acc:  0.95846\n",
      "************************************************************\n",
      "Iteration  609 : train_loss =  0.5857077   train_acc:  0.95780164\n",
      "Iteration  609 : test_loss =  0.6145167   test_acc:  0.95613\n",
      "************************************************************\n",
      "Iteration  610 : train_loss =  0.5728591   train_acc:  0.9611083\n",
      "Iteration  610 : test_loss =  0.60463953   test_acc:  0.95849\n",
      "************************************************************\n",
      "Iteration  611 : train_loss =  0.58539116   train_acc:  0.95782\n",
      "Iteration  611 : test_loss =  0.6142259   test_acc:  0.95617\n",
      "************************************************************\n",
      "Iteration  612 : train_loss =  0.57261   train_acc:  0.961125\n",
      "Iteration  612 : test_loss =  0.6044217   test_acc:  0.9585\n",
      "************************************************************\n",
      "Iteration  613 : train_loss =  0.58507484   train_acc:  0.95783\n",
      "Iteration  613 : test_loss =  0.6139391   test_acc:  0.9562\n",
      "************************************************************\n",
      "Iteration  614 : train_loss =  0.57236755   train_acc:  0.96114165\n",
      "Iteration  614 : test_loss =  0.6042042   test_acc:  0.95851\n",
      "************************************************************\n",
      "Iteration  615 : train_loss =  0.5847643   train_acc:  0.957825\n",
      "Iteration  615 : test_loss =  0.6136761   test_acc:  0.95619\n",
      "************************************************************\n",
      "Iteration  616 : train_loss =  0.57212746   train_acc:  0.9611433\n",
      "Iteration  616 : test_loss =  0.6039932   test_acc:  0.95856\n",
      "************************************************************\n",
      "Iteration  617 : train_loss =  0.5844502   train_acc:  0.9578483\n",
      "Iteration  617 : test_loss =  0.613394   test_acc:  0.95624\n",
      "************************************************************\n",
      "Iteration  618 : train_loss =  0.57188606   train_acc:  0.96115166\n",
      "Iteration  618 : test_loss =  0.60378665   test_acc:  0.95857\n",
      "************************************************************\n",
      "Iteration  619 : train_loss =  0.5841391   train_acc:  0.9578717\n",
      "Iteration  619 : test_loss =  0.6131093   test_acc:  0.95626\n",
      "************************************************************\n",
      "Iteration  620 : train_loss =  0.57164544   train_acc:  0.96115667\n",
      "Iteration  620 : test_loss =  0.6035752   test_acc:  0.95858\n",
      "************************************************************\n",
      "Iteration  621 : train_loss =  0.5838284   train_acc:  0.9578883\n",
      "Iteration  621 : test_loss =  0.6128304   test_acc:  0.95627\n",
      "************************************************************\n",
      "Iteration  622 : train_loss =  0.57140666   train_acc:  0.96117336\n",
      "Iteration  622 : test_loss =  0.60336727   test_acc:  0.95856\n",
      "************************************************************\n",
      "Iteration  623 : train_loss =  0.5835189   train_acc:  0.95791\n",
      "Iteration  623 : test_loss =  0.6125501   test_acc:  0.95629\n",
      "************************************************************\n",
      "Iteration  624 : train_loss =  0.5711675   train_acc:  0.9611933\n",
      "Iteration  624 : test_loss =  0.60316384   test_acc:  0.95856\n",
      "************************************************************\n",
      "Iteration  625 : train_loss =  0.58321214   train_acc:  0.95792335\n",
      "Iteration  625 : test_loss =  0.6122692   test_acc:  0.95632\n",
      "************************************************************\n",
      "Iteration  626 : train_loss =  0.570928   train_acc:  0.96119666\n",
      "Iteration  626 : test_loss =  0.60295635   test_acc:  0.95858\n",
      "************************************************************\n",
      "Iteration  627 : train_loss =  0.5829034   train_acc:  0.9579483\n",
      "Iteration  627 : test_loss =  0.6119841   test_acc:  0.95634\n",
      "************************************************************\n",
      "Iteration  628 : train_loss =  0.57068944   train_acc:  0.9612117\n",
      "Iteration  628 : test_loss =  0.6027574   test_acc:  0.95857\n",
      "************************************************************\n",
      "Iteration  629 : train_loss =  0.5826012   train_acc:  0.95798665\n",
      "Iteration  629 : test_loss =  0.61169815   test_acc:  0.95637\n",
      "************************************************************\n",
      "Iteration  630 : train_loss =  0.5704501   train_acc:  0.9612283\n",
      "Iteration  630 : test_loss =  0.60255253   test_acc:  0.95858\n",
      "************************************************************\n",
      "Iteration  631 : train_loss =  0.5822931   train_acc:  0.95803666\n",
      "Iteration  631 : test_loss =  0.61140376   test_acc:  0.9564\n",
      "************************************************************\n",
      "Iteration  632 : train_loss =  0.5702065   train_acc:  0.96123\n",
      "Iteration  632 : test_loss =  0.6023395   test_acc:  0.9586\n",
      "************************************************************\n",
      "Iteration  633 : train_loss =  0.5819885   train_acc:  0.9580467\n",
      "Iteration  633 : test_loss =  0.61113644   test_acc:  0.95637\n",
      "************************************************************\n",
      "Iteration  634 : train_loss =  0.56996435   train_acc:  0.96125335\n",
      "Iteration  634 : test_loss =  0.6021331   test_acc:  0.95861\n",
      "************************************************************\n",
      "Iteration  635 : train_loss =  0.5816846   train_acc:  0.95807666\n",
      "Iteration  635 : test_loss =  0.61086565   test_acc:  0.95639\n",
      "************************************************************\n",
      "Iteration  636 : train_loss =  0.56972307   train_acc:  0.961275\n",
      "Iteration  636 : test_loss =  0.6019248   test_acc:  0.95861\n",
      "************************************************************\n",
      "Iteration  637 : train_loss =  0.5813823   train_acc:  0.95809\n",
      "Iteration  637 : test_loss =  0.6105913   test_acc:  0.95644\n",
      "************************************************************\n",
      "Iteration  638 : train_loss =  0.5694808   train_acc:  0.96129\n",
      "Iteration  638 : test_loss =  0.6017123   test_acc:  0.95861\n",
      "************************************************************\n",
      "Iteration  639 : train_loss =  0.58108205   train_acc:  0.9581083\n",
      "Iteration  639 : test_loss =  0.6103242   test_acc:  0.95645\n",
      "************************************************************\n",
      "Iteration  640 : train_loss =  0.5692385   train_acc:  0.96132\n",
      "Iteration  640 : test_loss =  0.60150164   test_acc:  0.95863\n",
      "************************************************************\n",
      "Iteration  641 : train_loss =  0.58078176   train_acc:  0.95812833\n",
      "Iteration  641 : test_loss =  0.61005247   test_acc:  0.95646\n",
      "************************************************************\n",
      "Iteration  642 : train_loss =  0.5689986   train_acc:  0.96133167\n",
      "Iteration  642 : test_loss =  0.601287   test_acc:  0.95863\n",
      "************************************************************\n",
      "Iteration  643 : train_loss =  0.5804841   train_acc:  0.9581417\n",
      "Iteration  643 : test_loss =  0.6097867   test_acc:  0.95648\n",
      "************************************************************\n",
      "Iteration  644 : train_loss =  0.5686767   train_acc:  0.9613683\n",
      "Iteration  644 : test_loss =  0.6010063   test_acc:  0.95864\n",
      "************************************************************\n",
      "Iteration  645 : train_loss =  0.5801863   train_acc:  0.95814335\n",
      "Iteration  645 : test_loss =  0.60952735   test_acc:  0.95649\n",
      "************************************************************\n",
      "Iteration  646 : train_loss =  0.5684428   train_acc:  0.96138835\n",
      "Iteration  646 : test_loss =  0.60080343   test_acc:  0.95866\n",
      "************************************************************\n",
      "Iteration  647 : train_loss =  0.57988983   train_acc:  0.95816165\n",
      "Iteration  647 : test_loss =  0.60925245   test_acc:  0.95653\n",
      "************************************************************\n",
      "Iteration  648 : train_loss =  0.5682085   train_acc:  0.9614033\n",
      "Iteration  648 : test_loss =  0.60059834   test_acc:  0.95872\n",
      "************************************************************\n",
      "Iteration  649 : train_loss =  0.57959557   train_acc:  0.9581767\n",
      "Iteration  649 : test_loss =  0.6089892   test_acc:  0.95656\n",
      "************************************************************\n",
      "Iteration  650 : train_loss =  0.5679767   train_acc:  0.9614033\n",
      "Iteration  650 : test_loss =  0.6003981   test_acc:  0.95871\n",
      "************************************************************\n",
      "Iteration  651 : train_loss =  0.57930124   train_acc:  0.95819336\n",
      "Iteration  651 : test_loss =  0.60872126   test_acc:  0.9566\n",
      "************************************************************\n",
      "Iteration  652 : train_loss =  0.56774503   train_acc:  0.961425\n",
      "Iteration  652 : test_loss =  0.60019714   test_acc:  0.95872\n",
      "************************************************************\n",
      "Iteration  653 : train_loss =  0.5790069   train_acc:  0.9581983\n",
      "Iteration  653 : test_loss =  0.6084576   test_acc:  0.95664\n",
      "************************************************************\n",
      "Iteration  654 : train_loss =  0.56751347   train_acc:  0.96143335\n",
      "Iteration  654 : test_loss =  0.5999981   test_acc:  0.95874\n",
      "************************************************************\n",
      "Iteration  655 : train_loss =  0.57871485   train_acc:  0.958205\n",
      "Iteration  655 : test_loss =  0.60818624   test_acc:  0.95665\n",
      "************************************************************\n",
      "Iteration  656 : train_loss =  0.5672827   train_acc:  0.96144664\n",
      "Iteration  656 : test_loss =  0.5997943   test_acc:  0.95874\n",
      "************************************************************\n",
      "Iteration  657 : train_loss =  0.57842255   train_acc:  0.9582267\n",
      "Iteration  657 : test_loss =  0.60792625   test_acc:  0.95665\n",
      "************************************************************\n",
      "Iteration  658 : train_loss =  0.567052   train_acc:  0.961465\n",
      "Iteration  658 : test_loss =  0.5995928   test_acc:  0.95872\n",
      "************************************************************\n",
      "Iteration  659 : train_loss =  0.57813156   train_acc:  0.958235\n",
      "Iteration  659 : test_loss =  0.60765564   test_acc:  0.95667\n",
      "************************************************************\n",
      "Iteration  660 : train_loss =  0.5668233   train_acc:  0.96147\n",
      "Iteration  660 : test_loss =  0.5993867   test_acc:  0.95873\n",
      "************************************************************\n",
      "Iteration  661 : train_loss =  0.5778399   train_acc:  0.95824665\n",
      "Iteration  661 : test_loss =  0.6073907   test_acc:  0.95668\n",
      "************************************************************\n",
      "Iteration  662 : train_loss =  0.5665963   train_acc:  0.961495\n",
      "Iteration  662 : test_loss =  0.59918225   test_acc:  0.95876\n",
      "************************************************************\n",
      "Iteration  663 : train_loss =  0.5775546   train_acc:  0.958245\n",
      "Iteration  663 : test_loss =  0.6071502   test_acc:  0.95669\n",
      "************************************************************\n",
      "Iteration  664 : train_loss =  0.5663754   train_acc:  0.9615267\n",
      "Iteration  664 : test_loss =  0.5989872   test_acc:  0.95879\n",
      "************************************************************\n",
      "Iteration  665 : train_loss =  0.5772685   train_acc:  0.95824164\n",
      "Iteration  665 : test_loss =  0.6068994   test_acc:  0.95668\n",
      "************************************************************\n",
      "Iteration  666 : train_loss =  0.5661564   train_acc:  0.96153\n",
      "Iteration  666 : test_loss =  0.59879816   test_acc:  0.9588\n",
      "************************************************************\n",
      "Iteration  667 : train_loss =  0.5769827   train_acc:  0.9582667\n",
      "Iteration  667 : test_loss =  0.6066338   test_acc:  0.9567\n",
      "************************************************************\n",
      "Iteration  668 : train_loss =  0.5659359   train_acc:  0.96155167\n",
      "Iteration  668 : test_loss =  0.5986065   test_acc:  0.95881\n",
      "************************************************************\n",
      "Iteration  669 : train_loss =  0.57669914   train_acc:  0.95829165\n",
      "Iteration  669 : test_loss =  0.60637605   test_acc:  0.95671\n",
      "************************************************************\n",
      "Iteration  670 : train_loss =  0.5657169   train_acc:  0.9615717\n",
      "Iteration  670 : test_loss =  0.59841454   test_acc:  0.95885\n",
      "************************************************************\n",
      "Iteration  671 : train_loss =  0.5764155   train_acc:  0.9583117\n",
      "Iteration  671 : test_loss =  0.60611665   test_acc:  0.95672\n",
      "************************************************************\n",
      "Iteration  672 : train_loss =  0.56549627   train_acc:  0.96156836\n",
      "Iteration  672 : test_loss =  0.598226   test_acc:  0.95889\n",
      "************************************************************\n",
      "Iteration  673 : train_loss =  0.57613   train_acc:  0.95832336\n",
      "Iteration  673 : test_loss =  0.60585546   test_acc:  0.95673\n",
      "************************************************************\n",
      "Iteration  674 : train_loss =  0.5652756   train_acc:  0.96158165\n",
      "Iteration  674 : test_loss =  0.5980396   test_acc:  0.9589\n",
      "************************************************************\n",
      "Iteration  675 : train_loss =  0.5758462   train_acc:  0.95837\n",
      "Iteration  675 : test_loss =  0.60557693   test_acc:  0.95676\n",
      "************************************************************\n",
      "Iteration  676 : train_loss =  0.5650557   train_acc:  0.9615883\n",
      "Iteration  676 : test_loss =  0.5978542   test_acc:  0.95888\n",
      "************************************************************\n",
      "Iteration  677 : train_loss =  0.5755628   train_acc:  0.95841664\n",
      "Iteration  677 : test_loss =  0.6053025   test_acc:  0.95684\n",
      "************************************************************\n",
      "Iteration  678 : train_loss =  0.5648228   train_acc:  0.96160835\n",
      "Iteration  678 : test_loss =  0.5976504   test_acc:  0.95887\n",
      "************************************************************\n",
      "Iteration  679 : train_loss =  0.5752781   train_acc:  0.9584583\n",
      "Iteration  679 : test_loss =  0.6050402   test_acc:  0.95689\n",
      "************************************************************\n",
      "Iteration  680 : train_loss =  0.56458914   train_acc:  0.96163\n",
      "Iteration  680 : test_loss =  0.5974479   test_acc:  0.9589\n",
      "************************************************************\n",
      "Iteration  681 : train_loss =  0.5749945   train_acc:  0.95847\n",
      "Iteration  681 : test_loss =  0.60478127   test_acc:  0.95695\n",
      "************************************************************\n",
      "Iteration  682 : train_loss =  0.56428045   train_acc:  0.96163833\n",
      "Iteration  682 : test_loss =  0.5971824   test_acc:  0.9589\n",
      "************************************************************\n",
      "Iteration  683 : train_loss =  0.57471067   train_acc:  0.958465\n",
      "Iteration  683 : test_loss =  0.604533   test_acc:  0.95696\n",
      "************************************************************\n",
      "Iteration  684 : train_loss =  0.56405586   train_acc:  0.96165836\n",
      "Iteration  684 : test_loss =  0.5969878   test_acc:  0.95893\n",
      "************************************************************\n",
      "Iteration  685 : train_loss =  0.5744302   train_acc:  0.95847666\n",
      "Iteration  685 : test_loss =  0.6042762   test_acc:  0.95697\n",
      "************************************************************\n",
      "Iteration  686 : train_loss =  0.5638316   train_acc:  0.961675\n",
      "Iteration  686 : test_loss =  0.5967899   test_acc:  0.95895\n",
      "************************************************************\n",
      "Iteration  687 : train_loss =  0.5741504   train_acc:  0.95849836\n",
      "Iteration  687 : test_loss =  0.604025   test_acc:  0.95697\n",
      "************************************************************\n",
      "Iteration  688 : train_loss =  0.5636069   train_acc:  0.96168\n",
      "Iteration  688 : test_loss =  0.59659207   test_acc:  0.95898\n",
      "************************************************************\n",
      "Iteration  689 : train_loss =  0.57387096   train_acc:  0.95851165\n",
      "Iteration  689 : test_loss =  0.6037786   test_acc:  0.95698\n",
      "************************************************************\n",
      "Iteration  690 : train_loss =  0.56338555   train_acc:  0.9617017\n",
      "Iteration  690 : test_loss =  0.5964025   test_acc:  0.95899\n",
      "************************************************************\n",
      "Iteration  691 : train_loss =  0.5735931   train_acc:  0.95853335\n",
      "Iteration  691 : test_loss =  0.60353494   test_acc:  0.95699\n",
      "************************************************************\n",
      "Iteration  692 : train_loss =  0.5631637   train_acc:  0.961725\n",
      "Iteration  692 : test_loss =  0.5962097   test_acc:  0.95902\n",
      "************************************************************\n",
      "Iteration  693 : train_loss =  0.57331693   train_acc:  0.9585417\n",
      "Iteration  693 : test_loss =  0.60329145   test_acc:  0.957\n",
      "************************************************************\n",
      "Iteration  694 : train_loss =  0.56294316   train_acc:  0.96173334\n",
      "Iteration  694 : test_loss =  0.59602165   test_acc:  0.95905\n",
      "************************************************************\n",
      "Iteration  695 : train_loss =  0.57304156   train_acc:  0.958555\n",
      "Iteration  695 : test_loss =  0.6030483   test_acc:  0.95703\n",
      "************************************************************\n",
      "Iteration  696 : train_loss =  0.56272376   train_acc:  0.96175665\n",
      "Iteration  696 : test_loss =  0.59583366   test_acc:  0.95906\n",
      "************************************************************\n",
      "Iteration  697 : train_loss =  0.5727667   train_acc:  0.9585767\n",
      "Iteration  697 : test_loss =  0.60280603   test_acc:  0.95705\n",
      "************************************************************\n",
      "Iteration  698 : train_loss =  0.5625038   train_acc:  0.9617633\n",
      "Iteration  698 : test_loss =  0.5956458   test_acc:  0.95909\n",
      "************************************************************\n",
      "Iteration  699 : train_loss =  0.5724948   train_acc:  0.958585\n",
      "Iteration  699 : test_loss =  0.6025584   test_acc:  0.95705\n",
      "************************************************************\n",
      "Iteration  700 : train_loss =  0.5622853   train_acc:  0.9617767\n",
      "Iteration  700 : test_loss =  0.5954579   test_acc:  0.95911\n",
      "************************************************************\n",
      "Iteration  701 : train_loss =  0.5722232   train_acc:  0.95861334\n",
      "Iteration  701 : test_loss =  0.60232097   test_acc:  0.95706\n",
      "************************************************************\n",
      "Iteration  702 : train_loss =  0.5620654   train_acc:  0.96179\n",
      "Iteration  702 : test_loss =  0.59526867   test_acc:  0.95912\n",
      "************************************************************\n",
      "Iteration  703 : train_loss =  0.5719521   train_acc:  0.95862\n",
      "Iteration  703 : test_loss =  0.6020809   test_acc:  0.95708\n",
      "************************************************************\n",
      "Iteration  704 : train_loss =  0.5618471   train_acc:  0.96180665\n",
      "Iteration  704 : test_loss =  0.5950807   test_acc:  0.95912\n",
      "************************************************************\n",
      "Iteration  705 : train_loss =  0.57168114   train_acc:  0.9586433\n",
      "Iteration  705 : test_loss =  0.601837   test_acc:  0.95712\n",
      "************************************************************\n",
      "Iteration  706 : train_loss =  0.5615686   train_acc:  0.96182835\n",
      "Iteration  706 : test_loss =  0.5948396   test_acc:  0.95912\n",
      "************************************************************\n",
      "Iteration  707 : train_loss =  0.5714113   train_acc:  0.95866334\n",
      "Iteration  707 : test_loss =  0.60160255   test_acc:  0.95712\n",
      "************************************************************\n",
      "Iteration  708 : train_loss =  0.5613547   train_acc:  0.961835\n",
      "Iteration  708 : test_loss =  0.59465325   test_acc:  0.95914\n",
      "************************************************************\n",
      "Iteration  709 : train_loss =  0.5711423   train_acc:  0.9586767\n",
      "Iteration  709 : test_loss =  0.60136   test_acc:  0.95715\n",
      "************************************************************\n",
      "Iteration  710 : train_loss =  0.56114054   train_acc:  0.96184164\n",
      "Iteration  710 : test_loss =  0.5944672   test_acc:  0.95914\n",
      "************************************************************\n",
      "Iteration  711 : train_loss =  0.57087135   train_acc:  0.958695\n",
      "Iteration  711 : test_loss =  0.60112774   test_acc:  0.95718\n",
      "************************************************************\n",
      "Iteration  712 : train_loss =  0.5609284   train_acc:  0.96186334\n",
      "Iteration  712 : test_loss =  0.5942899   test_acc:  0.95914\n",
      "************************************************************\n",
      "Iteration  713 : train_loss =  0.57060105   train_acc:  0.958725\n",
      "Iteration  713 : test_loss =  0.6008837   test_acc:  0.95721\n",
      "************************************************************\n",
      "Iteration  714 : train_loss =  0.5607156   train_acc:  0.9618883\n",
      "Iteration  714 : test_loss =  0.59410733   test_acc:  0.95918\n",
      "************************************************************\n",
      "Iteration  715 : train_loss =  0.57032853   train_acc:  0.95875335\n",
      "Iteration  715 : test_loss =  0.60065067   test_acc:  0.95721\n",
      "************************************************************\n",
      "Iteration  716 : train_loss =  0.5605038   train_acc:  0.9618933\n",
      "Iteration  716 : test_loss =  0.5939322   test_acc:  0.95916\n",
      "************************************************************\n",
      "Iteration  717 : train_loss =  0.5700572   train_acc:  0.95877165\n",
      "Iteration  717 : test_loss =  0.6004146   test_acc:  0.95723\n",
      "************************************************************\n",
      "Iteration  718 : train_loss =  0.5602893   train_acc:  0.9619067\n",
      "Iteration  718 : test_loss =  0.5937518   test_acc:  0.9592\n",
      "************************************************************\n",
      "Iteration  719 : train_loss =  0.5697931   train_acc:  0.958795\n",
      "Iteration  719 : test_loss =  0.6002056   test_acc:  0.95725\n",
      "************************************************************\n",
      "Iteration  720 : train_loss =  0.56006753   train_acc:  0.961925\n",
      "Iteration  720 : test_loss =  0.59356946   test_acc:  0.9592\n",
      "************************************************************\n",
      "Iteration  721 : train_loss =  0.56953734   train_acc:  0.9588\n",
      "Iteration  721 : test_loss =  0.5999881   test_acc:  0.95727\n",
      "************************************************************\n",
      "Iteration  722 : train_loss =  0.5598596   train_acc:  0.961945\n",
      "Iteration  722 : test_loss =  0.5933864   test_acc:  0.95922\n",
      "************************************************************\n",
      "Iteration  723 : train_loss =  0.56928176   train_acc:  0.958835\n",
      "Iteration  723 : test_loss =  0.59976244   test_acc:  0.95728\n",
      "************************************************************\n",
      "Iteration  724 : train_loss =  0.5595736   train_acc:  0.96196336\n",
      "Iteration  724 : test_loss =  0.59313804   test_acc:  0.95922\n",
      "************************************************************\n",
      "Iteration  725 : train_loss =  0.5690272   train_acc:  0.95884335\n",
      "Iteration  725 : test_loss =  0.5995412   test_acc:  0.95731\n",
      "************************************************************\n",
      "Iteration  726 : train_loss =  0.5593711   train_acc:  0.96198165\n",
      "Iteration  726 : test_loss =  0.59296286   test_acc:  0.95924\n",
      "************************************************************\n",
      "Iteration  727 : train_loss =  0.56877446   train_acc:  0.9588533\n",
      "Iteration  727 : test_loss =  0.5993142   test_acc:  0.9573\n",
      "************************************************************\n",
      "Iteration  728 : train_loss =  0.55916977   train_acc:  0.9620017\n",
      "Iteration  728 : test_loss =  0.59278536   test_acc:  0.95924\n",
      "************************************************************\n",
      "Iteration  729 : train_loss =  0.5685223   train_acc:  0.95887667\n",
      "Iteration  729 : test_loss =  0.5990905   test_acc:  0.95735\n",
      "************************************************************\n",
      "Iteration  730 : train_loss =  0.55896884   train_acc:  0.96200335\n",
      "Iteration  730 : test_loss =  0.5926149   test_acc:  0.95925\n",
      "************************************************************\n",
      "Iteration  731 : train_loss =  0.56827277   train_acc:  0.9588983\n",
      "Iteration  731 : test_loss =  0.5988676   test_acc:  0.95735\n",
      "************************************************************\n",
      "Iteration  732 : train_loss =  0.55876994   train_acc:  0.9620117\n",
      "Iteration  732 : test_loss =  0.5924411   test_acc:  0.95926\n",
      "************************************************************\n",
      "Iteration  733 : train_loss =  0.56802356   train_acc:  0.95890164\n",
      "Iteration  733 : test_loss =  0.59864837   test_acc:  0.95738\n",
      "************************************************************\n",
      "Iteration  734 : train_loss =  0.5585726   train_acc:  0.9620283\n",
      "Iteration  734 : test_loss =  0.59227276   test_acc:  0.95928\n",
      "************************************************************\n",
      "Iteration  735 : train_loss =  0.56777835   train_acc:  0.95891666\n",
      "Iteration  735 : test_loss =  0.59842867   test_acc:  0.9574\n",
      "************************************************************\n",
      "Iteration  736 : train_loss =  0.55837506   train_acc:  0.96202666\n",
      "Iteration  736 : test_loss =  0.59209955   test_acc:  0.9593\n",
      "************************************************************\n",
      "Iteration  737 : train_loss =  0.5675342   train_acc:  0.95894\n",
      "Iteration  737 : test_loss =  0.5982091   test_acc:  0.95741\n",
      "************************************************************\n",
      "Iteration  738 : train_loss =  0.5581802   train_acc:  0.96204\n",
      "Iteration  738 : test_loss =  0.5919327   test_acc:  0.95931\n",
      "************************************************************\n",
      "Iteration  739 : train_loss =  0.5672899   train_acc:  0.95895666\n",
      "Iteration  739 : test_loss =  0.59799314   test_acc:  0.95743\n",
      "************************************************************\n",
      "Iteration  740 : train_loss =  0.5579863   train_acc:  0.96204334\n",
      "Iteration  740 : test_loss =  0.5917686   test_acc:  0.95933\n",
      "************************************************************\n",
      "Iteration  741 : train_loss =  0.5670474   train_acc:  0.95896834\n",
      "Iteration  741 : test_loss =  0.597777   test_acc:  0.95742\n",
      "************************************************************\n",
      "Iteration  742 : train_loss =  0.55779284   train_acc:  0.962055\n",
      "Iteration  742 : test_loss =  0.5915994   test_acc:  0.95936\n",
      "************************************************************\n",
      "Iteration  743 : train_loss =  0.56680506   train_acc:  0.958985\n",
      "Iteration  743 : test_loss =  0.59756124   test_acc:  0.95744\n",
      "************************************************************\n",
      "Iteration  744 : train_loss =  0.55760014   train_acc:  0.96208334\n",
      "Iteration  744 : test_loss =  0.5914357   test_acc:  0.95939\n",
      "************************************************************\n",
      "Iteration  745 : train_loss =  0.5665638   train_acc:  0.95900667\n",
      "Iteration  745 : test_loss =  0.5973476   test_acc:  0.95744\n",
      "************************************************************\n",
      "Iteration  746 : train_loss =  0.5574077   train_acc:  0.96210164\n",
      "Iteration  746 : test_loss =  0.59126747   test_acc:  0.9594\n",
      "************************************************************\n",
      "Iteration  747 : train_loss =  0.5663236   train_acc:  0.959025\n",
      "Iteration  747 : test_loss =  0.5971303   test_acc:  0.95744\n",
      "************************************************************\n",
      "Iteration  748 : train_loss =  0.5572138   train_acc:  0.9621083\n",
      "Iteration  748 : test_loss =  0.59109837   test_acc:  0.95941\n",
      "************************************************************\n",
      "Iteration  749 : train_loss =  0.5660843   train_acc:  0.9590283\n",
      "Iteration  749 : test_loss =  0.59691787   test_acc:  0.95748\n",
      "************************************************************\n",
      "Iteration  750 : train_loss =  0.5569463   train_acc:  0.96212167\n",
      "Iteration  750 : test_loss =  0.59086525   test_acc:  0.95945\n",
      "************************************************************\n",
      "Iteration  751 : train_loss =  0.56584513   train_acc:  0.95904166\n",
      "Iteration  751 : test_loss =  0.59671414   test_acc:  0.95748\n",
      "************************************************************\n",
      "Iteration  752 : train_loss =  0.5567593   train_acc:  0.9621317\n",
      "Iteration  752 : test_loss =  0.5907043   test_acc:  0.9595\n",
      "************************************************************\n",
      "Iteration  753 : train_loss =  0.5656081   train_acc:  0.95906\n",
      "Iteration  753 : test_loss =  0.5964987   test_acc:  0.95753\n",
      "************************************************************\n",
      "Iteration  754 : train_loss =  0.55657274   train_acc:  0.9621483\n",
      "Iteration  754 : test_loss =  0.590536   test_acc:  0.9595\n",
      "************************************************************\n",
      "Iteration  755 : train_loss =  0.5653719   train_acc:  0.95906335\n",
      "Iteration  755 : test_loss =  0.59628713   test_acc:  0.95755\n",
      "************************************************************\n",
      "Iteration  756 : train_loss =  0.5563866   train_acc:  0.962165\n",
      "Iteration  756 : test_loss =  0.59037447   test_acc:  0.9595\n",
      "************************************************************\n",
      "Iteration  757 : train_loss =  0.5651367   train_acc:  0.95909333\n",
      "Iteration  757 : test_loss =  0.59607273   test_acc:  0.95756\n",
      "************************************************************\n",
      "Iteration  758 : train_loss =  0.55620146   train_acc:  0.96216834\n",
      "Iteration  758 : test_loss =  0.59021074   test_acc:  0.95952\n",
      "************************************************************\n",
      "Iteration  759 : train_loss =  0.5649012   train_acc:  0.9591017\n",
      "Iteration  759 : test_loss =  0.5958607   test_acc:  0.9576\n",
      "************************************************************\n",
      "Iteration  760 : train_loss =  0.5560156   train_acc:  0.962175\n",
      "Iteration  760 : test_loss =  0.59004766   test_acc:  0.95953\n",
      "************************************************************\n",
      "Iteration  761 : train_loss =  0.5646667   train_acc:  0.9591183\n",
      "Iteration  761 : test_loss =  0.5956511   test_acc:  0.95761\n",
      "************************************************************\n",
      "Iteration  762 : train_loss =  0.5558319   train_acc:  0.9621933\n",
      "Iteration  762 : test_loss =  0.5898896   test_acc:  0.95954\n",
      "************************************************************\n",
      "Iteration  763 : train_loss =  0.56443316   train_acc:  0.9591333\n",
      "Iteration  763 : test_loss =  0.5954377   test_acc:  0.95764\n",
      "************************************************************\n",
      "Iteration  764 : train_loss =  0.55565006   train_acc:  0.9621933\n",
      "Iteration  764 : test_loss =  0.5897283   test_acc:  0.95953\n",
      "************************************************************\n",
      "Iteration  765 : train_loss =  0.5642003   train_acc:  0.95914334\n",
      "Iteration  765 : test_loss =  0.59523094   test_acc:  0.95764\n",
      "************************************************************\n",
      "Iteration  766 : train_loss =  0.555466   train_acc:  0.962205\n",
      "Iteration  766 : test_loss =  0.589573   test_acc:  0.95958\n",
      "************************************************************\n",
      "Iteration  767 : train_loss =  0.5639685   train_acc:  0.9591567\n",
      "Iteration  767 : test_loss =  0.595021   test_acc:  0.95764\n",
      "************************************************************\n",
      "Iteration  768 : train_loss =  0.5552852   train_acc:  0.9622267\n",
      "Iteration  768 : test_loss =  0.58941156   test_acc:  0.95958\n",
      "************************************************************\n",
      "Iteration  769 : train_loss =  0.56373715   train_acc:  0.9591733\n",
      "Iteration  769 : test_loss =  0.59481573   test_acc:  0.95766\n",
      "************************************************************\n",
      "Iteration  770 : train_loss =  0.555094   train_acc:  0.9622333\n",
      "Iteration  770 : test_loss =  0.58924323   test_acc:  0.95964\n",
      "************************************************************\n",
      "Iteration  771 : train_loss =  0.56350636   train_acc:  0.95918167\n",
      "Iteration  771 : test_loss =  0.5946103   test_acc:  0.95768\n",
      "************************************************************\n",
      "Iteration  772 : train_loss =  0.5549138   train_acc:  0.96224\n",
      "Iteration  772 : test_loss =  0.58908594   test_acc:  0.95965\n",
      "************************************************************\n",
      "Iteration  773 : train_loss =  0.5632762   train_acc:  0.9592017\n",
      "Iteration  773 : test_loss =  0.59440213   test_acc:  0.95768\n",
      "************************************************************\n",
      "Iteration  774 : train_loss =  0.5547318   train_acc:  0.96225\n",
      "Iteration  774 : test_loss =  0.5889262   test_acc:  0.95967\n",
      "************************************************************\n",
      "Iteration  775 : train_loss =  0.5630473   train_acc:  0.95922667\n",
      "Iteration  775 : test_loss =  0.59419405   test_acc:  0.95773\n",
      "************************************************************\n",
      "Iteration  776 : train_loss =  0.55454826   train_acc:  0.96226\n",
      "Iteration  776 : test_loss =  0.58876556   test_acc:  0.95968\n",
      "************************************************************\n",
      "Iteration  777 : train_loss =  0.5628189   train_acc:  0.959235\n",
      "Iteration  777 : test_loss =  0.5939872   test_acc:  0.95776\n",
      "************************************************************\n",
      "Iteration  778 : train_loss =  0.55436915   train_acc:  0.96226835\n",
      "Iteration  778 : test_loss =  0.58860886   test_acc:  0.9597\n",
      "************************************************************\n",
      "Iteration  779 : train_loss =  0.56259036   train_acc:  0.9592533\n",
      "Iteration  779 : test_loss =  0.5937832   test_acc:  0.95778\n",
      "************************************************************\n",
      "Iteration  780 : train_loss =  0.5541126   train_acc:  0.96228665\n",
      "Iteration  780 : test_loss =  0.58838564   test_acc:  0.95976\n",
      "************************************************************\n",
      "Iteration  781 : train_loss =  0.5623613   train_acc:  0.95925665\n",
      "Iteration  781 : test_loss =  0.59358245   test_acc:  0.95779\n",
      "************************************************************\n",
      "Iteration  782 : train_loss =  0.55393845   train_acc:  0.9622933\n",
      "Iteration  782 : test_loss =  0.58823156   test_acc:  0.95978\n",
      "************************************************************\n",
      "Iteration  783 : train_loss =  0.5621349   train_acc:  0.95927835\n",
      "Iteration  783 : test_loss =  0.59337723   test_acc:  0.95783\n",
      "************************************************************\n",
      "Iteration  784 : train_loss =  0.5537651   train_acc:  0.9623067\n",
      "Iteration  784 : test_loss =  0.58808017   test_acc:  0.95975\n",
      "************************************************************\n",
      "Iteration  785 : train_loss =  0.56190896   train_acc:  0.959295\n",
      "Iteration  785 : test_loss =  0.593174   test_acc:  0.95783\n",
      "************************************************************\n",
      "Iteration  786 : train_loss =  0.5535929   train_acc:  0.96232\n",
      "Iteration  786 : test_loss =  0.5879289   test_acc:  0.95973\n",
      "************************************************************\n",
      "Iteration  787 : train_loss =  0.56168437   train_acc:  0.95931\n",
      "Iteration  787 : test_loss =  0.5929719   test_acc:  0.95782\n",
      "************************************************************\n",
      "Iteration  788 : train_loss =  0.55342054   train_acc:  0.96232665\n",
      "Iteration  788 : test_loss =  0.5877779   test_acc:  0.95972\n",
      "************************************************************\n",
      "Iteration  789 : train_loss =  0.56146044   train_acc:  0.95932335\n",
      "Iteration  789 : test_loss =  0.5927705   test_acc:  0.9578\n",
      "************************************************************\n",
      "Iteration  790 : train_loss =  0.55325013   train_acc:  0.96233666\n",
      "Iteration  790 : test_loss =  0.58763164   test_acc:  0.95972\n",
      "************************************************************\n",
      "Iteration  791 : train_loss =  0.56123793   train_acc:  0.959345\n",
      "Iteration  791 : test_loss =  0.59257257   test_acc:  0.95781\n",
      "************************************************************\n",
      "Iteration  792 : train_loss =  0.5530794   train_acc:  0.96234334\n",
      "Iteration  792 : test_loss =  0.58748376   test_acc:  0.95975\n",
      "************************************************************\n",
      "Iteration  793 : train_loss =  0.5610154   train_acc:  0.9593767\n",
      "Iteration  793 : test_loss =  0.5923727   test_acc:  0.95781\n",
      "************************************************************\n",
      "Iteration  794 : train_loss =  0.5529099   train_acc:  0.96234834\n",
      "Iteration  794 : test_loss =  0.587339   test_acc:  0.95973\n",
      "************************************************************\n",
      "Iteration  795 : train_loss =  0.5607943   train_acc:  0.95939666\n",
      "Iteration  795 : test_loss =  0.5921708   test_acc:  0.95784\n",
      "************************************************************\n",
      "Iteration  796 : train_loss =  0.55274063   train_acc:  0.9623467\n",
      "Iteration  796 : test_loss =  0.5871892   test_acc:  0.95973\n",
      "************************************************************\n",
      "Iteration  797 : train_loss =  0.5605741   train_acc:  0.95942\n",
      "Iteration  797 : test_loss =  0.5919749   test_acc:  0.95789\n",
      "************************************************************\n",
      "Iteration  798 : train_loss =  0.55257183   train_acc:  0.96235836\n",
      "Iteration  798 : test_loss =  0.58704513   test_acc:  0.95975\n",
      "************************************************************\n",
      "Iteration  799 : train_loss =  0.56035477   train_acc:  0.9594333\n",
      "Iteration  799 : test_loss =  0.5917764   test_acc:  0.95791\n",
      "************************************************************\n",
      "Iteration  800 : train_loss =  0.552403   train_acc:  0.962365\n",
      "Iteration  800 : test_loss =  0.5868961   test_acc:  0.95976\n",
      "************************************************************\n",
      "Iteration  801 : train_loss =  0.5601354   train_acc:  0.9594617\n",
      "Iteration  801 : test_loss =  0.5915815   test_acc:  0.95791\n",
      "************************************************************\n",
      "Iteration  802 : train_loss =  0.55222774   train_acc:  0.962375\n",
      "Iteration  802 : test_loss =  0.5867473   test_acc:  0.95976\n",
      "************************************************************\n",
      "Iteration  803 : train_loss =  0.55991834   train_acc:  0.95949\n",
      "Iteration  803 : test_loss =  0.5913857   test_acc:  0.95792\n",
      "************************************************************\n",
      "Iteration  804 : train_loss =  0.5520608   train_acc:  0.962375\n",
      "Iteration  804 : test_loss =  0.58660156   test_acc:  0.95976\n",
      "************************************************************\n",
      "Iteration  805 : train_loss =  0.5597018   train_acc:  0.95952\n",
      "Iteration  805 : test_loss =  0.5911949   test_acc:  0.95795\n",
      "************************************************************\n",
      "Iteration  806 : train_loss =  0.5518926   train_acc:  0.9623967\n",
      "Iteration  806 : test_loss =  0.58645964   test_acc:  0.95973\n",
      "************************************************************\n",
      "Iteration  807 : train_loss =  0.5594868   train_acc:  0.95953\n",
      "Iteration  807 : test_loss =  0.5909996   test_acc:  0.95797\n",
      "************************************************************\n",
      "Iteration  808 : train_loss =  0.55168164   train_acc:  0.962395\n",
      "Iteration  808 : test_loss =  0.5862759   test_acc:  0.95974\n",
      "************************************************************\n",
      "Iteration  809 : train_loss =  0.5592711   train_acc:  0.95954\n",
      "Iteration  809 : test_loss =  0.5908151   test_acc:  0.95799\n",
      "************************************************************\n",
      "Iteration  810 : train_loss =  0.5515187   train_acc:  0.96240336\n",
      "Iteration  810 : test_loss =  0.5861365   test_acc:  0.95975\n",
      "************************************************************\n",
      "Iteration  811 : train_loss =  0.55905664   train_acc:  0.959555\n",
      "Iteration  811 : test_loss =  0.59062034   test_acc:  0.95801\n",
      "************************************************************\n",
      "Iteration  812 : train_loss =  0.5513547   train_acc:  0.962405\n",
      "Iteration  812 : test_loss =  0.585993   test_acc:  0.95977\n",
      "************************************************************\n",
      "Iteration  813 : train_loss =  0.55884147   train_acc:  0.9595733\n",
      "Iteration  813 : test_loss =  0.59042907   test_acc:  0.95801\n",
      "************************************************************\n",
      "Iteration  814 : train_loss =  0.5511919   train_acc:  0.96241665\n",
      "Iteration  814 : test_loss =  0.58585393   test_acc:  0.9598\n",
      "************************************************************\n",
      "Iteration  815 : train_loss =  0.5586271   train_acc:  0.9595867\n",
      "Iteration  815 : test_loss =  0.5902344   test_acc:  0.95801\n",
      "************************************************************\n",
      "Iteration  816 : train_loss =  0.5510311   train_acc:  0.9624233\n",
      "Iteration  816 : test_loss =  0.58571404   test_acc:  0.9598\n",
      "************************************************************\n",
      "Iteration  817 : train_loss =  0.5584134   train_acc:  0.95961165\n",
      "Iteration  817 : test_loss =  0.59004533   test_acc:  0.95801\n",
      "************************************************************\n",
      "Iteration  818 : train_loss =  0.5508669   train_acc:  0.96243\n",
      "Iteration  818 : test_loss =  0.58557826   test_acc:  0.95981\n",
      "************************************************************\n",
      "Iteration  819 : train_loss =  0.55820143   train_acc:  0.95962\n",
      "Iteration  819 : test_loss =  0.58985674   test_acc:  0.95801\n",
      "************************************************************\n",
      "Iteration  820 : train_loss =  0.55070597   train_acc:  0.962435\n",
      "Iteration  820 : test_loss =  0.58543855   test_acc:  0.95979\n",
      "************************************************************\n",
      "Iteration  821 : train_loss =  0.55798995   train_acc:  0.9596367\n",
      "Iteration  821 : test_loss =  0.5896643   test_acc:  0.95805\n",
      "************************************************************\n",
      "Iteration  822 : train_loss =  0.55054516   train_acc:  0.9624533\n",
      "Iteration  822 : test_loss =  0.5852981   test_acc:  0.9598\n",
      "************************************************************\n",
      "Iteration  823 : train_loss =  0.5577799   train_acc:  0.95966667\n",
      "Iteration  823 : test_loss =  0.58947533   test_acc:  0.95806\n",
      "************************************************************\n",
      "Iteration  824 : train_loss =  0.55031383   train_acc:  0.9624633\n",
      "Iteration  824 : test_loss =  0.5850997   test_acc:  0.95982\n",
      "************************************************************\n",
      "Iteration  825 : train_loss =  0.5575695   train_acc:  0.95966834\n",
      "Iteration  825 : test_loss =  0.58929217   test_acc:  0.95807\n",
      "************************************************************\n",
      "Iteration  826 : train_loss =  0.55015725   train_acc:  0.96246666\n",
      "Iteration  826 : test_loss =  0.5849627   test_acc:  0.95983\n",
      "************************************************************\n",
      "Iteration  827 : train_loss =  0.55736065   train_acc:  0.959675\n",
      "Iteration  827 : test_loss =  0.5891038   test_acc:  0.9581\n",
      "************************************************************\n",
      "Iteration  828 : train_loss =  0.5500001   train_acc:  0.96247166\n",
      "Iteration  828 : test_loss =  0.584825   test_acc:  0.95984\n",
      "************************************************************\n",
      "Iteration  829 : train_loss =  0.557152   train_acc:  0.9596933\n",
      "Iteration  829 : test_loss =  0.5889131   test_acc:  0.95811\n",
      "************************************************************\n",
      "Iteration  830 : train_loss =  0.5498437   train_acc:  0.962475\n",
      "Iteration  830 : test_loss =  0.58468777   test_acc:  0.95986\n",
      "************************************************************\n",
      "Iteration  831 : train_loss =  0.55694443   train_acc:  0.9597117\n",
      "Iteration  831 : test_loss =  0.58873   test_acc:  0.95814\n",
      "************************************************************\n",
      "Iteration  832 : train_loss =  0.5496883   train_acc:  0.9624817\n",
      "Iteration  832 : test_loss =  0.584557   test_acc:  0.95986\n",
      "************************************************************\n",
      "Iteration  833 : train_loss =  0.5567368   train_acc:  0.95972335\n",
      "Iteration  833 : test_loss =  0.5885427   test_acc:  0.95815\n",
      "************************************************************\n",
      "Iteration  834 : train_loss =  0.5495336   train_acc:  0.962495\n",
      "Iteration  834 : test_loss =  0.5844221   test_acc:  0.95986\n",
      "************************************************************\n",
      "Iteration  835 : train_loss =  0.55653167   train_acc:  0.95974165\n",
      "Iteration  835 : test_loss =  0.5883607   test_acc:  0.95819\n",
      "************************************************************\n",
      "Iteration  836 : train_loss =  0.54937875   train_acc:  0.96249664\n",
      "Iteration  836 : test_loss =  0.58429414   test_acc:  0.95986\n",
      "************************************************************\n",
      "Iteration  837 : train_loss =  0.55632585   train_acc:  0.95975\n",
      "Iteration  837 : test_loss =  0.5881744   test_acc:  0.95821\n",
      "************************************************************\n",
      "Iteration  838 : train_loss =  0.5492259   train_acc:  0.96250165\n",
      "Iteration  838 : test_loss =  0.58415735   test_acc:  0.95987\n",
      "************************************************************\n",
      "Iteration  839 : train_loss =  0.55612224   train_acc:  0.95975834\n",
      "Iteration  839 : test_loss =  0.5879934   test_acc:  0.95822\n",
      "************************************************************\n",
      "Iteration  840 : train_loss =  0.5490717   train_acc:  0.96252\n",
      "Iteration  840 : test_loss =  0.5840254   test_acc:  0.95988\n",
      "************************************************************\n",
      "Iteration  841 : train_loss =  0.5559176   train_acc:  0.9597933\n",
      "Iteration  841 : test_loss =  0.587806   test_acc:  0.95823\n",
      "************************************************************\n",
      "Iteration  842 : train_loss =  0.5489181   train_acc:  0.9625367\n",
      "Iteration  842 : test_loss =  0.58389276   test_acc:  0.95989\n",
      "************************************************************\n",
      "Iteration  843 : train_loss =  0.5557153   train_acc:  0.95980835\n",
      "Iteration  843 : test_loss =  0.5876234   test_acc:  0.95824\n",
      "************************************************************\n",
      "Iteration  844 : train_loss =  0.5487645   train_acc:  0.9625433\n",
      "Iteration  844 : test_loss =  0.58375674   test_acc:  0.9599\n",
      "************************************************************\n",
      "Iteration  845 : train_loss =  0.5555119   train_acc:  0.95982\n",
      "Iteration  845 : test_loss =  0.58744675   test_acc:  0.95823\n",
      "************************************************************\n",
      "Iteration  846 : train_loss =  0.5486067   train_acc:  0.962545\n",
      "Iteration  846 : test_loss =  0.58362615   test_acc:  0.9599\n",
      "************************************************************\n",
      "Iteration  847 : train_loss =  0.55531037   train_acc:  0.95983666\n",
      "Iteration  847 : test_loss =  0.587263   test_acc:  0.95825\n",
      "************************************************************\n",
      "Iteration  848 : train_loss =  0.5484542   train_acc:  0.962555\n",
      "Iteration  848 : test_loss =  0.5834892   test_acc:  0.95989\n",
      "************************************************************\n",
      "Iteration  849 : train_loss =  0.55510926   train_acc:  0.9598617\n",
      "Iteration  849 : test_loss =  0.58708364   test_acc:  0.95827\n",
      "************************************************************\n",
      "Iteration  850 : train_loss =  0.5483013   train_acc:  0.962565\n",
      "Iteration  850 : test_loss =  0.58335906   test_acc:  0.95987\n",
      "************************************************************\n",
      "Iteration  851 : train_loss =  0.55490834   train_acc:  0.95987165\n",
      "Iteration  851 : test_loss =  0.58690494   test_acc:  0.95829\n",
      "************************************************************\n",
      "Iteration  852 : train_loss =  0.5481487   train_acc:  0.96257335\n",
      "Iteration  852 : test_loss =  0.58322793   test_acc:  0.9599\n",
      "************************************************************\n",
      "Iteration  853 : train_loss =  0.5547085   train_acc:  0.95989\n",
      "Iteration  853 : test_loss =  0.5867272   test_acc:  0.95831\n",
      "************************************************************\n",
      "Iteration  854 : train_loss =  0.5479931   train_acc:  0.9625883\n",
      "Iteration  854 : test_loss =  0.58309567   test_acc:  0.95989\n",
      "************************************************************\n",
      "Iteration  855 : train_loss =  0.55450946   train_acc:  0.95989835\n",
      "Iteration  855 : test_loss =  0.58654726   test_acc:  0.95834\n",
      "************************************************************\n",
      "Iteration  856 : train_loss =  0.5478422   train_acc:  0.96261\n",
      "Iteration  856 : test_loss =  0.5829619   test_acc:  0.9599\n",
      "************************************************************\n",
      "Iteration  857 : train_loss =  0.5543108   train_acc:  0.95990664\n",
      "Iteration  857 : test_loss =  0.58636916   test_acc:  0.95833\n",
      "************************************************************\n",
      "Iteration  858 : train_loss =  0.5476909   train_acc:  0.9626167\n",
      "Iteration  858 : test_loss =  0.58283055   test_acc:  0.9599\n",
      "************************************************************\n",
      "Iteration  859 : train_loss =  0.55411285   train_acc:  0.9599133\n",
      "Iteration  859 : test_loss =  0.5861928   test_acc:  0.95838\n",
      "************************************************************\n",
      "Iteration  860 : train_loss =  0.5475353   train_acc:  0.9626267\n",
      "Iteration  860 : test_loss =  0.5826968   test_acc:  0.95992\n",
      "************************************************************\n",
      "Iteration  861 : train_loss =  0.5539155   train_acc:  0.959935\n",
      "Iteration  861 : test_loss =  0.58601165   test_acc:  0.9584\n",
      "************************************************************\n",
      "Iteration  862 : train_loss =  0.5473865   train_acc:  0.96263\n",
      "Iteration  862 : test_loss =  0.58256376   test_acc:  0.95993\n",
      "************************************************************\n",
      "Iteration  863 : train_loss =  0.5537185   train_acc:  0.9599467\n",
      "Iteration  863 : test_loss =  0.58583677   test_acc:  0.95843\n",
      "************************************************************\n",
      "Iteration  864 : train_loss =  0.5472363   train_acc:  0.962635\n",
      "Iteration  864 : test_loss =  0.58243287   test_acc:  0.95995\n",
      "************************************************************\n",
      "Iteration  865 : train_loss =  0.55352175   train_acc:  0.95995665\n",
      "Iteration  865 : test_loss =  0.58566135   test_acc:  0.95844\n",
      "************************************************************\n",
      "Iteration  866 : train_loss =  0.5470874   train_acc:  0.96263164\n",
      "Iteration  866 : test_loss =  0.5823039   test_acc:  0.95995\n",
      "************************************************************\n",
      "Iteration  867 : train_loss =  0.55332685   train_acc:  0.95997167\n",
      "Iteration  867 : test_loss =  0.58548206   test_acc:  0.95848\n",
      "************************************************************\n",
      "Iteration  868 : train_loss =  0.5469397   train_acc:  0.96264833\n",
      "Iteration  868 : test_loss =  0.58216923   test_acc:  0.95994\n",
      "************************************************************\n",
      "Iteration  869 : train_loss =  0.55313134   train_acc:  0.95998\n",
      "Iteration  869 : test_loss =  0.5853134   test_acc:  0.95847\n",
      "************************************************************\n",
      "Iteration  870 : train_loss =  0.5467169   train_acc:  0.96265334\n",
      "Iteration  870 : test_loss =  0.58198404   test_acc:  0.95994\n",
      "************************************************************\n",
      "Iteration  871 : train_loss =  0.55293715   train_acc:  0.9599817\n",
      "Iteration  871 : test_loss =  0.5851465   test_acc:  0.95852\n",
      "************************************************************\n",
      "Iteration  872 : train_loss =  0.5465706   train_acc:  0.96267164\n",
      "Iteration  872 : test_loss =  0.5818492   test_acc:  0.95995\n",
      "************************************************************\n",
      "Iteration  873 : train_loss =  0.55274296   train_acc:  0.959995\n",
      "Iteration  873 : test_loss =  0.5849729   test_acc:  0.95852\n",
      "************************************************************\n",
      "Iteration  874 : train_loss =  0.5464252   train_acc:  0.9626783\n",
      "Iteration  874 : test_loss =  0.5817225   test_acc:  0.95997\n",
      "************************************************************\n",
      "Iteration  875 : train_loss =  0.5525496   train_acc:  0.96\n",
      "Iteration  875 : test_loss =  0.5848009   test_acc:  0.95852\n",
      "************************************************************\n",
      "Iteration  876 : train_loss =  0.5462806   train_acc:  0.96269\n",
      "Iteration  876 : test_loss =  0.58159393   test_acc:  0.95999\n",
      "************************************************************\n",
      "Iteration  877 : train_loss =  0.5523574   train_acc:  0.9600217\n",
      "Iteration  877 : test_loss =  0.5846301   test_acc:  0.95852\n",
      "************************************************************\n",
      "Iteration  878 : train_loss =  0.546137   train_acc:  0.9627\n",
      "Iteration  878 : test_loss =  0.58146757   test_acc:  0.96001\n",
      "************************************************************\n",
      "Iteration  879 : train_loss =  0.5521654   train_acc:  0.96004164\n",
      "Iteration  879 : test_loss =  0.58445644   test_acc:  0.95854\n",
      "************************************************************\n",
      "Iteration  880 : train_loss =  0.5459943   train_acc:  0.96270835\n",
      "Iteration  880 : test_loss =  0.58134246   test_acc:  0.96001\n",
      "************************************************************\n",
      "Iteration  881 : train_loss =  0.55197424   train_acc:  0.96005166\n",
      "Iteration  881 : test_loss =  0.5842837   test_acc:  0.95857\n",
      "************************************************************\n",
      "Iteration  882 : train_loss =  0.54585147   train_acc:  0.9627283\n",
      "Iteration  882 : test_loss =  0.58121634   test_acc:  0.95999\n",
      "************************************************************\n",
      "Iteration  883 : train_loss =  0.5517847   train_acc:  0.96005833\n",
      "Iteration  883 : test_loss =  0.58411276   test_acc:  0.95858\n",
      "************************************************************\n",
      "Iteration  884 : train_loss =  0.54570884   train_acc:  0.962745\n",
      "Iteration  884 : test_loss =  0.5810916   test_acc:  0.95999\n",
      "************************************************************\n",
      "Iteration  885 : train_loss =  0.5515952   train_acc:  0.96006167\n",
      "Iteration  885 : test_loss =  0.5839412   test_acc:  0.95858\n",
      "************************************************************\n",
      "Iteration  886 : train_loss =  0.5455661   train_acc:  0.9627567\n",
      "Iteration  886 : test_loss =  0.5809662   test_acc:  0.95999\n",
      "************************************************************\n",
      "Iteration  887 : train_loss =  0.5514073   train_acc:  0.9600717\n",
      "Iteration  887 : test_loss =  0.5837759   test_acc:  0.95856\n",
      "************************************************************\n",
      "Iteration  888 : train_loss =  0.5454221   train_acc:  0.962765\n",
      "Iteration  888 : test_loss =  0.58084226   test_acc:  0.96002\n",
      "************************************************************\n",
      "Iteration  889 : train_loss =  0.5512187   train_acc:  0.96008664\n",
      "Iteration  889 : test_loss =  0.58360946   test_acc:  0.95856\n",
      "************************************************************\n",
      "Iteration  890 : train_loss =  0.5452802   train_acc:  0.96276\n",
      "Iteration  890 : test_loss =  0.5807163   test_acc:  0.96005\n",
      "************************************************************\n",
      "Iteration  891 : train_loss =  0.55103153   train_acc:  0.9600933\n",
      "Iteration  891 : test_loss =  0.5834444   test_acc:  0.95858\n",
      "************************************************************\n",
      "Iteration  892 : train_loss =  0.5451379   train_acc:  0.96276164\n",
      "Iteration  892 : test_loss =  0.58059365   test_acc:  0.96007\n",
      "************************************************************\n",
      "Iteration  893 : train_loss =  0.5508444   train_acc:  0.96010834\n",
      "Iteration  893 : test_loss =  0.58327794   test_acc:  0.95862\n",
      "************************************************************\n",
      "Iteration  894 : train_loss =  0.5449972   train_acc:  0.96277666\n",
      "Iteration  894 : test_loss =  0.5804708   test_acc:  0.96007\n",
      "************************************************************\n",
      "Iteration  895 : train_loss =  0.5506579   train_acc:  0.96010834\n",
      "Iteration  895 : test_loss =  0.5831114   test_acc:  0.95864\n",
      "************************************************************\n",
      "Iteration  896 : train_loss =  0.5448556   train_acc:  0.9627917\n",
      "Iteration  896 : test_loss =  0.5803469   test_acc:  0.9601\n",
      "************************************************************\n",
      "Iteration  897 : train_loss =  0.5504712   train_acc:  0.96012\n",
      "Iteration  897 : test_loss =  0.58294296   test_acc:  0.95864\n",
      "************************************************************\n",
      "Iteration  898 : train_loss =  0.54471564   train_acc:  0.9628033\n",
      "Iteration  898 : test_loss =  0.58022493   test_acc:  0.96012\n",
      "************************************************************\n",
      "Iteration  899 : train_loss =  0.55028546   train_acc:  0.960135\n",
      "Iteration  899 : test_loss =  0.5827759   test_acc:  0.95867\n",
      "************************************************************\n",
      "Iteration  900 : train_loss =  0.5445752   train_acc:  0.9628133\n",
      "Iteration  900 : test_loss =  0.5801034   test_acc:  0.96012\n",
      "************************************************************\n",
      "Iteration  901 : train_loss =  0.5500991   train_acc:  0.96015\n",
      "Iteration  901 : test_loss =  0.58261025   test_acc:  0.95868\n",
      "************************************************************\n",
      "Iteration  902 : train_loss =  0.5444355   train_acc:  0.96282333\n",
      "Iteration  902 : test_loss =  0.57998234   test_acc:  0.96014\n",
      "************************************************************\n",
      "Iteration  903 : train_loss =  0.5499143   train_acc:  0.96016\n",
      "Iteration  903 : test_loss =  0.58244467   test_acc:  0.9587\n",
      "************************************************************\n",
      "Iteration  904 : train_loss =  0.54429054   train_acc:  0.96283334\n",
      "Iteration  904 : test_loss =  0.57985896   test_acc:  0.96015\n",
      "************************************************************\n",
      "Iteration  905 : train_loss =  0.5497293   train_acc:  0.96016836\n",
      "Iteration  905 : test_loss =  0.5822811   test_acc:  0.95869\n",
      "************************************************************\n",
      "Iteration  906 : train_loss =  0.5441513   train_acc:  0.96285\n",
      "Iteration  906 : test_loss =  0.57974035   test_acc:  0.96014\n",
      "************************************************************\n",
      "Iteration  907 : train_loss =  0.5495457   train_acc:  0.9601733\n",
      "Iteration  907 : test_loss =  0.58211464   test_acc:  0.9587\n",
      "************************************************************\n",
      "Iteration  908 : train_loss =  0.54401034   train_acc:  0.96285164\n",
      "Iteration  908 : test_loss =  0.57962006   test_acc:  0.96014\n",
      "************************************************************\n",
      "Iteration  909 : train_loss =  0.5493617   train_acc:  0.96018666\n",
      "Iteration  909 : test_loss =  0.5819506   test_acc:  0.95869\n",
      "************************************************************\n",
      "Iteration  910 : train_loss =  0.543856   train_acc:  0.96286666\n",
      "Iteration  910 : test_loss =  0.579487   test_acc:  0.96014\n",
      "************************************************************\n",
      "Iteration  911 : train_loss =  0.5491797   train_acc:  0.9602\n",
      "Iteration  911 : test_loss =  0.5817902   test_acc:  0.95873\n",
      "************************************************************\n",
      "Iteration  912 : train_loss =  0.5437177   train_acc:  0.96286833\n",
      "Iteration  912 : test_loss =  0.57936764   test_acc:  0.96016\n",
      "************************************************************\n",
      "Iteration  913 : train_loss =  0.54899776   train_acc:  0.9602067\n",
      "Iteration  913 : test_loss =  0.5816275   test_acc:  0.95871\n",
      "************************************************************\n",
      "Iteration  914 : train_loss =  0.54357994   train_acc:  0.9628717\n",
      "Iteration  914 : test_loss =  0.5792487   test_acc:  0.96016\n",
      "************************************************************\n",
      "Iteration  915 : train_loss =  0.5488169   train_acc:  0.96021664\n",
      "Iteration  915 : test_loss =  0.58146864   test_acc:  0.95872\n",
      "************************************************************\n",
      "Iteration  916 : train_loss =  0.54344034   train_acc:  0.96288836\n",
      "Iteration  916 : test_loss =  0.5791308   test_acc:  0.96015\n",
      "************************************************************\n",
      "Iteration  917 : train_loss =  0.54863614   train_acc:  0.96023\n",
      "Iteration  917 : test_loss =  0.5813071   test_acc:  0.95874\n",
      "************************************************************\n",
      "Iteration  918 : train_loss =  0.54330313   train_acc:  0.96290666\n",
      "Iteration  918 : test_loss =  0.5790127   test_acc:  0.96014\n",
      "************************************************************\n",
      "Iteration  919 : train_loss =  0.5484572   train_acc:  0.9602417\n",
      "Iteration  919 : test_loss =  0.58114713   test_acc:  0.95876\n",
      "************************************************************\n",
      "Iteration  920 : train_loss =  0.5431625   train_acc:  0.96293\n",
      "Iteration  920 : test_loss =  0.57889074   test_acc:  0.96013\n",
      "************************************************************\n",
      "Iteration  921 : train_loss =  0.54827815   train_acc:  0.96024835\n",
      "Iteration  921 : test_loss =  0.5809864   test_acc:  0.95877\n",
      "************************************************************\n",
      "Iteration  922 : train_loss =  0.5430232   train_acc:  0.96294165\n",
      "Iteration  922 : test_loss =  0.57876766   test_acc:  0.96016\n",
      "************************************************************\n",
      "Iteration  923 : train_loss =  0.5481009   train_acc:  0.96027833\n",
      "Iteration  923 : test_loss =  0.58082664   test_acc:  0.95878\n",
      "************************************************************\n",
      "Iteration  924 : train_loss =  0.54287815   train_acc:  0.96296\n",
      "Iteration  924 : test_loss =  0.5786407   test_acc:  0.9602\n",
      "************************************************************\n",
      "Iteration  925 : train_loss =  0.54792345   train_acc:  0.960285\n",
      "Iteration  925 : test_loss =  0.58067226   test_acc:  0.95878\n",
      "************************************************************\n",
      "Iteration  926 : train_loss =  0.54273915   train_acc:  0.96296835\n",
      "Iteration  926 : test_loss =  0.57852197   test_acc:  0.9602\n",
      "************************************************************\n",
      "Iteration  927 : train_loss =  0.54774845   train_acc:  0.9603\n",
      "Iteration  927 : test_loss =  0.5805158   test_acc:  0.95879\n",
      "************************************************************\n",
      "Iteration  928 : train_loss =  0.54260045   train_acc:  0.96297\n",
      "Iteration  928 : test_loss =  0.57840085   test_acc:  0.96019\n",
      "************************************************************\n",
      "Iteration  929 : train_loss =  0.5475733   train_acc:  0.9603133\n",
      "Iteration  929 : test_loss =  0.58035886   test_acc:  0.9588\n",
      "************************************************************\n",
      "Iteration  930 : train_loss =  0.54245734   train_acc:  0.9629833\n",
      "Iteration  930 : test_loss =  0.5782789   test_acc:  0.9602\n",
      "************************************************************\n",
      "Iteration  931 : train_loss =  0.5473986   train_acc:  0.9603317\n",
      "Iteration  931 : test_loss =  0.5802039   test_acc:  0.95882\n",
      "************************************************************\n",
      "Iteration  932 : train_loss =  0.5423235   train_acc:  0.963\n",
      "Iteration  932 : test_loss =  0.5781629   test_acc:  0.96022\n",
      "************************************************************\n",
      "Iteration  933 : train_loss =  0.54722375   train_acc:  0.96034336\n",
      "Iteration  933 : test_loss =  0.5800456   test_acc:  0.95883\n",
      "************************************************************\n",
      "Iteration  934 : train_loss =  0.5421888   train_acc:  0.96300167\n",
      "Iteration  934 : test_loss =  0.578046   test_acc:  0.96023\n",
      "************************************************************\n",
      "Iteration  935 : train_loss =  0.54705   train_acc:  0.96035165\n",
      "Iteration  935 : test_loss =  0.5798887   test_acc:  0.95881\n",
      "************************************************************\n",
      "Iteration  936 : train_loss =  0.54205084   train_acc:  0.96300834\n",
      "Iteration  936 : test_loss =  0.57792556   test_acc:  0.96022\n",
      "************************************************************\n",
      "Iteration  937 : train_loss =  0.5468762   train_acc:  0.96035665\n",
      "Iteration  937 : test_loss =  0.57973194   test_acc:  0.95881\n",
      "************************************************************\n",
      "Iteration  938 : train_loss =  0.54191357   train_acc:  0.963025\n",
      "Iteration  938 : test_loss =  0.57780594   test_acc:  0.96023\n",
      "************************************************************\n",
      "Iteration  939 : train_loss =  0.54670185   train_acc:  0.96037334\n",
      "Iteration  939 : test_loss =  0.5795746   test_acc:  0.95884\n",
      "************************************************************\n",
      "Iteration  940 : train_loss =  0.54177547   train_acc:  0.96302\n",
      "Iteration  940 : test_loss =  0.5776868   test_acc:  0.96023\n",
      "************************************************************\n",
      "Iteration  941 : train_loss =  0.5465293   train_acc:  0.96039665\n",
      "Iteration  941 : test_loss =  0.5794188   test_acc:  0.95882\n",
      "************************************************************\n",
      "Iteration  942 : train_loss =  0.5416406   train_acc:  0.96303\n",
      "Iteration  942 : test_loss =  0.5775683   test_acc:  0.96029\n",
      "************************************************************\n",
      "Iteration  943 : train_loss =  0.54635704   train_acc:  0.96040666\n",
      "Iteration  943 : test_loss =  0.5792637   test_acc:  0.95885\n",
      "************************************************************\n",
      "Iteration  944 : train_loss =  0.5415043   train_acc:  0.96303666\n",
      "Iteration  944 : test_loss =  0.57745105   test_acc:  0.96031\n",
      "************************************************************\n",
      "Iteration  945 : train_loss =  0.5461847   train_acc:  0.960415\n",
      "Iteration  945 : test_loss =  0.5791101   test_acc:  0.95884\n",
      "************************************************************\n",
      "Iteration  946 : train_loss =  0.54137117   train_acc:  0.963045\n",
      "Iteration  946 : test_loss =  0.57733595   test_acc:  0.96032\n",
      "************************************************************\n",
      "Iteration  947 : train_loss =  0.5460124   train_acc:  0.9604333\n",
      "Iteration  947 : test_loss =  0.5789526   test_acc:  0.95885\n",
      "************************************************************\n",
      "Iteration  948 : train_loss =  0.54123765   train_acc:  0.963055\n",
      "Iteration  948 : test_loss =  0.5772196   test_acc:  0.96033\n",
      "************************************************************\n",
      "Iteration  949 : train_loss =  0.54583997   train_acc:  0.96045\n",
      "Iteration  949 : test_loss =  0.5787964   test_acc:  0.95886\n",
      "************************************************************\n",
      "Iteration  950 : train_loss =  0.5411028   train_acc:  0.96305835\n",
      "Iteration  950 : test_loss =  0.5771031   test_acc:  0.9603\n",
      "************************************************************\n",
      "Iteration  951 : train_loss =  0.5456671   train_acc:  0.960465\n",
      "Iteration  951 : test_loss =  0.57863843   test_acc:  0.95889\n",
      "************************************************************\n",
      "Iteration  952 : train_loss =  0.5409727   train_acc:  0.9630733\n",
      "Iteration  952 : test_loss =  0.5769903   test_acc:  0.96031\n",
      "************************************************************\n",
      "Iteration  953 : train_loss =  0.54549575   train_acc:  0.96047336\n",
      "Iteration  953 : test_loss =  0.57848126   test_acc:  0.95894\n",
      "************************************************************\n",
      "Iteration  954 : train_loss =  0.5408339   train_acc:  0.96308\n",
      "Iteration  954 : test_loss =  0.57686883   test_acc:  0.96032\n",
      "************************************************************\n",
      "Iteration  955 : train_loss =  0.5453251   train_acc:  0.960495\n",
      "Iteration  955 : test_loss =  0.57832474   test_acc:  0.95895\n",
      "************************************************************\n",
      "Iteration  956 : train_loss =  0.5407014   train_acc:  0.9631\n",
      "Iteration  956 : test_loss =  0.5767521   test_acc:  0.96033\n",
      "************************************************************\n",
      "Iteration  957 : train_loss =  0.54515415   train_acc:  0.960515\n",
      "Iteration  957 : test_loss =  0.578167   test_acc:  0.95894\n",
      "************************************************************\n",
      "Iteration  958 : train_loss =  0.54057026   train_acc:  0.96310335\n",
      "Iteration  958 : test_loss =  0.5766372   test_acc:  0.96033\n",
      "************************************************************\n",
      "Iteration  959 : train_loss =  0.54498386   train_acc:  0.960525\n",
      "Iteration  959 : test_loss =  0.5780087   test_acc:  0.95894\n",
      "************************************************************\n",
      "Iteration  960 : train_loss =  0.54043543   train_acc:  0.9631233\n",
      "Iteration  960 : test_loss =  0.576521   test_acc:  0.96033\n",
      "************************************************************\n",
      "Iteration  961 : train_loss =  0.5448127   train_acc:  0.96054\n",
      "Iteration  961 : test_loss =  0.57785267   test_acc:  0.95895\n",
      "************************************************************\n",
      "Iteration  962 : train_loss =  0.5403061   train_acc:  0.9631283\n",
      "Iteration  962 : test_loss =  0.57640886   test_acc:  0.96034\n",
      "************************************************************\n",
      "Iteration  963 : train_loss =  0.54464424   train_acc:  0.96055835\n",
      "Iteration  963 : test_loss =  0.57769835   test_acc:  0.95894\n",
      "************************************************************\n",
      "Iteration  964 : train_loss =  0.5401648   train_acc:  0.96313334\n",
      "Iteration  964 : test_loss =  0.5762883   test_acc:  0.96034\n",
      "************************************************************\n",
      "Iteration  965 : train_loss =  0.5444753   train_acc:  0.96056336\n",
      "Iteration  965 : test_loss =  0.577548   test_acc:  0.95897\n",
      "************************************************************\n",
      "Iteration  966 : train_loss =  0.5400361   train_acc:  0.96314\n",
      "Iteration  966 : test_loss =  0.57617664   test_acc:  0.96034\n",
      "************************************************************\n",
      "Iteration  967 : train_loss =  0.5443065   train_acc:  0.9605733\n",
      "Iteration  967 : test_loss =  0.57739407   test_acc:  0.95897\n",
      "************************************************************\n",
      "Iteration  968 : train_loss =  0.5399044   train_acc:  0.963145\n",
      "Iteration  968 : test_loss =  0.57606333   test_acc:  0.96036\n",
      "************************************************************\n",
      "Iteration  969 : train_loss =  0.54413915   train_acc:  0.9605917\n",
      "Iteration  969 : test_loss =  0.5772424   test_acc:  0.95898\n",
      "************************************************************\n",
      "Iteration  970 : train_loss =  0.5397676   train_acc:  0.96315664\n",
      "Iteration  970 : test_loss =  0.57594615   test_acc:  0.96037\n",
      "************************************************************\n",
      "Iteration  971 : train_loss =  0.5439704   train_acc:  0.960605\n",
      "Iteration  971 : test_loss =  0.57709134   test_acc:  0.95898\n",
      "************************************************************\n",
      "Iteration  972 : train_loss =  0.5396403   train_acc:  0.96317\n",
      "Iteration  972 : test_loss =  0.57583797   test_acc:  0.96038\n",
      "************************************************************\n",
      "Iteration  973 : train_loss =  0.54380435   train_acc:  0.9606133\n",
      "Iteration  973 : test_loss =  0.5769393   test_acc:  0.95898\n",
      "************************************************************\n",
      "Iteration  974 : train_loss =  0.5395007   train_acc:  0.96318334\n",
      "Iteration  974 : test_loss =  0.57571775   test_acc:  0.96037\n",
      "************************************************************\n",
      "Iteration  975 : train_loss =  0.5436376   train_acc:  0.96062666\n",
      "Iteration  975 : test_loss =  0.5767897   test_acc:  0.959\n",
      "************************************************************\n",
      "Iteration  976 : train_loss =  0.5393711   train_acc:  0.9631917\n",
      "Iteration  976 : test_loss =  0.5756051   test_acc:  0.96039\n",
      "************************************************************\n",
      "Iteration  977 : train_loss =  0.5434709   train_acc:  0.9606367\n",
      "Iteration  977 : test_loss =  0.57663935   test_acc:  0.95901\n",
      "************************************************************\n",
      "Iteration  978 : train_loss =  0.539239   train_acc:  0.963195\n",
      "Iteration  978 : test_loss =  0.5754925   test_acc:  0.96042\n",
      "************************************************************\n",
      "Iteration  979 : train_loss =  0.54330385   train_acc:  0.96064335\n",
      "Iteration  979 : test_loss =  0.57648945   test_acc:  0.95903\n",
      "************************************************************\n",
      "Iteration  980 : train_loss =  0.5391081   train_acc:  0.96320164\n",
      "Iteration  980 : test_loss =  0.5753791   test_acc:  0.96042\n",
      "************************************************************\n",
      "Iteration  981 : train_loss =  0.54313743   train_acc:  0.96065664\n",
      "Iteration  981 : test_loss =  0.5763373   test_acc:  0.95902\n",
      "************************************************************\n",
      "Iteration  982 : train_loss =  0.5389763   train_acc:  0.96320164\n",
      "Iteration  982 : test_loss =  0.57526433   test_acc:  0.96043\n",
      "************************************************************\n",
      "Iteration  983 : train_loss =  0.54297215   train_acc:  0.960665\n",
      "Iteration  983 : test_loss =  0.57618946   test_acc:  0.95903\n",
      "************************************************************\n",
      "Iteration  984 : train_loss =  0.5388458   train_acc:  0.963205\n",
      "Iteration  984 : test_loss =  0.5751517   test_acc:  0.96045\n",
      "************************************************************\n",
      "Iteration  985 : train_loss =  0.54280657   train_acc:  0.96068835\n",
      "Iteration  985 : test_loss =  0.5760393   test_acc:  0.95905\n",
      "************************************************************\n",
      "Iteration  986 : train_loss =  0.53871596   train_acc:  0.96321666\n",
      "Iteration  986 : test_loss =  0.5750407   test_acc:  0.96045\n",
      "************************************************************\n",
      "Iteration  987 : train_loss =  0.54264176   train_acc:  0.9607\n",
      "Iteration  987 : test_loss =  0.57589114   test_acc:  0.95906\n",
      "************************************************************\n",
      "Iteration  988 : train_loss =  0.5385866   train_acc:  0.9632267\n",
      "Iteration  988 : test_loss =  0.5749293   test_acc:  0.96047\n",
      "************************************************************\n",
      "Iteration  989 : train_loss =  0.54247767   train_acc:  0.96071\n",
      "Iteration  989 : test_loss =  0.5757431   test_acc:  0.95906\n",
      "************************************************************\n",
      "Iteration  990 : train_loss =  0.5384571   train_acc:  0.9632367\n",
      "Iteration  990 : test_loss =  0.5748187   test_acc:  0.96049\n",
      "************************************************************\n",
      "Iteration  991 : train_loss =  0.54231393   train_acc:  0.9607367\n",
      "Iteration  991 : test_loss =  0.5755961   test_acc:  0.95905\n",
      "************************************************************\n",
      "Iteration  992 : train_loss =  0.538327   train_acc:  0.963245\n",
      "Iteration  992 : test_loss =  0.5747069   test_acc:  0.96052\n",
      "************************************************************\n",
      "Iteration  993 : train_loss =  0.5421514   train_acc:  0.96075\n",
      "Iteration  993 : test_loss =  0.5754505   test_acc:  0.95906\n",
      "************************************************************\n",
      "Iteration  994 : train_loss =  0.5381937   train_acc:  0.963255\n",
      "Iteration  994 : test_loss =  0.57459366   test_acc:  0.96053\n",
      "************************************************************\n",
      "Iteration  995 : train_loss =  0.5419882   train_acc:  0.9607583\n",
      "Iteration  995 : test_loss =  0.5753066   test_acc:  0.95907\n",
      "************************************************************\n",
      "Iteration  996 : train_loss =  0.5380666   train_acc:  0.96327\n",
      "Iteration  996 : test_loss =  0.5744849   test_acc:  0.96053\n",
      "************************************************************\n",
      "Iteration  997 : train_loss =  0.54182607   train_acc:  0.9607717\n",
      "Iteration  997 : test_loss =  0.5751623   test_acc:  0.95906\n",
      "************************************************************\n",
      "Iteration  998 : train_loss =  0.5379361   train_acc:  0.9632817\n",
      "Iteration  998 : test_loss =  0.574372   test_acc:  0.96055\n",
      "************************************************************\n",
      "Iteration  999 : train_loss =  0.54166424   train_acc:  0.96078336\n",
      "Iteration  999 : test_loss =  0.57501847   test_acc:  0.95906\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainModel(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
