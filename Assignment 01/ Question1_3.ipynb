{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Shape of training features  (784, 60000)\n",
      "Shape of test features  (784, 10000)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "tr_x = tr_x.T\n",
    "te_x = te_x.T\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_x = tf.cast(tr_x, tf.float32) \n",
    "te_x = tf.cast(te_x, tf.float32)\n",
    "tr_y = tf.cast(tr_y, tf.float32)\n",
    "te_y = tf.cast(te_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = tf.Variable(tf.random.normal(shape=[784,300], mean=0.0, stddev=0.05))\n",
    "w2 = tf.Variable(tf.random.normal(shape=[300,100], mean=0.0, stddev=0.05))\n",
    "w3 = tf.Variable(tf.random.normal(shape=[100,10], mean=0.0, stddev=0.05))\n",
    "b = tf.Variable([0.])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forwardPass(x, w1,w2, w3, b):\n",
    "    w1 = tf.transpose(w1)\n",
    "    w2 = tf.transpose(w2)\n",
    "    w3 = tf.transpose(w3)\n",
    "     \n",
    "    y_pred = tf.matmul(w1, x) + b \n",
    "    a1 = tf.maximum(y_pred, 1) \n",
    "    \n",
    "    y_pred = tf.matmul(w2, a1) + b \n",
    "    a2 = tf.maximum(y_pred, 1) \n",
    "    \n",
    "    y_pred = tf.matmul(w3, a2) + b\n",
    "    return softmax(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(y_pred):\n",
    "    y_pred_exp = tf.math.exp(y_pred)\n",
    "    summation = tf.reduce_sum(y_pred_exp, 0, keepdims=True) \n",
    "    return y_pred_exp / summation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropyL1(y_true, y_pred, factor = 0.01):\n",
    "    m = 3\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    regularlization = 0\n",
    "    loss_per_classes = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=0)\n",
    "    regularlization = (factor/2) * (  tf.reduce_sum(tf.abs(w1))+  tf.reduce_sum(tf.abs(w2)) + tf.reduce_sum(tf.abs(w2)))\n",
    "    return tf.reduce_mean(loss_per_classes+regularlization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropyL2(y_true, y_pred, factor=0.01):\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    loss_per_classes = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=0)\n",
    "    regularlization = (factor/2 ) * (  tf.reduce_sum(tf.square(w1))+  tf.reduce_sum(tf.square(w2)) + tf.reduce_sum(tf.square(w3)))\n",
    "    return tf.reduce_mean(loss_per_classes+regularlization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(x, y, w1,w2,w3, b):\n",
    "  \n",
    "  y_pred_softmax = forwardPass(x,w1,w2,w3, b)\n",
    "  predictions_correct = tf.cast(tf.equal(tf.round(y_pred_softmax), y), tf.float32)\n",
    "  return tf.reduce_mean(predictions_correct),y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelL1(num_Iterations = 50):\n",
    "   \n",
    "    trainingLosses= []\n",
    "    testLosses= []\n",
    "    trainingAccuracies = []\n",
    "    testAccuracies = []\n",
    "    \n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        y_pred = forwardPass(tr_x,w1,w2,w3, b)\n",
    "        currentLoss = cross_entropyL1(tr_y, y_pred)\n",
    "      \n",
    "      gradients = tape.gradient(currentLoss, [w1,w2,w3, b])\n",
    "        \n",
    "     \n",
    "      tr_accuracy, y_pred_softmax = calculate_accuracy(tr_x, tr_y, w1,w2,w3, b)\n",
    "      te_accuracy, y_pred_softmax = calculate_accuracy(te_x, te_y, w1,w2,w3, b)\n",
    "      te_currentLoss = cross_entropyL1(te_y, y_pred_softmax)\n",
    "    \n",
    "      # Appending and print the information for training instances\n",
    "      trainingAccuracies.append(tr_accuracy.numpy())\n",
    "      trainingLosses.append(currentLoss.numpy())\n",
    "      print (\"Iteration \", i, \": train_loss = \",currentLoss.numpy(), \"  train_acc: \", tr_accuracy.numpy())\n",
    "      \n",
    "      # Appending and print the information for validation instances\n",
    "      testLosses.append(te_currentLoss.numpy())\n",
    "      testAccuracies.append(te_accuracy.numpy())\n",
    "      print (\"Iteration \", i, \": test_loss = \",te_currentLoss.numpy(), \"  test_acc: \", te_accuracy.numpy())\n",
    "      print(\"*\"*60)\n",
    "    \n",
    "      #Calling Adam optimizer the for updating the weights and trainfing the data\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.001).apply_gradients(zip(gradients, [w1,w2,w3,b])) \n",
    "        \n",
    "    \n",
    "    plt.style.use(\"ggplot\") \n",
    "    plt.figure()\n",
    "    plt.plot(testLosses, label=\"Val Loss\")\n",
    "    plt.plot(trainingLosses, label=\"Train Loss\")\n",
    "    plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
    "    plt.plot(testAccuracies, label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : train_loss =  61.164623   train_acc:  0.9\n",
      "Iteration  0 : test_loss =  61.16504   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  1 : train_loss =  59.693024   train_acc:  0.9\n",
      "Iteration  1 : test_loss =  59.693386   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  2 : train_loss =  58.25135   train_acc:  0.9\n",
      "Iteration  2 : test_loss =  58.251724   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  3 : train_loss =  56.83924   train_acc:  0.9\n",
      "Iteration  3 : test_loss =  56.83958   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  4 : train_loss =  55.453785   train_acc:  0.9\n",
      "Iteration  4 : test_loss =  55.454155   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  5 : train_loss =  54.096725   train_acc:  0.9\n",
      "Iteration  5 : test_loss =  54.09706   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  6 : train_loss =  52.76441   train_acc:  0.9\n",
      "Iteration  6 : test_loss =  52.764717   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  7 : train_loss =  51.455643   train_acc:  0.9\n",
      "Iteration  7 : test_loss =  51.45593   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  8 : train_loss =  50.169823   train_acc:  0.9\n",
      "Iteration  8 : test_loss =  50.170113   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  9 : train_loss =  48.90805   train_acc:  0.9\n",
      "Iteration  9 : test_loss =  48.9083   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  10 : train_loss =  47.6675   train_acc:  0.9\n",
      "Iteration  10 : test_loss =  47.66775   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  11 : train_loss =  46.450775   train_acc:  0.9\n",
      "Iteration  11 : test_loss =  46.450977   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  12 : train_loss =  45.25641   train_acc:  0.9\n",
      "Iteration  12 : test_loss =  45.256638   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  13 : train_loss =  44.08531   train_acc:  0.9\n",
      "Iteration  13 : test_loss =  44.085506   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  14 : train_loss =  42.935776   train_acc:  0.9\n",
      "Iteration  14 : test_loss =  42.935963   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  15 : train_loss =  41.810226   train_acc:  0.9\n",
      "Iteration  15 : test_loss =  41.810406   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  16 : train_loss =  40.70546   train_acc:  0.9\n",
      "Iteration  16 : test_loss =  40.70567   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  17 : train_loss =  39.624313   train_acc:  0.9\n",
      "Iteration  17 : test_loss =  39.62448   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  18 : train_loss =  38.564636   train_acc:  0.9\n",
      "Iteration  18 : test_loss =  38.564785   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  19 : train_loss =  37.5287   train_acc:  0.9\n",
      "Iteration  19 : test_loss =  37.528828   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  20 : train_loss =  36.51249   train_acc:  0.9\n",
      "Iteration  20 : test_loss =  36.51256   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  21 : train_loss =  35.51986   train_acc:  0.9\n",
      "Iteration  21 : test_loss =  35.519966   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  22 : train_loss =  34.54733   train_acc:  0.9\n",
      "Iteration  22 : test_loss =  34.547432   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  23 : train_loss =  33.597553   train_acc:  0.9\n",
      "Iteration  23 : test_loss =  33.597664   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  24 : train_loss =  32.66692   train_acc:  0.9\n",
      "Iteration  24 : test_loss =  32.66697   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  25 : train_loss =  31.760468   train_acc:  0.9\n",
      "Iteration  25 : test_loss =  31.76055   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  26 : train_loss =  30.86978   train_acc:  0.9\n",
      "Iteration  26 : test_loss =  30.869854   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  27 : train_loss =  30.001833   train_acc:  0.9\n",
      "Iteration  27 : test_loss =  30.001944   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  28 : train_loss =  29.152779   train_acc:  0.9\n",
      "Iteration  28 : test_loss =  29.152878   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  29 : train_loss =  28.325388   train_acc:  0.9\n",
      "Iteration  29 : test_loss =  28.3255   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  30 : train_loss =  27.515804   train_acc:  0.9\n",
      "Iteration  30 : test_loss =  27.51584   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  31 : train_loss =  26.727688   train_acc:  0.9\n",
      "Iteration  31 : test_loss =  26.727697   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  32 : train_loss =  25.956188   train_acc:  0.9\n",
      "Iteration  32 : test_loss =  25.956223   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  33 : train_loss =  25.206434   train_acc:  0.9\n",
      "Iteration  33 : test_loss =  25.206442   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  34 : train_loss =  24.472683   train_acc:  0.9\n",
      "Iteration  34 : test_loss =  24.472738   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  35 : train_loss =  23.762003   train_acc:  0.9\n",
      "Iteration  35 : test_loss =  23.761986   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  36 : train_loss =  23.064774   train_acc:  0.9\n",
      "Iteration  36 : test_loss =  23.064785   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  37 : train_loss =  22.38843   train_acc:  0.9\n",
      "Iteration  37 : test_loss =  22.38845   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  38 : train_loss =  21.728455   train_acc:  0.9\n",
      "Iteration  38 : test_loss =  21.728466   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  39 : train_loss =  21.087807   train_acc:  0.9\n",
      "Iteration  39 : test_loss =  21.087807   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  40 : train_loss =  20.463043   train_acc:  0.9\n",
      "Iteration  40 : test_loss =  20.463076   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  41 : train_loss =  19.856943   train_acc:  0.9\n",
      "Iteration  41 : test_loss =  19.856926   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  42 : train_loss =  19.264933   train_acc:  0.9\n",
      "Iteration  42 : test_loss =  19.264929   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  43 : train_loss =  18.691717   train_acc:  0.9\n",
      "Iteration  43 : test_loss =  18.691725   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  44 : train_loss =  18.132648   train_acc:  0.9\n",
      "Iteration  44 : test_loss =  18.132652   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  45 : train_loss =  17.591164   train_acc:  0.9\n",
      "Iteration  45 : test_loss =  17.591152   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  46 : train_loss =  17.062897   train_acc:  0.9\n",
      "Iteration  46 : test_loss =  17.06291   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  47 : train_loss =  16.551949   train_acc:  0.9\n",
      "Iteration  47 : test_loss =  16.55195   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  48 : train_loss =  16.0549   train_acc:  0.9\n",
      "Iteration  48 : test_loss =  16.05491   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  49 : train_loss =  15.57413   train_acc:  0.9\n",
      "Iteration  49 : test_loss =  15.574138   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  50 : train_loss =  15.106102   train_acc:  0.9\n",
      "Iteration  50 : test_loss =  15.106128   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  51 : train_loss =  14.653892   train_acc:  0.9\n",
      "Iteration  51 : test_loss =  14.653884   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  52 : train_loss =  14.213397   train_acc:  0.9\n",
      "Iteration  52 : test_loss =  14.213405   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  53 : train_loss =  13.78784   train_acc:  0.9\n",
      "Iteration  53 : test_loss =  13.7879095   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  54 : train_loss =  13.373484   train_acc:  0.9\n",
      "Iteration  54 : test_loss =  13.373455   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  55 : train_loss =  12.973848   train_acc:  0.9\n",
      "Iteration  55 : test_loss =  12.973857   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  56 : train_loss =  12.584434   train_acc:  0.9\n",
      "Iteration  56 : test_loss =  12.584457   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  57 : train_loss =  12.209584   train_acc:  0.9\n",
      "Iteration  57 : test_loss =  12.209633   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  58 : train_loss =  11.844673   train_acc:  0.9\n",
      "Iteration  58 : test_loss =  11.844651   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  59 : train_loss =  11.493436   train_acc:  0.9\n",
      "Iteration  59 : test_loss =  11.493399   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  60 : train_loss =  11.151673   train_acc:  0.9\n",
      "Iteration  60 : test_loss =  11.15168   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  61 : train_loss =  10.823544   train_acc:  0.9\n",
      "Iteration  61 : test_loss =  10.823517   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  62 : train_loss =  10.504492   train_acc:  0.9\n",
      "Iteration  62 : test_loss =  10.504494   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  63 : train_loss =  10.198158   train_acc:  0.9\n",
      "Iteration  63 : test_loss =  10.198171   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  64 : train_loss =  9.900872   train_acc:  0.9\n",
      "Iteration  64 : test_loss =  9.900867   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  65 : train_loss =  9.615135   train_acc:  0.9\n",
      "Iteration  65 : test_loss =  9.615131   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  66 : train_loss =  9.338577   train_acc:  0.9\n",
      "Iteration  66 : test_loss =  9.338561   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  67 : train_loss =  9.073414   train_acc:  0.9\n",
      "Iteration  67 : test_loss =  9.073419   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  68 : train_loss =  8.815703   train_acc:  0.9\n",
      "Iteration  68 : test_loss =  8.815694   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  69 : train_loss =  8.570024   train_acc:  0.9\n",
      "Iteration  69 : test_loss =  8.570003   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  70 : train_loss =  8.33011   train_acc:  0.9\n",
      "Iteration  70 : test_loss =  8.330101   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  71 : train_loss =  8.101687   train_acc:  0.9\n",
      "Iteration  71 : test_loss =  8.101678   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  72 : train_loss =  7.879651   train_acc:  0.9\n",
      "Iteration  72 : test_loss =  7.8796577   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  73 : train_loss =  7.668298   train_acc:  0.9\n",
      "Iteration  73 : test_loss =  7.6683164   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  74 : train_loss =  7.4627676   train_acc:  0.9\n",
      "Iteration  74 : test_loss =  7.462756   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  75 : train_loss =  7.2666616   train_acc:  0.9\n",
      "Iteration  75 : test_loss =  7.2666674   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  76 : train_loss =  7.076246   train_acc:  0.9\n",
      "Iteration  76 : test_loss =  7.0762343   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  77 : train_loss =  6.895339   train_acc:  0.9\n",
      "Iteration  77 : test_loss =  6.8953266   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  78 : train_loss =  6.719309   train_acc:  0.9\n",
      "Iteration  78 : test_loss =  6.7192955   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  79 : train_loss =  6.5520782   train_acc:  0.9\n",
      "Iteration  79 : test_loss =  6.5520678   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  80 : train_loss =  6.389486   train_acc:  0.9\n",
      "Iteration  80 : test_loss =  6.3894796   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  81 : train_loss =  6.2355256   train_acc:  0.9\n",
      "Iteration  81 : test_loss =  6.2355375   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  82 : train_loss =  6.08558   train_acc:  0.9\n",
      "Iteration  82 : test_loss =  6.0855927   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  83 : train_loss =  5.943704   train_acc:  0.9\n",
      "Iteration  83 : test_loss =  5.943704   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  84 : train_loss =  5.806253   train_acc:  0.9\n",
      "Iteration  84 : test_loss =  5.806255   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  85 : train_loss =  5.6764255   train_acc:  0.9\n",
      "Iteration  85 : test_loss =  5.676431   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  86 : train_loss =  5.5503917   train_acc:  0.9\n",
      "Iteration  86 : test_loss =  5.5503917   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  87 : train_loss =  5.4314647   train_acc:  0.9\n",
      "Iteration  87 : test_loss =  5.4314694   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  88 : train_loss =  5.3155866   train_acc:  0.9\n",
      "Iteration  88 : test_loss =  5.315578   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  89 : train_loss =  5.206064   train_acc:  0.9\n",
      "Iteration  89 : test_loss =  5.206056   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  90 : train_loss =  5.0998864   train_acc:  0.9\n",
      "Iteration  90 : test_loss =  5.0998893   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  91 : train_loss =  5.0001082   train_acc:  0.9\n",
      "Iteration  91 : test_loss =  5.0001154   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  92 : train_loss =  4.902903   train_acc:  0.9\n",
      "Iteration  92 : test_loss =  4.9029016   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  93 : train_loss =  4.812105   train_acc:  0.9\n",
      "Iteration  93 : test_loss =  4.812112   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  94 : train_loss =  4.722854   train_acc:  0.9\n",
      "Iteration  94 : test_loss =  4.722844   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  95 : train_loss =  4.6400833   train_acc:  0.9\n",
      "Iteration  95 : test_loss =  4.640077   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  96 : train_loss =  4.558208   train_acc:  0.9\n",
      "Iteration  96 : test_loss =  4.558195   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  97 : train_loss =  4.4827867   train_acc:  0.9\n",
      "Iteration  97 : test_loss =  4.482784   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  98 : train_loss =  4.408175   train_acc:  0.9\n",
      "Iteration  98 : test_loss =  4.4081855   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  99 : train_loss =  4.3396635   train_acc:  0.9\n",
      "Iteration  99 : test_loss =  4.3396587   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  100 : train_loss =  4.2715616   train_acc:  0.9\n",
      "Iteration  100 : test_loss =  4.271552   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  101 : train_loss =  4.2091327   train_acc:  0.9\n",
      "Iteration  101 : test_loss =  4.2091336   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  102 : train_loss =  4.1467624   train_acc:  0.9\n",
      "Iteration  102 : test_loss =  4.1467676   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  103 : train_loss =  4.090285   train_acc:  0.9\n",
      "Iteration  103 : test_loss =  4.090286   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  104 : train_loss =  4.0335746   train_acc:  0.9\n",
      "Iteration  104 : test_loss =  4.0335765   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  105 : train_loss =  3.982388   train_acc:  0.9\n",
      "Iteration  105 : test_loss =  3.9823956   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  106 : train_loss =  3.9306312   train_acc:  0.9\n",
      "Iteration  106 : test_loss =  3.9306273   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  107 : train_loss =  3.8842754   train_acc:  0.9\n",
      "Iteration  107 : test_loss =  3.884287   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  108 : train_loss =  3.8373115   train_acc:  0.9\n",
      "Iteration  108 : test_loss =  3.8373153   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  109 : train_loss =  3.795574   train_acc:  0.9\n",
      "Iteration  109 : test_loss =  3.7955704   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  110 : train_loss =  3.7526355   train_acc:  0.9\n",
      "Iteration  110 : test_loss =  3.7526348   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  111 : train_loss =  3.7150404   train_acc:  0.9\n",
      "Iteration  111 : test_loss =  3.7150297   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  112 : train_loss =  3.6762183   train_acc:  0.9\n",
      "Iteration  112 : test_loss =  3.6762297   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  113 : train_loss =  3.6424284   train_acc:  0.9\n",
      "Iteration  113 : test_loss =  3.6424274   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  114 : train_loss =  3.607272   train_acc:  0.9\n",
      "Iteration  114 : test_loss =  3.6072707   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  115 : train_loss =  3.576713   train_acc:  0.9\n",
      "Iteration  115 : test_loss =  3.576703   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  116 : train_loss =  3.544789   train_acc:  0.9\n",
      "Iteration  116 : test_loss =  3.544789   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  117 : train_loss =  3.5174756   train_acc:  0.9\n",
      "Iteration  117 : test_loss =  3.5174727   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  118 : train_loss =  3.4886107   train_acc:  0.9\n",
      "Iteration  118 : test_loss =  3.4886038   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  119 : train_loss =  3.464289   train_acc:  0.9\n",
      "Iteration  119 : test_loss =  3.4642925   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  120 : train_loss =  3.4383209   train_acc:  0.9\n",
      "Iteration  120 : test_loss =  3.438328   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  121 : train_loss =  3.4165366   train_acc:  0.9\n",
      "Iteration  121 : test_loss =  3.4165277   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  122 : train_loss =  3.3928754   train_acc:  0.9\n",
      "Iteration  122 : test_loss =  3.3928897   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  123 : train_loss =  3.3734624   train_acc:  0.9\n",
      "Iteration  123 : test_loss =  3.3734624   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  124 : train_loss =  3.3519313   train_acc:  0.9\n",
      "Iteration  124 : test_loss =  3.3519204   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  125 : train_loss =  3.33473   train_acc:  0.9\n",
      "Iteration  125 : test_loss =  3.3347368   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  126 : train_loss =  3.3156066   train_acc:  0.9\n",
      "Iteration  126 : test_loss =  3.3155968   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  127 : train_loss =  3.3007455   train_acc:  0.9\n",
      "Iteration  127 : test_loss =  3.3007402   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  128 : train_loss =  3.2835393   train_acc:  0.9\n",
      "Iteration  128 : test_loss =  3.2835367   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  129 : train_loss =  3.2705286   train_acc:  0.9\n",
      "Iteration  129 : test_loss =  3.270539   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  130 : train_loss =  3.2550929   train_acc:  0.9\n",
      "Iteration  130 : test_loss =  3.2550883   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  131 : train_loss =  3.2436316   train_acc:  0.9\n",
      "Iteration  131 : test_loss =  3.2436337   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  132 : train_loss =  3.229655   train_acc:  0.9\n",
      "Iteration  132 : test_loss =  3.2296512   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  133 : train_loss =  3.2195492   train_acc:  0.9\n",
      "Iteration  133 : test_loss =  3.219535   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  134 : train_loss =  3.2070484   train_acc:  0.9\n",
      "Iteration  134 : test_loss =  3.2070565   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  135 : train_loss =  3.198251   train_acc:  0.9\n",
      "Iteration  135 : test_loss =  3.1982436   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  136 : train_loss =  3.1869037   train_acc:  0.9\n",
      "Iteration  136 : test_loss =  3.1869097   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  137 : train_loss =  3.179066   train_acc:  0.9\n",
      "Iteration  137 : test_loss =  3.1790721   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  138 : train_loss =  3.1688933   train_acc:  0.9\n",
      "Iteration  138 : test_loss =  3.168902   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  139 : train_loss =  3.16213   train_acc:  0.9\n",
      "Iteration  139 : test_loss =  3.162129   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  140 : train_loss =  3.1528614   train_acc:  0.9\n",
      "Iteration  140 : test_loss =  3.1528645   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  141 : train_loss =  3.14702   train_acc:  0.9\n",
      "Iteration  141 : test_loss =  3.1470184   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  142 : train_loss =  3.1386795   train_acc:  0.9\n",
      "Iteration  142 : test_loss =  3.1386833   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  143 : train_loss =  3.1338003   train_acc:  0.9\n",
      "Iteration  143 : test_loss =  3.133794   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  144 : train_loss =  3.126317   train_acc:  0.9\n",
      "Iteration  144 : test_loss =  3.126316   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  145 : train_loss =  3.122226   train_acc:  0.9\n",
      "Iteration  145 : test_loss =  3.12223   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  146 : train_loss =  3.115501   train_acc:  0.9\n",
      "Iteration  146 : test_loss =  3.1155005   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  147 : train_loss =  3.1120932   train_acc:  0.9\n",
      "Iteration  147 : test_loss =  3.1120968   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  148 : train_loss =  3.106146   train_acc:  0.9\n",
      "Iteration  148 : test_loss =  3.1061437   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  149 : train_loss =  3.1032887   train_acc:  0.9\n",
      "Iteration  149 : test_loss =  3.1032956   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  150 : train_loss =  3.097941   train_acc:  0.9\n",
      "Iteration  150 : test_loss =  3.0979378   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  151 : train_loss =  3.0956428   train_acc:  0.9\n",
      "Iteration  151 : test_loss =  3.095658   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  152 : train_loss =  3.0907533   train_acc:  0.9\n",
      "Iteration  152 : test_loss =  3.090766   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  153 : train_loss =  3.088949   train_acc:  0.9\n",
      "Iteration  153 : test_loss =  3.0889523   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  154 : train_loss =  3.0845027   train_acc:  0.9\n",
      "Iteration  154 : test_loss =  3.0845046   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  155 : train_loss =  3.0830307   train_acc:  0.9\n",
      "Iteration  155 : test_loss =  3.0830297   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  156 : train_loss =  3.0788748   train_acc:  0.9\n",
      "Iteration  156 : test_loss =  3.0788813   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  157 : train_loss =  3.0777307   train_acc:  0.9\n",
      "Iteration  157 : test_loss =  3.07774   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  158 : train_loss =  3.0739324   train_acc:  0.9\n",
      "Iteration  158 : test_loss =  3.0739446   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  159 : train_loss =  3.0731118   train_acc:  0.9\n",
      "Iteration  159 : test_loss =  3.0731082   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  160 : train_loss =  3.069653   train_acc:  0.9\n",
      "Iteration  160 : test_loss =  3.0696533   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  161 : train_loss =  3.0690906   train_acc:  0.9\n",
      "Iteration  161 : test_loss =  3.069095   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  162 : train_loss =  3.0658815   train_acc:  0.9\n",
      "Iteration  162 : test_loss =  3.0658774   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  163 : train_loss =  3.0654595   train_acc:  0.9\n",
      "Iteration  163 : test_loss =  3.0654433   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  164 : train_loss =  3.0624108   train_acc:  0.9\n",
      "Iteration  164 : test_loss =  3.0624142   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  165 : train_loss =  3.0622156   train_acc:  0.9\n",
      "Iteration  165 : test_loss =  3.0622094   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  166 : train_loss =  3.059345   train_acc:  0.9\n",
      "Iteration  166 : test_loss =  3.0593429   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  167 : train_loss =  3.0593386   train_acc:  0.9\n",
      "Iteration  167 : test_loss =  3.0593443   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  168 : train_loss =  3.0567207   train_acc:  0.9\n",
      "Iteration  168 : test_loss =  3.0567331   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  169 : train_loss =  3.0569415   train_acc:  0.9\n",
      "Iteration  169 : test_loss =  3.056935   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  170 : train_loss =  3.0545294   train_acc:  0.9\n",
      "Iteration  170 : test_loss =  3.0545242   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  171 : train_loss =  3.054938   train_acc:  0.9\n",
      "Iteration  171 : test_loss =  3.054931   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  172 : train_loss =  3.0526633   train_acc:  0.9\n",
      "Iteration  172 : test_loss =  3.0526733   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  173 : train_loss =  3.0532033   train_acc:  0.9\n",
      "Iteration  173 : test_loss =  3.0532157   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  174 : train_loss =  3.0510793   train_acc:  0.9\n",
      "Iteration  174 : test_loss =  3.0510907   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  175 : train_loss =  3.0517316   train_acc:  0.9\n",
      "Iteration  175 : test_loss =  3.0517216   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  176 : train_loss =  3.0496883   train_acc:  0.9\n",
      "Iteration  176 : test_loss =  3.0497012   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  177 : train_loss =  3.0504453   train_acc:  0.9\n",
      "Iteration  177 : test_loss =  3.0504384   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  178 : train_loss =  3.0484865   train_acc:  0.9\n",
      "Iteration  178 : test_loss =  3.048496   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  179 : train_loss =  3.0493052   train_acc:  0.9\n",
      "Iteration  179 : test_loss =  3.0493102   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  180 : train_loss =  3.0474424   train_acc:  0.9\n",
      "Iteration  180 : test_loss =  3.047444   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  181 : train_loss =  3.0483162   train_acc:  0.9\n",
      "Iteration  181 : test_loss =  3.0483162   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  182 : train_loss =  3.0465298   train_acc:  0.9\n",
      "Iteration  182 : test_loss =  3.0465286   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  183 : train_loss =  3.0474365   train_acc:  0.9\n",
      "Iteration  183 : test_loss =  3.0474265   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  184 : train_loss =  3.0456595   train_acc:  0.9\n",
      "Iteration  184 : test_loss =  3.045661   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  185 : train_loss =  3.0466363   train_acc:  0.9\n",
      "Iteration  185 : test_loss =  3.046629   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  186 : train_loss =  3.0449038   train_acc:  0.9\n",
      "Iteration  186 : test_loss =  3.0449064   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  187 : train_loss =  3.0459344   train_acc:  0.9\n",
      "Iteration  187 : test_loss =  3.0459325   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  188 : train_loss =  3.044265   train_acc:  0.9\n",
      "Iteration  188 : test_loss =  3.0442758   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  189 : train_loss =  3.0453515   train_acc:  0.9\n",
      "Iteration  189 : test_loss =  3.0453582   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  190 : train_loss =  3.0437558   train_acc:  0.9\n",
      "Iteration  190 : test_loss =  3.0437577   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  191 : train_loss =  3.0448632   train_acc:  0.9\n",
      "Iteration  191 : test_loss =  3.0448594   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  192 : train_loss =  3.0432832   train_acc:  0.9\n",
      "Iteration  192 : test_loss =  3.043274   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  193 : train_loss =  3.044412   train_acc:  0.9\n",
      "Iteration  193 : test_loss =  3.0444145   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  194 : train_loss =  3.0428553   train_acc:  0.9\n",
      "Iteration  194 : test_loss =  3.0428588   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  195 : train_loss =  3.0440574   train_acc:  0.9\n",
      "Iteration  195 : test_loss =  3.044052   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  196 : train_loss =  3.0425317   train_acc:  0.9\n",
      "Iteration  196 : test_loss =  3.0425327   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  197 : train_loss =  3.0437422   train_acc:  0.9\n",
      "Iteration  197 : test_loss =  3.0437396   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  198 : train_loss =  3.0422692   train_acc:  0.9\n",
      "Iteration  198 : test_loss =  3.0422628   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  199 : train_loss =  3.0434816   train_acc:  0.9\n",
      "Iteration  199 : test_loss =  3.0434809   test_acc:  0.9\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainModelL1(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelL2(num_Iterations = 50):\n",
    "   \n",
    "    trainingLosses= []\n",
    "    testLosses= []\n",
    "    trainingAccuracies = []\n",
    "    testAccuracies = []\n",
    "    \n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        y_pred = forwardPass(tr_x,w1,w2,w3, b)\n",
    "        currentLoss = cross_entropyL2(tr_y, y_pred)\n",
    "      \n",
    "      gradients = tape.gradient(currentLoss, [w1,w2,w3, b])\n",
    "        \n",
    "     \n",
    "      tr_accuracy, y_pred_softmax = calculate_accuracy(tr_x, tr_y, w1,w2,w3, b)\n",
    "      te_accuracy, y_pred_softmax = calculate_accuracy(te_x, te_y, w1,w2,w3, b)\n",
    "      te_currentLoss = cross_entropyL2(te_y, y_pred_softmax)\n",
    "    \n",
    "      # Appending and print the information for training instances\n",
    "      trainingAccuracies.append(tr_accuracy.numpy())\n",
    "      trainingLosses.append(currentLoss.numpy())\n",
    "      print (\"Iteration \", i, \": train_loss = \",currentLoss.numpy(), \"  train_acc: \", tr_accuracy.numpy())\n",
    "      \n",
    "      # Appending and print the information for validation instances\n",
    "      testLosses.append(te_currentLoss.numpy())\n",
    "      testAccuracies.append(te_accuracy.numpy())\n",
    "      print (\"Iteration \", i, \": test_loss = \",te_currentLoss.numpy(), \"  test_acc: \", te_accuracy.numpy())\n",
    "      print(\"*\"*60)\n",
    "    \n",
    "      #Calling Adam optimizer the for updating the weights and trainfing the data\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.001).apply_gradients(zip(gradients, [w1,w2,w3,b])) \n",
    "        \n",
    "    \n",
    "    plt.style.use(\"ggplot\") \n",
    "    plt.figure()\n",
    "    plt.plot(testLosses, label=\"Val Loss\")\n",
    "    plt.plot(trainingLosses, label=\"Train Loss\")\n",
    "    plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
    "    plt.plot(testAccuracies, label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : train_loss =  5.738605   train_acc:  0.9\n",
      "Iteration  0 : test_loss =  5.7387276   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  1 : train_loss =  5.594483   train_acc:  0.9\n",
      "Iteration  1 : test_loss =  5.5945864   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  2 : train_loss =  5.470128   train_acc:  0.9\n",
      "Iteration  2 : test_loss =  5.4702773   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  3 : train_loss =  5.354712   train_acc:  0.9\n",
      "Iteration  3 : test_loss =  5.354824   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  4 : train_loss =  5.249103   train_acc:  0.9\n",
      "Iteration  4 : test_loss =  5.2492404   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  5 : train_loss =  5.1488075   train_acc:  0.9\n",
      "Iteration  5 : test_loss =  5.148954   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  6 : train_loss =  5.053676   train_acc:  0.9\n",
      "Iteration  6 : test_loss =  5.053901   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  7 : train_loss =  4.963903   train_acc:  0.9\n",
      "Iteration  7 : test_loss =  4.9640703   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  8 : train_loss =  4.875728   train_acc:  0.9\n",
      "Iteration  8 : test_loss =  4.8759885   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  9 : train_loss =  4.7886615   train_acc:  0.9\n",
      "Iteration  9 : test_loss =  4.788873   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  10 : train_loss =  4.701927   train_acc:  0.9\n",
      "Iteration  10 : test_loss =  4.702284   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  11 : train_loss =  4.618801   train_acc:  0.9\n",
      "Iteration  11 : test_loss =  4.6190963   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  12 : train_loss =  4.536906   train_acc:  0.9\n",
      "Iteration  12 : test_loss =  4.537315   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  13 : train_loss =  4.4560776   train_acc:  0.9\n",
      "Iteration  13 : test_loss =  4.4564424   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  14 : train_loss =  4.376073   train_acc:  0.9\n",
      "Iteration  14 : test_loss =  4.376579   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  15 : train_loss =  4.296763   train_acc:  0.9\n",
      "Iteration  15 : test_loss =  4.2973027   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  16 : train_loss =  4.220475   train_acc:  0.9\n",
      "Iteration  16 : test_loss =  4.2211685   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  17 : train_loss =  4.1432323   train_acc:  0.9\n",
      "Iteration  17 : test_loss =  4.1437993   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  18 : train_loss =  4.0700088   train_acc:  0.9\n",
      "Iteration  18 : test_loss =  4.0707693   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  19 : train_loss =  3.9947324   train_acc:  0.9\n",
      "Iteration  19 : test_loss =  3.9954352   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  20 : train_loss =  3.9228306   train_acc:  0.9\n",
      "Iteration  20 : test_loss =  3.9237664   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  21 : train_loss =  3.8484342   train_acc:  0.9\n",
      "Iteration  21 : test_loss =  3.8492942   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  22 : train_loss =  3.7791626   train_acc:  0.9\n",
      "Iteration  22 : test_loss =  3.7801657   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  23 : train_loss =  3.7079325   train_acc:  0.9\n",
      "Iteration  23 : test_loss =  3.7088797   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  24 : train_loss =  3.6367605   train_acc:  0.9\n",
      "Iteration  24 : test_loss =  3.637827   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  25 : train_loss =  3.564551   train_acc:  0.9\n",
      "Iteration  25 : test_loss =  3.565822   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  26 : train_loss =  3.4960418   train_acc:  0.9\n",
      "Iteration  26 : test_loss =  3.4974062   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  27 : train_loss =  3.4269433   train_acc:  0.9\n",
      "Iteration  27 : test_loss =  3.4285488   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  28 : train_loss =  3.3603745   train_acc:  0.9\n",
      "Iteration  28 : test_loss =  3.361939   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  29 : train_loss =  3.292712   train_acc:  0.9\n",
      "Iteration  29 : test_loss =  3.2947626   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  30 : train_loss =  3.2279282   train_acc:  0.9\n",
      "Iteration  30 : test_loss =  3.2297585   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  31 : train_loss =  3.1645563   train_acc:  0.9\n",
      "Iteration  31 : test_loss =  3.1667268   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  32 : train_loss =  3.1098785   train_acc:  0.9\n",
      "Iteration  32 : test_loss =  3.112314   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  33 : train_loss =  3.046662   train_acc:  0.9\n",
      "Iteration  33 : test_loss =  3.0489426   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  34 : train_loss =  2.9966683   train_acc:  0.9\n",
      "Iteration  34 : test_loss =  2.9994442   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  35 : train_loss =  2.9338624   train_acc:  0.9\n",
      "Iteration  35 : test_loss =  2.9362679   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  36 : train_loss =  2.8829725   train_acc:  0.9\n",
      "Iteration  36 : test_loss =  2.885708   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  37 : train_loss =  2.8231723   train_acc:  0.9\n",
      "Iteration  37 : test_loss =  2.825682   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  38 : train_loss =  2.7783623   train_acc:  0.9\n",
      "Iteration  38 : test_loss =  2.7813349   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  39 : train_loss =  2.7213962   train_acc:  0.9000033\n",
      "Iteration  39 : test_loss =  2.7240481   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  40 : train_loss =  2.6732123   train_acc:  0.9\n",
      "Iteration  40 : test_loss =  2.6760385   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  41 : train_loss =  2.6189172   train_acc:  0.90003335\n",
      "Iteration  41 : test_loss =  2.6216743   test_acc:  0.90004\n",
      "************************************************************\n",
      "Iteration  42 : train_loss =  2.5744545   train_acc:  0.9\n",
      "Iteration  42 : test_loss =  2.5773277   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  43 : train_loss =  2.52325   train_acc:  0.90019166\n",
      "Iteration  43 : test_loss =  2.5259366   test_acc:  0.90014\n",
      "************************************************************\n",
      "Iteration  44 : train_loss =  2.483836   train_acc:  0.9\n",
      "Iteration  44 : test_loss =  2.4869428   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  45 : train_loss =  2.4327478   train_acc:  0.9004617\n",
      "Iteration  45 : test_loss =  2.4356275   test_acc:  0.9004\n",
      "************************************************************\n",
      "Iteration  46 : train_loss =  2.3971488   train_acc:  0.90000165\n",
      "Iteration  46 : test_loss =  2.400374   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  47 : train_loss =  2.3509743   train_acc:  0.90086\n",
      "Iteration  47 : test_loss =  2.3538058   test_acc:  0.90074\n",
      "************************************************************\n",
      "Iteration  48 : train_loss =  2.3150115   train_acc:  0.9000033\n",
      "Iteration  48 : test_loss =  2.318346   test_acc:  0.9\n",
      "************************************************************\n",
      "Iteration  49 : train_loss =  2.2737129   train_acc:  0.9013017\n",
      "Iteration  49 : test_loss =  2.2764761   test_acc:  0.90107\n",
      "************************************************************\n",
      "Iteration  50 : train_loss =  2.2406256   train_acc:  0.9001383\n",
      "Iteration  50 : test_loss =  2.2442422   test_acc:  0.90014\n",
      "************************************************************\n",
      "Iteration  51 : train_loss =  2.2029188   train_acc:  0.90168667\n",
      "Iteration  51 : test_loss =  2.2059305   test_acc:  0.90148\n",
      "************************************************************\n",
      "Iteration  52 : train_loss =  2.1711564   train_acc:  0.9014417\n",
      "Iteration  52 : test_loss =  2.1748774   test_acc:  0.9015\n",
      "************************************************************\n",
      "Iteration  53 : train_loss =  2.1365507   train_acc:  0.9020183\n",
      "Iteration  53 : test_loss =  2.1397557   test_acc:  0.90181\n",
      "************************************************************\n",
      "Iteration  54 : train_loss =  2.1085007   train_acc:  0.9027117\n",
      "Iteration  54 : test_loss =  2.1120234   test_acc:  0.9027\n",
      "************************************************************\n",
      "Iteration  55 : train_loss =  2.0747945   train_acc:  0.90302664\n",
      "Iteration  55 : test_loss =  2.0780325   test_acc:  0.90273\n",
      "************************************************************\n",
      "Iteration  56 : train_loss =  2.0476246   train_acc:  0.90354836\n",
      "Iteration  56 : test_loss =  2.0513594   test_acc:  0.90339\n",
      "************************************************************\n",
      "Iteration  57 : train_loss =  2.0187304   train_acc:  0.90358835\n",
      "Iteration  57 : test_loss =  2.022499   test_acc:  0.90333\n",
      "************************************************************\n",
      "Iteration  58 : train_loss =  1.9931381   train_acc:  0.90493834\n",
      "Iteration  58 : test_loss =  1.9967941   test_acc:  0.90495\n",
      "************************************************************\n",
      "Iteration  59 : train_loss =  1.965978   train_acc:  0.904795\n",
      "Iteration  59 : test_loss =  1.9699024   test_acc:  0.90438\n",
      "************************************************************\n",
      "Iteration  60 : train_loss =  1.9400287   train_acc:  0.90542835\n",
      "Iteration  60 : test_loss =  1.9437742   test_acc:  0.90527\n",
      "************************************************************\n",
      "Iteration  61 : train_loss =  1.916664   train_acc:  0.90764666\n",
      "Iteration  61 : test_loss =  1.9206291   test_acc:  0.9072\n",
      "************************************************************\n",
      "Iteration  62 : train_loss =  1.8926415   train_acc:  0.906185\n",
      "Iteration  62 : test_loss =  1.8964949   test_acc:  0.90617\n",
      "************************************************************\n",
      "Iteration  63 : train_loss =  1.873556   train_acc:  0.90902334\n",
      "Iteration  63 : test_loss =  1.877749   test_acc:  0.90884\n",
      "************************************************************\n",
      "Iteration  64 : train_loss =  1.8505487   train_acc:  0.90667164\n",
      "Iteration  64 : test_loss =  1.8544438   test_acc:  0.9066\n",
      "************************************************************\n",
      "Iteration  65 : train_loss =  1.8326836   train_acc:  0.91071665\n",
      "Iteration  65 : test_loss =  1.8370769   test_acc:  0.91024\n",
      "************************************************************\n",
      "Iteration  66 : train_loss =  1.8109956   train_acc:  0.90715\n",
      "Iteration  66 : test_loss =  1.8149656   test_acc:  0.90713\n",
      "************************************************************\n",
      "Iteration  67 : train_loss =  1.7951208   train_acc:  0.91186\n",
      "Iteration  67 : test_loss =  1.7996293   test_acc:  0.91142\n",
      "************************************************************\n",
      "Iteration  68 : train_loss =  1.7741189   train_acc:  0.9074417\n",
      "Iteration  68 : test_loss =  1.7782129   test_acc:  0.90737\n",
      "************************************************************\n",
      "Iteration  69 : train_loss =  1.7596967   train_acc:  0.91333336\n",
      "Iteration  69 : test_loss =  1.7642531   test_acc:  0.91286\n",
      "************************************************************\n",
      "Iteration  70 : train_loss =  1.7386696   train_acc:  0.9090933\n",
      "Iteration  70 : test_loss =  1.7429624   test_acc:  0.90887\n",
      "************************************************************\n",
      "Iteration  71 : train_loss =  1.7268958   train_acc:  0.9144167\n",
      "Iteration  71 : test_loss =  1.7314105   test_acc:  0.91388\n",
      "************************************************************\n",
      "Iteration  72 : train_loss =  1.7033716   train_acc:  0.9104983\n",
      "Iteration  72 : test_loss =  1.7078142   test_acc:  0.91015\n",
      "************************************************************\n",
      "Iteration  73 : train_loss =  1.6944852   train_acc:  0.91603833\n",
      "Iteration  73 : test_loss =  1.6987348   test_acc:  0.91557\n",
      "************************************************************\n",
      "Iteration  74 : train_loss =  1.67104   train_acc:  0.9149467\n",
      "Iteration  74 : test_loss =  1.6753355   test_acc:  0.91437\n",
      "************************************************************\n",
      "Iteration  75 : train_loss =  1.6620263   train_acc:  0.9139817\n",
      "Iteration  75 : test_loss =  1.6661164   test_acc:  0.91377\n",
      "************************************************************\n",
      "Iteration  76 : train_loss =  1.6406738   train_acc:  0.9156617\n",
      "Iteration  76 : test_loss =  1.6455592   test_acc:  0.91512\n",
      "************************************************************\n",
      "Iteration  77 : train_loss =  1.6329098   train_acc:  0.9137383\n",
      "Iteration  77 : test_loss =  1.6366085   test_acc:  0.91345\n",
      "************************************************************\n",
      "Iteration  78 : train_loss =  1.6151149   train_acc:  0.916245\n",
      "Iteration  78 : test_loss =  1.6203024   test_acc:  0.91574\n",
      "************************************************************\n",
      "Iteration  79 : train_loss =  1.6061171   train_acc:  0.91584665\n",
      "Iteration  79 : test_loss =  1.6098077   test_acc:  0.91548\n",
      "************************************************************\n",
      "Iteration  80 : train_loss =  1.5895054   train_acc:  0.91746336\n",
      "Iteration  80 : test_loss =  1.5945848   test_acc:  0.91703\n",
      "************************************************************\n",
      "Iteration  81 : train_loss =  1.5820432   train_acc:  0.9171967\n",
      "Iteration  81 : test_loss =  1.5857863   test_acc:  0.9171\n",
      "************************************************************\n",
      "Iteration  82 : train_loss =  1.5656732   train_acc:  0.91787666\n",
      "Iteration  82 : test_loss =  1.5707026   test_acc:  0.91737\n",
      "************************************************************\n",
      "Iteration  83 : train_loss =  1.5592386   train_acc:  0.917985\n",
      "Iteration  83 : test_loss =  1.5630041   test_acc:  0.91785\n",
      "************************************************************\n",
      "Iteration  84 : train_loss =  1.544921   train_acc:  0.9186317\n",
      "Iteration  84 : test_loss =  1.5499195   test_acc:  0.918\n",
      "************************************************************\n",
      "Iteration  85 : train_loss =  1.539269   train_acc:  0.9196733\n",
      "Iteration  85 : test_loss =  1.5430571   test_acc:  0.91942\n",
      "************************************************************\n",
      "Iteration  86 : train_loss =  1.5266892   train_acc:  0.91915\n",
      "Iteration  86 : test_loss =  1.5316666   test_acc:  0.91858\n",
      "************************************************************\n",
      "Iteration  87 : train_loss =  1.5213873   train_acc:  0.92090666\n",
      "Iteration  87 : test_loss =  1.5252548   test_acc:  0.92055\n",
      "************************************************************\n",
      "Iteration  88 : train_loss =  1.5086919   train_acc:  0.9192967\n",
      "Iteration  88 : test_loss =  1.5136129   test_acc:  0.91869\n",
      "************************************************************\n",
      "Iteration  89 : train_loss =  1.5042934   train_acc:  0.92117333\n",
      "Iteration  89 : test_loss =  1.5081543   test_acc:  0.92083\n",
      "************************************************************\n",
      "Iteration  90 : train_loss =  1.4926131   train_acc:  0.91997164\n",
      "Iteration  90 : test_loss =  1.4975446   test_acc:  0.91947\n",
      "************************************************************\n",
      "Iteration  91 : train_loss =  1.4889119   train_acc:  0.9226\n",
      "Iteration  91 : test_loss =  1.4927765   test_acc:  0.92232\n",
      "************************************************************\n",
      "Iteration  92 : train_loss =  1.4781283   train_acc:  0.92037\n",
      "Iteration  92 : test_loss =  1.4830714   test_acc:  0.91987\n",
      "************************************************************\n",
      "Iteration  93 : train_loss =  1.4746573   train_acc:  0.923365\n",
      "Iteration  93 : test_loss =  1.4785856   test_acc:  0.92309\n",
      "************************************************************\n",
      "Iteration  94 : train_loss =  1.464214   train_acc:  0.9205383\n",
      "Iteration  94 : test_loss =  1.4691509   test_acc:  0.92005\n",
      "************************************************************\n",
      "Iteration  95 : train_loss =  1.4615729   train_acc:  0.924555\n",
      "Iteration  95 : test_loss =  1.4655132   test_acc:  0.92434\n",
      "************************************************************\n",
      "Iteration  96 : train_loss =  1.451793   train_acc:  0.9212317\n",
      "Iteration  96 : test_loss =  1.4567754   test_acc:  0.92083\n",
      "************************************************************\n",
      "Iteration  97 : train_loss =  1.4494771   train_acc:  0.9250417\n",
      "Iteration  97 : test_loss =  1.4534371   test_acc:  0.92467\n",
      "************************************************************\n",
      "Iteration  98 : train_loss =  1.4398155   train_acc:  0.92145336\n",
      "Iteration  98 : test_loss =  1.444805   test_acc:  0.92097\n",
      "************************************************************\n",
      "Iteration  99 : train_loss =  1.4380432   train_acc:  0.9265867\n",
      "Iteration  99 : test_loss =  1.4420565   test_acc:  0.9264\n",
      "************************************************************\n",
      "Iteration  100 : train_loss =  1.4290768   train_acc:  0.9221733\n",
      "Iteration  100 : test_loss =  1.4341432   test_acc:  0.92152\n",
      "************************************************************\n",
      "Iteration  101 : train_loss =  1.427768   train_acc:  0.9269933\n",
      "Iteration  101 : test_loss =  1.4317838   test_acc:  0.92672\n",
      "************************************************************\n",
      "Iteration  102 : train_loss =  1.4181057   train_acc:  0.9239783\n",
      "Iteration  102 : test_loss =  1.4231449   test_acc:  0.9237\n",
      "************************************************************\n",
      "Iteration  103 : train_loss =  1.4176683   train_acc:  0.92724\n",
      "Iteration  103 : test_loss =  1.4216765   test_acc:  0.92723\n",
      "************************************************************\n",
      "Iteration  104 : train_loss =  1.4079196   train_acc:  0.92472\n",
      "Iteration  104 : test_loss =  1.4129474   test_acc:  0.92435\n",
      "************************************************************\n",
      "Iteration  105 : train_loss =  1.4081204   train_acc:  0.92748666\n",
      "Iteration  105 : test_loss =  1.4122486   test_acc:  0.92738\n",
      "************************************************************\n",
      "Iteration  106 : train_loss =  1.3985837   train_acc:  0.92519\n",
      "Iteration  106 : test_loss =  1.4036691   test_acc:  0.92465\n",
      "************************************************************\n",
      "Iteration  107 : train_loss =  1.3996948   train_acc:  0.92808336\n",
      "Iteration  107 : test_loss =  1.403864   test_acc:  0.92788\n",
      "************************************************************\n",
      "Iteration  108 : train_loss =  1.3902656   train_acc:  0.92558664\n",
      "Iteration  108 : test_loss =  1.3953925   test_acc:  0.92495\n",
      "************************************************************\n",
      "Iteration  109 : train_loss =  1.3919225   train_acc:  0.92875\n",
      "Iteration  109 : test_loss =  1.3961805   test_acc:  0.92858\n",
      "************************************************************\n",
      "Iteration  110 : train_loss =  1.3822831   train_acc:  0.92593336\n",
      "Iteration  110 : test_loss =  1.3874594   test_acc:  0.92529\n",
      "************************************************************\n",
      "Iteration  111 : train_loss =  1.3847555   train_acc:  0.9292783\n",
      "Iteration  111 : test_loss =  1.3890369   test_acc:  0.9292\n",
      "************************************************************\n",
      "Iteration  112 : train_loss =  1.3750672   train_acc:  0.92626834\n",
      "Iteration  112 : test_loss =  1.3802904   test_acc:  0.92564\n",
      "************************************************************\n",
      "Iteration  113 : train_loss =  1.378145   train_acc:  0.9297\n",
      "Iteration  113 : test_loss =  1.3824458   test_acc:  0.92963\n",
      "************************************************************\n",
      "Iteration  114 : train_loss =  1.368545   train_acc:  0.92658836\n",
      "Iteration  114 : test_loss =  1.3738729   test_acc:  0.92601\n",
      "************************************************************\n",
      "Iteration  115 : train_loss =  1.3719008   train_acc:  0.93015665\n",
      "Iteration  115 : test_loss =  1.3761666   test_acc:  0.9301\n",
      "************************************************************\n",
      "Iteration  116 : train_loss =  1.3626401   train_acc:  0.9268633\n",
      "Iteration  116 : test_loss =  1.3680012   test_acc:  0.92634\n",
      "************************************************************\n",
      "Iteration  117 : train_loss =  1.3660316   train_acc:  0.93063\n",
      "Iteration  117 : test_loss =  1.3703297   test_acc:  0.93057\n",
      "************************************************************\n",
      "Iteration  118 : train_loss =  1.3571974   train_acc:  0.92707336\n",
      "Iteration  118 : test_loss =  1.3625777   test_acc:  0.92652\n",
      "************************************************************\n",
      "Iteration  119 : train_loss =  1.3607228   train_acc:  0.93097\n",
      "Iteration  119 : test_loss =  1.365124   test_acc:  0.93098\n",
      "************************************************************\n",
      "Iteration  120 : train_loss =  1.3517246   train_acc:  0.92727333\n",
      "Iteration  120 : test_loss =  1.357064   test_acc:  0.92672\n",
      "************************************************************\n",
      "Iteration  121 : train_loss =  1.3558753   train_acc:  0.93123335\n",
      "Iteration  121 : test_loss =  1.3604586   test_acc:  0.93127\n",
      "************************************************************\n",
      "Iteration  122 : train_loss =  1.346186   train_acc:  0.9275067\n",
      "Iteration  122 : test_loss =  1.3514785   test_acc:  0.92704\n",
      "************************************************************\n",
      "Iteration  123 : train_loss =  1.3506924   train_acc:  0.93152165\n",
      "Iteration  123 : test_loss =  1.3554287   test_acc:  0.93153\n",
      "************************************************************\n",
      "Iteration  124 : train_loss =  1.3406329   train_acc:  0.927755\n",
      "Iteration  124 : test_loss =  1.3459224   test_acc:  0.92724\n",
      "************************************************************\n",
      "Iteration  125 : train_loss =  1.3457102   train_acc:  0.93180835\n",
      "Iteration  125 : test_loss =  1.3505398   test_acc:  0.93179\n",
      "************************************************************\n",
      "Iteration  126 : train_loss =  1.3355548   train_acc:  0.928015\n",
      "Iteration  126 : test_loss =  1.340934   test_acc:  0.92762\n",
      "************************************************************\n",
      "Iteration  127 : train_loss =  1.3414224   train_acc:  0.93204165\n",
      "Iteration  127 : test_loss =  1.3462741   test_acc:  0.93204\n",
      "************************************************************\n",
      "Iteration  128 : train_loss =  1.3314307   train_acc:  0.92824835\n",
      "Iteration  128 : test_loss =  1.3369908   test_acc:  0.92798\n",
      "************************************************************\n",
      "Iteration  129 : train_loss =  1.337756   train_acc:  0.9321967\n",
      "Iteration  129 : test_loss =  1.3425412   test_acc:  0.93234\n",
      "************************************************************\n",
      "Iteration  130 : train_loss =  1.3275039   train_acc:  0.928495\n",
      "Iteration  130 : test_loss =  1.3331689   test_acc:  0.92822\n",
      "************************************************************\n",
      "Iteration  131 : train_loss =  1.3341879   train_acc:  0.93237334\n",
      "Iteration  131 : test_loss =  1.3389415   test_acc:  0.93254\n",
      "************************************************************\n",
      "Iteration  132 : train_loss =  1.3237336   train_acc:  0.92883664\n",
      "Iteration  132 : test_loss =  1.3294044   test_acc:  0.92859\n",
      "************************************************************\n",
      "Iteration  133 : train_loss =  1.3305748   train_acc:  0.93254334\n",
      "Iteration  133 : test_loss =  1.3353995   test_acc:  0.93271\n",
      "************************************************************\n",
      "Iteration  134 : train_loss =  1.3205193   train_acc:  0.92898\n",
      "Iteration  134 : test_loss =  1.3262223   test_acc:  0.92871\n",
      "************************************************************\n",
      "Iteration  135 : train_loss =  1.3274882   train_acc:  0.9327383\n",
      "Iteration  135 : test_loss =  1.3323718   test_acc:  0.93295\n",
      "************************************************************\n",
      "Iteration  136 : train_loss =  1.3172364   train_acc:  0.9291217\n",
      "Iteration  136 : test_loss =  1.3229462   test_acc:  0.92895\n",
      "************************************************************\n",
      "Iteration  137 : train_loss =  1.3244554   train_acc:  0.93291\n",
      "Iteration  137 : test_loss =  1.3294363   test_acc:  0.93304\n",
      "************************************************************\n",
      "Iteration  138 : train_loss =  1.3141574   train_acc:  0.9292617\n",
      "Iteration  138 : test_loss =  1.3198911   test_acc:  0.92911\n",
      "************************************************************\n",
      "Iteration  139 : train_loss =  1.3215951   train_acc:  0.9330483\n",
      "Iteration  139 : test_loss =  1.3266615   test_acc:  0.9331\n",
      "************************************************************\n",
      "Iteration  140 : train_loss =  1.3111571   train_acc:  0.9294017\n",
      "Iteration  140 : test_loss =  1.3169096   test_acc:  0.92925\n",
      "************************************************************\n",
      "Iteration  141 : train_loss =  1.3189255   train_acc:  0.933165\n",
      "Iteration  141 : test_loss =  1.3240254   test_acc:  0.93326\n",
      "************************************************************\n",
      "Iteration  142 : train_loss =  1.3083581   train_acc:  0.92954\n",
      "Iteration  142 : test_loss =  1.314157   test_acc:  0.92947\n",
      "************************************************************\n",
      "Iteration  143 : train_loss =  1.3164264   train_acc:  0.93333\n",
      "Iteration  143 : test_loss =  1.3215301   test_acc:  0.93345\n",
      "************************************************************\n",
      "Iteration  144 : train_loss =  1.3060368   train_acc:  0.92965\n",
      "Iteration  144 : test_loss =  1.3118768   test_acc:  0.92955\n",
      "************************************************************\n",
      "Iteration  145 : train_loss =  1.3139651   train_acc:  0.9334767\n",
      "Iteration  145 : test_loss =  1.3190591   test_acc:  0.93362\n",
      "************************************************************\n",
      "Iteration  146 : train_loss =  1.3033412   train_acc:  0.92990667\n",
      "Iteration  146 : test_loss =  1.3092157   test_acc:  0.92985\n",
      "************************************************************\n",
      "Iteration  147 : train_loss =  1.3115911   train_acc:  0.9336283\n",
      "Iteration  147 : test_loss =  1.3166685   test_acc:  0.93376\n",
      "************************************************************\n",
      "Iteration  148 : train_loss =  1.3011026   train_acc:  0.9299883\n",
      "Iteration  148 : test_loss =  1.3069838   test_acc:  0.9299\n",
      "************************************************************\n",
      "Iteration  149 : train_loss =  1.3092104   train_acc:  0.93380165\n",
      "Iteration  149 : test_loss =  1.3143594   test_acc:  0.93397\n",
      "************************************************************\n",
      "Iteration  150 : train_loss =  1.2987391   train_acc:  0.93009335\n",
      "Iteration  150 : test_loss =  1.3046248   test_acc:  0.92999\n",
      "************************************************************\n",
      "Iteration  151 : train_loss =  1.3071849   train_acc:  0.9339517\n",
      "Iteration  151 : test_loss =  1.3123896   test_acc:  0.93411\n",
      "************************************************************\n",
      "Iteration  152 : train_loss =  1.2962698   train_acc:  0.93021834\n",
      "Iteration  152 : test_loss =  1.3021178   test_acc:  0.93006\n",
      "************************************************************\n",
      "Iteration  153 : train_loss =  1.3049631   train_acc:  0.9340417\n",
      "Iteration  153 : test_loss =  1.3102925   test_acc:  0.93416\n",
      "************************************************************\n",
      "Iteration  154 : train_loss =  1.293573   train_acc:  0.93035334\n",
      "Iteration  154 : test_loss =  1.2993869   test_acc:  0.93017\n",
      "************************************************************\n",
      "Iteration  155 : train_loss =  1.3022839   train_acc:  0.9342217\n",
      "Iteration  155 : test_loss =  1.307699   test_acc:  0.93436\n",
      "************************************************************\n",
      "Iteration  156 : train_loss =  1.2907479   train_acc:  0.93047166\n",
      "Iteration  156 : test_loss =  1.2966173   test_acc:  0.93031\n",
      "************************************************************\n",
      "Iteration  157 : train_loss =  1.2999948   train_acc:  0.93436\n",
      "Iteration  157 : test_loss =  1.305438   test_acc:  0.93449\n",
      "************************************************************\n",
      "Iteration  158 : train_loss =  1.2883354   train_acc:  0.9306383\n",
      "Iteration  158 : test_loss =  1.2942947   test_acc:  0.93045\n",
      "************************************************************\n",
      "Iteration  159 : train_loss =  1.2979305   train_acc:  0.934535\n",
      "Iteration  159 : test_loss =  1.3033297   test_acc:  0.9346\n",
      "************************************************************\n",
      "Iteration  160 : train_loss =  1.2865356   train_acc:  0.930715\n",
      "Iteration  160 : test_loss =  1.2926028   test_acc:  0.93059\n",
      "************************************************************\n",
      "Iteration  161 : train_loss =  1.295973   train_acc:  0.93469\n",
      "Iteration  161 : test_loss =  1.301308   test_acc:  0.93478\n",
      "************************************************************\n",
      "Iteration  162 : train_loss =  1.2848073   train_acc:  0.930775\n",
      "Iteration  162 : test_loss =  1.2909172   test_acc:  0.93069\n",
      "************************************************************\n",
      "Iteration  163 : train_loss =  1.2941425   train_acc:  0.9348583\n",
      "Iteration  163 : test_loss =  1.2994717   test_acc:  0.93498\n",
      "************************************************************\n",
      "Iteration  164 : train_loss =  1.282856   train_acc:  0.9308917\n",
      "Iteration  164 : test_loss =  1.2889829   test_acc:  0.9308\n",
      "************************************************************\n",
      "Iteration  165 : train_loss =  1.2924088   train_acc:  0.93495667\n",
      "Iteration  165 : test_loss =  1.2978032   test_acc:  0.93506\n",
      "************************************************************\n",
      "Iteration  166 : train_loss =  1.2809914   train_acc:  0.93103665\n",
      "Iteration  166 : test_loss =  1.2871051   test_acc:  0.93097\n",
      "************************************************************\n",
      "Iteration  167 : train_loss =  1.2904866   train_acc:  0.935105\n",
      "Iteration  167 : test_loss =  1.2959859   test_acc:  0.93524\n",
      "************************************************************\n",
      "Iteration  168 : train_loss =  1.2791045   train_acc:  0.93110836\n",
      "Iteration  168 : test_loss =  1.2852262   test_acc:  0.93101\n",
      "************************************************************\n",
      "Iteration  169 : train_loss =  1.2889726   train_acc:  0.93521833\n",
      "Iteration  169 : test_loss =  1.2945035   test_acc:  0.93536\n",
      "************************************************************\n",
      "Iteration  170 : train_loss =  1.2771837   train_acc:  0.93126166\n",
      "Iteration  170 : test_loss =  1.2833197   test_acc:  0.93117\n",
      "************************************************************\n",
      "Iteration  171 : train_loss =  1.2872034   train_acc:  0.935375\n",
      "Iteration  171 : test_loss =  1.2927837   test_acc:  0.93554\n",
      "************************************************************\n",
      "Iteration  172 : train_loss =  1.2756325   train_acc:  0.93131834\n",
      "Iteration  172 : test_loss =  1.281812   test_acc:  0.93117\n",
      "************************************************************\n",
      "Iteration  173 : train_loss =  1.2859194   train_acc:  0.93545\n",
      "Iteration  173 : test_loss =  1.2914865   test_acc:  0.93559\n",
      "************************************************************\n",
      "Iteration  174 : train_loss =  1.2739029   train_acc:  0.931465\n",
      "Iteration  174 : test_loss =  1.2801175   test_acc:  0.93134\n",
      "************************************************************\n",
      "Iteration  175 : train_loss =  1.2841573   train_acc:  0.93560666\n",
      "Iteration  175 : test_loss =  1.289759   test_acc:  0.93575\n",
      "************************************************************\n",
      "Iteration  176 : train_loss =  1.2723734   train_acc:  0.93149\n",
      "Iteration  176 : test_loss =  1.2786205   test_acc:  0.93131\n",
      "************************************************************\n",
      "Iteration  177 : train_loss =  1.2826455   train_acc:  0.93576336\n",
      "Iteration  177 : test_loss =  1.2882662   test_acc:  0.93586\n",
      "************************************************************\n",
      "Iteration  178 : train_loss =  1.2705427   train_acc:  0.93161\n",
      "Iteration  178 : test_loss =  1.2768008   test_acc:  0.93148\n",
      "************************************************************\n",
      "Iteration  179 : train_loss =  1.2811488   train_acc:  0.93588334\n",
      "Iteration  179 : test_loss =  1.2868279   test_acc:  0.93599\n",
      "************************************************************\n",
      "Iteration  180 : train_loss =  1.2688733   train_acc:  0.93170166\n",
      "Iteration  180 : test_loss =  1.2751489   test_acc:  0.93158\n",
      "************************************************************\n",
      "Iteration  181 : train_loss =  1.2794809   train_acc:  0.93604165\n",
      "Iteration  181 : test_loss =  1.2852002   test_acc:  0.93616\n",
      "************************************************************\n",
      "Iteration  182 : train_loss =  1.2673565   train_acc:  0.9316983\n",
      "Iteration  182 : test_loss =  1.2736529   test_acc:  0.93161\n",
      "************************************************************\n",
      "Iteration  183 : train_loss =  1.2783127   train_acc:  0.93620336\n",
      "Iteration  183 : test_loss =  1.2840564   test_acc:  0.93628\n",
      "************************************************************\n",
      "Iteration  184 : train_loss =  1.2656138   train_acc:  0.93186164\n",
      "Iteration  184 : test_loss =  1.2719344   test_acc:  0.93174\n",
      "************************************************************\n",
      "Iteration  185 : train_loss =  1.2765932   train_acc:  0.93636\n",
      "Iteration  185 : test_loss =  1.2823842   test_acc:  0.93653\n",
      "************************************************************\n",
      "Iteration  186 : train_loss =  1.264101   train_acc:  0.9318867\n",
      "Iteration  186 : test_loss =  1.2704527   test_acc:  0.93177\n",
      "************************************************************\n",
      "Iteration  187 : train_loss =  1.275087   train_acc:  0.9364833\n",
      "Iteration  187 : test_loss =  1.2808884   test_acc:  0.93674\n",
      "************************************************************\n",
      "Iteration  188 : train_loss =  1.2625237   train_acc:  0.93199664\n",
      "Iteration  188 : test_loss =  1.2689288   test_acc:  0.93193\n",
      "************************************************************\n",
      "Iteration  189 : train_loss =  1.27388   train_acc:  0.936615\n",
      "Iteration  189 : test_loss =  1.2796946   test_acc:  0.93689\n",
      "************************************************************\n",
      "Iteration  190 : train_loss =  1.2612661   train_acc:  0.932055\n",
      "Iteration  190 : test_loss =  1.2677084   test_acc:  0.93207\n",
      "************************************************************\n",
      "Iteration  191 : train_loss =  1.2722477   train_acc:  0.9367717\n",
      "Iteration  191 : test_loss =  1.2780813   test_acc:  0.93698\n",
      "************************************************************\n",
      "Iteration  192 : train_loss =  1.2599648   train_acc:  0.932065\n",
      "Iteration  192 : test_loss =  1.266437   test_acc:  0.93209\n",
      "************************************************************\n",
      "Iteration  193 : train_loss =  1.2712476   train_acc:  0.93687\n",
      "Iteration  193 : test_loss =  1.2771195   test_acc:  0.93709\n",
      "************************************************************\n",
      "Iteration  194 : train_loss =  1.2583095   train_acc:  0.93223\n",
      "Iteration  194 : test_loss =  1.2647408   test_acc:  0.93223\n",
      "************************************************************\n",
      "Iteration  195 : train_loss =  1.2699307   train_acc:  0.9370217\n",
      "Iteration  195 : test_loss =  1.275936   test_acc:  0.93714\n",
      "************************************************************\n",
      "Iteration  196 : train_loss =  1.2566959   train_acc:  0.93228\n",
      "Iteration  196 : test_loss =  1.2629604   test_acc:  0.93222\n",
      "************************************************************\n",
      "Iteration  197 : train_loss =  1.2679893   train_acc:  0.93710834\n",
      "Iteration  197 : test_loss =  1.2741681   test_acc:  0.93726\n",
      "************************************************************\n",
      "Iteration  198 : train_loss =  1.2538415   train_acc:  0.9324933\n",
      "Iteration  198 : test_loss =  1.2598151   test_acc:  0.9325\n",
      "************************************************************\n",
      "Iteration  199 : train_loss =  1.2642611   train_acc:  0.93743\n",
      "Iteration  199 : test_loss =  1.270668   test_acc:  0.93767\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainModelL2(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
